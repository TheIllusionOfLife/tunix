{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3de3bc",
   "metadata": {
    "papermill": {
     "duration": 0.002715,
     "end_time": "2026-01-07T05:54:30.242942",
     "exception": false,
     "start_time": "2026-01-07T05:54:30.240227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tunix SFT: Teaching Reasoning Through Demonstration\n",
    "\n",
    "**Strategy**: Supervised Fine-Tuning on high-quality reasoning traces across diverse domains.\n",
    "\n",
    "**Key Insight**: For 2B parameter models, learning from demonstrations is more effective than reinforcement learning. SFT provides dense supervision at every token, while RL provides sparse rewards only at sequence end.\n",
    "\n",
    "**Datasets**: \n",
    "- Raiden-DeepSeek-R1 (Creative/Analytical)\n",
    "- OpenO1-SFT (General Reasoning)\n",
    "- CoT-Collection (Commonsense/Ethics)\n",
    "- GlaiveAI-Reasoning (Math/Code/General)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0819a",
   "metadata": {
    "papermill": {
     "duration": 0.002215,
     "end_time": "2026-01-07T05:54:30.247453",
     "exception": false,
     "start_time": "2026-01-07T05:54:30.245238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Overall training and evaluation strategy\n",
    "\n",
    "**Strategy: SFT on Diverse Domain Reasoning Traces**\n",
    "\n",
    "Competition FAQ explicitly states that verifiable tasks (math/coding) have \"much lower weights\". Our strategy prioritizes non-verifiable domains:\n",
    "\n",
    "1.  **Base Model**: We start with `Gemma-2-2b-it` for its instruction-following foundation.\n",
    "2.  **SFT Training**: We fine-tune on ~100K reasoning traces from diverse domains (creative, analytical, philosophical, commonsense).\n",
    "3.  **Format**: All data uses explicit `<reasoning>` and `<answer>` tags for structured outputs.\n",
    "\n",
    "## ðŸ—ºï¸ Workflow Diagram\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Gemma-2B-IT] --> B{SFT Training}\n",
    "    B -->|Creative| C[Raiden-DeepSeek-R1]\n",
    "    B -->|Reasoning| D[OpenO1-SFT]\n",
    "    B -->|Ethics| E[CoT-Collection]\n",
    "    B -->|General| F[GlaiveAI]\n",
    "    C & D & E & F --> G[Trained Model]\n",
    "    G --> H[Submission]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf82c5",
   "metadata": {
    "papermill": {
     "duration": 0.002371,
     "end_time": "2026-01-07T05:54:30.252197",
     "exception": false,
     "start_time": "2026-01-07T05:54:30.249826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## How your finetuning dataset is created\n",
    "\n",
    "We employ a **Diverse Domain Strategy** using publicly available datasets with reasoning traces:\n",
    "\n",
    "| Dataset | Source | Samples | Domain | License |\n",
    "|:---|:---|:---:|:---|:---|\n",
    "| Raiden-DeepSeek-R1 | HuggingFace | 62.9K | Creative/Analytical | Apache 2.0 |\n",
    "| OpenO1-SFT | HuggingFace | 20K (English-only) | General Reasoning | Apache 2.0 |\n",
    "| CoT-Collection | HuggingFace | 10K (pre-sampled) | Commonsense/Ethics | CC-BY-4.0 |\n",
    "| GlaiveAI-Reasoning | HuggingFace | 30K | Non-math/code | Apache 2.0 |\n",
    "\n",
    "All datasets are pre-processed and attached as parquet files for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b603a",
   "metadata": {
    "papermill": {
     "duration": 0.00237,
     "end_time": "2026-01-07T05:54:30.256941",
     "exception": false,
     "start_time": "2026-01-07T05:54:30.254571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tunix finetuning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8326da13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T05:54:30.264387Z",
     "iopub.status.busy": "2026-01-07T05:54:30.264200Z",
     "iopub.status.idle": "2026-01-07T05:54:30.268556Z",
     "shell.execute_reply": "2026-01-07T05:54:30.267753Z"
    },
    "papermill": {
     "duration": 0.008817,
     "end_time": "2026-01-07T05:54:30.269203",
     "exception": false,
     "start_time": "2026-01-07T05:54:30.260386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template variables defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters\n",
    "TEMPERATURE=0.7\n",
    "TOP_K=50\n",
    "TOP_P=0.9\n",
    "MAX_GENERATION_STEPS=768\n",
    "\n",
    "# Output Tags\n",
    "REASONING_START = \"<reasoning>\"\n",
    "REASONING_END = \"</reasoning>\"\n",
    "SOLUTION_START = \"<answer>\"\n",
    "SOLUTION_END = \"</answer>\"\n",
    "\n",
    "# Inference Params\n",
    "INF_TEMPERATURE=0\n",
    "INF_TOP_K=1\n",
    "INF_TOP_P=None\n",
    "SEED=42\n",
    "\n",
    "# System prompt and template\n",
    "SYSTEM_PROMPT = \"You are a deep thinking AI. Think step by step about the problem and provide your reasoning between <reasoning> and </reasoning> tags. Then, provide the final answer between <answer> and </answer> tags.\"\n",
    "TEMPLATE = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{{question}}<end_of_turn>\\n<start_of_turn>model\"\n",
    "\n",
    "print(\"Template variables defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c71d619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T05:54:30.275313Z",
     "iopub.status.busy": "2026-01-07T05:54:30.275123Z",
     "iopub.status.idle": "2026-01-07T05:55:52.602835Z",
     "shell.execute_reply": "2026-01-07T05:55:52.601685Z"
    },
    "papermill": {
     "duration": 82.331725,
     "end_time": "2026-01-07T05:55:52.603479",
     "exception": false,
     "start_time": "2026-01-07T05:54:30.271754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-tunix==0.1.5 (from google-tunix[prod]==0.1.5)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading google_tunix-0.1.5-py3-none-any.whl.metadata (8.2 kB)\r\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2.14.4)\r\n",
      "Requirement already satisfied: flax>=0.11.1 in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.12.1)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2025.12.0)\r\n",
      "Requirement already satisfied: google-metrax>=0.2.3 in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.2.4)\r\n",
      "Requirement already satisfied: grain in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.2.15)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.36.0)\r\n",
      "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.3.4)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (3.1.6)\r\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.3.13)\r\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.63.0b1)\r\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2.4.0.dev4)\r\n",
      "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (3.0a33)\r\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.2.1)\r\n",
      "Requirement already satisfied: qwix in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.1.4)\r\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.2.1)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.14.0)\r\n",
      "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (4.9.9)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (4.67.1)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (4.57.1)\r\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.1.9)\r\n",
      "Requirement already satisfied: jax!=0.7.2,>=0.6.0 in /usr/local/lib/python3.12/site-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.5) (0.8.1)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2.4.0rc1)\r\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.1.2)\r\n",
      "Requirement already satisfied: optax in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.2.6)\r\n",
      "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.11.31)\r\n",
      "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.1.80)\r\n",
      "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (14.2.0)\r\n",
      "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (4.15.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (6.0.3)\r\n",
      "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.1.10)\r\n",
      "Requirement already satisfied: clu>=0.0.12 in /usr/local/lib/python3.12/site-packages (from google-metrax>=0.2.3->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.0.12)\r\n",
      "Requirement already satisfied: tensorboardx>=2.6.4 in /usr/local/lib/python3.12/site-packages (from google-metrax>=0.2.3->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2.6.4)\r\n",
      "Requirement already satisfied: jaxlib<=0.8.1,>=0.8.1 in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.5) (0.8.1)\r\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.5) (0.5.4)\r\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.5) (3.4.0)\r\n",
      "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.5) (1.17.0rc1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting libtpu==0.0.30.* (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.5)\r\n",
      "  Downloading libtpu-0.0.30-cp312-cp312-manylinux_2_31_x86_64.whl.metadata (1.2 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.5) (2.32.5)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (22.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (3.0.0rc0)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (3.6.0)\r\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.70.15)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (3.13.2)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (25.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from huggingface_hub->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (3.20.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface_hub->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.2.1rc0)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2.3.1)\r\n",
      "Requirement already satisfied: array-record>=0.8.1 in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.8.3)\r\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (3.1.2)\r\n",
      "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.13.0)\r\n",
      "Requirement already satisfied: protobuf>=5.28.3 in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (6.33.2)\r\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/site-packages (from jaxtyping->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.1.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (3.0.3)\r\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in /usr/local/lib/python3.12/site-packages (from numba->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.46.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.1.9)\r\n",
      "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (4.2.2)\r\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2.3)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (7.1.3)\r\n",
      "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.1.7)\r\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.17.2)\r\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (3.2.0)\r\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.10.2)\r\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/site-packages (from transformers->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2025.11.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/site-packages (from transformers->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.22.2rc0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/site-packages (from transformers->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.7.0)\r\n",
      "Requirement already satisfied: ml-collections in /usr/local/lib/python3.12/site-packages (from clu>=0.0.12->google-metrax>=0.2.3->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.1.0)\r\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.8.1)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (6.5.2)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (3.23.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/site-packages (from aiohttp->datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/site-packages (from aiohttp->datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp->datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (25.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/site-packages (from aiohttp->datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.8.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/site-packages (from aiohttp->datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (6.7.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/site-packages (from aiohttp->datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.4.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/site-packages (from aiohttp->datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.22.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.5) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.5) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.5) (2.6.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.5) (2025.11.12)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (4.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2.19.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/site-packages (from optax->flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.1.91)\r\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.6.0)\r\n",
      "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (25.1.0)\r\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (4.14.0)\r\n",
      "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (3.20.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas->datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2.9.0.post0)\r\n",
      "Requirement already satisfied: tzdata>=2023.3 in /usr/local/lib/python3.12/site-packages (from pandas->datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (2025.3)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.12/site-packages (from promise->tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.17.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.12/site-packages (from simple_parsing->tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.17.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.12/site-packages (from tensorflow-metadata->tensorflow_datasets->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.72.0)\r\n",
      "Requirement already satisfied: toolz>=1.0.0 in /usr/local/lib/python3.12/site-packages (from chex>=0.1.87->optax->flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (1.1.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.11.1->google-tunix==0.1.5->google-tunix[prod]==0.1.5) (0.1.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading google_tunix-0.1.5-py3-none-any.whl (310 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading libtpu-0.0.30-cp312-cp312-manylinux_2_31_x86_64.whl (192.7 MB)\r\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/192.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/192.7 MB\u001b[0m \u001b[31m240.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.4/192.7 MB\u001b[0m \u001b[31m253.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m155.5/192.7 MB\u001b[0m \u001b[31m257.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m192.7/192.7 MB\u001b[0m \u001b[31m259.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m192.7/192.7 MB\u001b[0m \u001b[31m259.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m192.7/192.7 MB\u001b[0m \u001b[31m259.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m192.7/192.7 MB\u001b[0m \u001b[31m259.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m192.7/192.7 MB\u001b[0m \u001b[31m259.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m192.7/192.7 MB\u001b[0m \u001b[31m259.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m192.7/192.7 MB\u001b[0m \u001b[31m259.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m192.7/192.7 MB\u001b[0m \u001b[31m259.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.7/192.7 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: libtpu, google-tunix\r\n",
      "  Attempting uninstall: libtpu\r\n",
      "    Found existing installation: libtpu 0.0.17\r\n",
      "    Uninstalling libtpu-0.0.17:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled libtpu-0.0.17\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: google-tunix\r\n",
      "    Found existing installation: google-tunix 0.1.6\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling google-tunix-0.1.6:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled google-tunix-0.1.6\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed google-tunix-0.1.5 libtpu-0.0.30\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/google/qwix\r\n",
      "  Cloning https://github.com/google/qwix to /tmp/pip-req-build-0fo4b5us\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/google/qwix /tmp/pip-req-build-0fo4b5us\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved https://github.com/google/qwix to commit 5c9ba31450984c4efe23d20151a331bf3bf13f45\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \b\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \b|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \b/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \bdone\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: jax in /usr/local/lib/python3.12/site-packages (from qwix==0.1.5) (0.8.1)\r\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/site-packages (from qwix==0.1.5) (0.8.1)\r\n",
      "Requirement already satisfied: flax>=0.12.0 in /usr/local/lib/python3.12/site-packages (from qwix==0.1.5) (0.12.1)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/site-packages (from flax>=0.12.0->qwix==0.1.5) (2.4.0rc1)\r\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/site-packages (from flax>=0.12.0->qwix==0.1.5) (1.1.2)\r\n",
      "Requirement already satisfied: optax in /usr/local/lib/python3.12/site-packages (from flax>=0.12.0->qwix==0.1.5) (0.2.6)\r\n",
      "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/site-packages (from flax>=0.12.0->qwix==0.1.5) (0.11.31)\r\n",
      "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/site-packages (from flax>=0.12.0->qwix==0.1.5) (0.1.80)\r\n",
      "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/site-packages (from flax>=0.12.0->qwix==0.1.5) (14.2.0)\r\n",
      "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/site-packages (from flax>=0.12.0->qwix==0.1.5) (4.15.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/site-packages (from flax>=0.12.0->qwix==0.1.5) (6.0.3)\r\n",
      "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/site-packages (from flax>=0.12.0->qwix==0.1.5) (0.1.10)\r\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/site-packages (from jax->qwix==0.1.5) (0.5.4)\r\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/site-packages (from jax->qwix==0.1.5) (3.4.0)\r\n",
      "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/site-packages (from jax->qwix==0.1.5) (1.17.0rc1)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1->flax>=0.12.0->qwix==0.1.5) (4.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1->flax>=0.12.0->qwix==0.1.5) (2.19.2)\r\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/site-packages (from optax->flax>=0.12.0->qwix==0.1.5) (2.3.1)\r\n",
      "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/site-packages (from optax->flax>=0.12.0->qwix==0.1.5) (0.1.91)\r\n",
      "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.12.0->qwix==0.1.5) (1.13.0)\r\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.12.0->qwix==0.1.5) (1.6.0)\r\n",
      "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.12.0->qwix==0.1.5) (25.1.0)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.12.0->qwix==0.1.5) (6.33.2)\r\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.12.0->qwix==0.1.5) (4.14.0)\r\n",
      "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.12.0->qwix==0.1.5) (3.20.2)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.12.0->qwix==0.1.5) (7.1.3)\r\n",
      "Requirement already satisfied: toolz>=1.0.0 in /usr/local/lib/python3.12/site-packages (from chex>=0.1.87->optax->flax>=0.12.0->qwix==0.1.5) (1.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.12.0->qwix==0.1.5) (0.1.2)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.12.0->qwix==0.1.5) (2025.12.0)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.12.0->qwix==0.1.5) (6.5.2)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.12.0->qwix==0.1.5) (3.23.0)\r\n",
      "Building wheels for collected packages: qwix\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for qwix (pyproject.toml) ... \u001b[?25l-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \bdone\r\n",
      "\u001b[?25h  Created wheel for qwix: filename=qwix-0.1.5-py3-none-any.whl size=97804 sha256=2cbfa760a59221f499ef8f0537ea46107a9f5c12e27f44743376b24b6f31ab11\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-t7nvmoin/wheels/78/05/1e/4dc706f6088e8b0a34785cf3cecdaa45f669e78b3d267d5f37\r\n",
      "Successfully built qwix\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: qwix\r\n",
      "  Attempting uninstall: qwix\r\n",
      "    Found existing installation: qwix 0.1.4\r\n",
      "    Uninstalling qwix-0.1.4:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled qwix-0.1.4\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed qwix-0.1.5\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flax==0.12.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading flax-0.12.0-py3-none-any.whl.metadata (11 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/site-packages (from flax==0.12.0) (2.4.0rc1)\r\n",
      "Requirement already satisfied: jax>=0.7.1 in /usr/local/lib/python3.12/site-packages (from flax==0.12.0) (0.8.1)\r\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/site-packages (from flax==0.12.0) (1.1.2)\r\n",
      "Requirement already satisfied: optax in /usr/local/lib/python3.12/site-packages (from flax==0.12.0) (0.2.6)\r\n",
      "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/site-packages (from flax==0.12.0) (0.11.31)\r\n",
      "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/site-packages (from flax==0.12.0) (0.1.80)\r\n",
      "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/site-packages (from flax==0.12.0) (14.2.0)\r\n",
      "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/site-packages (from flax==0.12.0) (4.15.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/site-packages (from flax==0.12.0) (6.0.3)\r\n",
      "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/site-packages (from flax==0.12.0) (0.1.10)\r\n",
      "Requirement already satisfied: jaxlib<=0.8.1,>=0.8.1 in /usr/local/lib/python3.12/site-packages (from jax>=0.7.1->flax==0.12.0) (0.8.1)\r\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/site-packages (from jax>=0.7.1->flax==0.12.0) (0.5.4)\r\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/site-packages (from jax>=0.7.1->flax==0.12.0) (3.4.0)\r\n",
      "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/site-packages (from jax>=0.7.1->flax==0.12.0) (1.17.0rc1)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1->flax==0.12.0) (4.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1->flax==0.12.0) (2.19.2)\r\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/site-packages (from optax->flax==0.12.0) (2.3.1)\r\n",
      "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/site-packages (from optax->flax==0.12.0) (0.1.91)\r\n",
      "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax==0.12.0) (1.13.0)\r\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax==0.12.0) (1.6.0)\r\n",
      "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax==0.12.0) (25.1.0)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax==0.12.0) (6.33.2)\r\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax==0.12.0) (4.14.0)\r\n",
      "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax==0.12.0) (3.20.2)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax==0.12.0) (7.1.3)\r\n",
      "Requirement already satisfied: toolz>=1.0.0 in /usr/local/lib/python3.12/site-packages (from chex>=0.1.87->optax->flax==0.12.0) (1.1.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.12.0) (0.1.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (2025.12.0)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (6.5.2)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (3.23.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading flax-0.12.0-py3-none-any.whl (466 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: flax\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed flax-0.12.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "distrax 0.1.7 requires chex>=0.1.90, but you have chex 0.1.88 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/torch_xla/__init__.py:258: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1767765344.483801      74 common_lib.cc:650] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: === \n",
      "learning/45eac/tfrc/runtime/common_lib.cc:238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=4, process_index=0, coords=(0,2,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(1,2,0), core_on_chip=0), TpuDevice(id=6, process_index=0, coords=(0,3,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,3,0), core_on_chip=0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Setup & Install ---\n",
    "!pip install -q wandb==0.22.0\n",
    "!pip install -q kagglehub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q transformers\n",
    "!pip install -q grain\n",
    "\n",
    "# Tunix/Qwix Installation\n",
    "# Check if we are offline (no internet), if so, assume wheels are attached\n",
    "import socket\n",
    "import os\n",
    "\n",
    "def is_connected():\n",
    "    try:\n",
    "        # Check simple connectivity\n",
    "        socket.create_connection((\"1.1.1.1\", 53))\n",
    "        return True\n",
    "    except OSError:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "if is_connected():\n",
    "    !pip install \"google-tunix[prod]==0.1.5\"\n",
    "    !pip install git+https://github.com/google/qwix\n",
    "else:\n",
    "    print(\"Offline mode detected. Assuming dependencies are installed or wheels provided.\")\n",
    "    # Fallback: Try installing from local wheels if available\n",
    "    if os.path.exists(\"/kaggle/input/tunix-wheels\"):\n",
    "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels google-tunix\n",
    "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels qwix\n",
    "\n",
    "\n",
    "# Fix Flax Version to 0.12.0 as required\n",
    "!pip uninstall -q -y flax\n",
    "!pip install flax==0.12.0\n",
    "\n",
    "!pip install -q datasets==3.2.0 optax==0.2.4 chex==0.1.88\n",
    "\n",
    "# --- Imports ---\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "from pprint import pprint\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "import tensorflow_datasets as tfds\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Tunix Imports\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma import model as gemma_lib\n",
    "from tunix.models.gemma import params as params_lib\n",
    "from tunix.sft import metrics_logger\n",
    "from tunix.sft import peft_trainer\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Stability Configs ---\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.95'\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "\n",
    "print(f\"JAX Devices: {jax.devices()}\")\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "DATASET_PATH = \"/kaggle/input/tunix-sft-data\"\n",
    "SFT_OUTPUT_DIR = \"/kaggle/working/sft_checkpoint\"\n",
    "\n",
    "# Tuning Hyperparams - Adjust these for HP tuning\n",
    "SFT_STEPS = 8000  # ~2 epochs with 122k samples, effective batch 32\n",
    "TRAIN_BATCH_SIZE = 8 # Per-step batch size across all 8 TPU chips (1 sample/chip)\n",
    "GRADIENT_ACCUMULATION = 4  # Effective batch = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION\n",
    "EFFECTIVE_BATCH = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION  # 32\n",
    "\n",
    "# Learning Rate - Key HP for tuning\n",
    "LEARNING_RATE = 2e-5  # Try: 5e-5, 2e-5, 1e-5\n",
    "WARMUP_STEPS = 200  # Warmup before reaching peak LR\n",
    "\n",
    "# LoRA Hyperparams\n",
    "RANK = 64\n",
    "ALPHA = 64.0\n",
    "\n",
    "# Sequence Length\n",
    "MAX_SEQ_LEN = 2048  # Critical: increased from 1024 to avoid truncating reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ea4dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T05:55:52.617151Z",
     "iopub.status.busy": "2026-01-07T05:55:52.616627Z",
     "iopub.status.idle": "2026-01-07T05:55:52.622322Z",
     "shell.execute_reply": "2026-01-07T05:55:52.621597Z"
    },
    "papermill": {
     "duration": 0.013007,
     "end_time": "2026-01-07T05:55:52.622949",
     "exception": false,
     "start_time": "2026-01-07T05:55:52.609942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Utilities ---\n",
    "MESH = [(8, 1), (\"fsdp\", \"tp\")]\n",
    "\n",
    "def get_gemma_model(ckpt_path):\n",
    "    mesh = jax.make_mesh(*MESH)\n",
    "    model_config = gemma_lib.ModelConfig.gemma2_2b()\n",
    "    abs_gemma: nnx.Module = nnx.eval_shape(\n",
    "        lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
    "    )\n",
    "    abs_state = nnx.state(abs_gemma)\n",
    "    abs_state = jax.tree.map(\n",
    "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "        abs_state,\n",
    "        nnx.get_named_sharding(abs_state, mesh),\n",
    "    )\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "\n",
    "    graph_def, _ = nnx.split(abs_gemma)\n",
    "    gemma = nnx.merge(graph_def, restored_params)\n",
    "    return gemma, mesh, model_config\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    # LoRA config uses RANK and ALPHA from constants\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "            \".*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "\n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, rngs=nnx.Rngs(params=0), **model_input\n",
    "    )\n",
    "\n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "\n",
    "    return lora_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d881093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T05:55:52.635421Z",
     "iopub.status.busy": "2026-01-07T05:55:52.635217Z",
     "iopub.status.idle": "2026-01-07T05:55:55.197880Z",
     "shell.execute_reply": "2026-01-07T05:55:55.196887Z"
    },
    "papermill": {
     "duration": 2.569905,
     "end_time": "2026-01-07T05:55:55.198659",
     "exception": false,
     "start_time": "2026-01-07T05:55:52.628754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmukaiyuya\u001b[0m (\u001b[33mmukaiyuya-mukai-entertainment\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20260107_055554-i1l5sqs6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msft-run-v2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mukaiyuya-mukai-entertainment/tunix-sft-diverse?apiKey=7acf620806f8ef333b70b80e7c732585079d1ed5\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mukaiyuya-mukai-entertainment/tunix-sft-diverse/runs/i1l5sqs6?apiKey=7acf620806f8ef333b70b80e7c732585079d1ed5\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB Logging Enabled with hyperparameter tracking.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- WandB Logging with Metrics Backend ---\n",
    "WANDB_ENABLED = False\n",
    "\n",
    "# Define WandB Backend for MetricsLogger\n",
    "class WandbBackend:\n",
    "    '''Custom backend to stream metrics to WandB during training'''\n",
    "    def log_scalar(self, event: str, value, **kwargs):\n",
    "        if WANDB_ENABLED:\n",
    "            step = kwargs.get(\"step\", 0)\n",
    "            wandb.log({event: float(value)}, step=step)\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "    if secret_value:\n",
    "        wandb.login(key=secret_value)\n",
    "        # Log hyperparameters to WandB config\n",
    "        wandb.init(\n",
    "            project=\"tunix-sft-diverse\",\n",
    "            name=\"sft-run-v2\",\n",
    "            anonymous=\"allow\",\n",
    "            config={\n",
    "                \"sft_steps\": SFT_STEPS,\n",
    "                \"learning_rate\": LEARNING_RATE,\n",
    "                \"warmup_steps\": WARMUP_STEPS,\n",
    "                \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
    "                \"gradient_accumulation\": GRADIENT_ACCUMULATION,\n",
    "                \"effective_batch\": EFFECTIVE_BATCH,\n",
    "                \"max_seq_len\": MAX_SEQ_LEN,\n",
    "                \"lora_rank\": RANK,\n",
    "                \"lora_alpha\": ALPHA,\n",
    "                \"model_id\": MODEL_ID,\n",
    "            }\n",
    "        )\n",
    "        WANDB_ENABLED = True\n",
    "        print(\"WandB Logging Enabled with hyperparameter tracking.\")\n",
    "    else:\n",
    "        raise ValueError(\"Empty WANDB_API_KEY\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"WandB not enabled: {e}\")\n",
    "    os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "    print(\"Proceeding without cloud logging (WANDB_MODE='disabled').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de12529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T05:55:55.213273Z",
     "iopub.status.busy": "2026-01-07T05:55:55.213071Z",
     "iopub.status.idle": "2026-01-07T05:56:32.587678Z",
     "shell.execute_reply": "2026-01-07T05:56:32.586177Z"
    },
    "papermill": {
     "duration": 37.383059,
     "end_time": "2026-01-07T05:56:32.588387",
     "exception": false,
     "start_time": "2026-01-07T05:55:55.205328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from Kaggle/HuggingFace...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c024f40a5c48eaba9a0d2cb85e9c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 samples from /kaggle/input/tunix-sft-data/cot_collection_10k.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d0a373c6a94d6e895d813e985280dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20000 samples from /kaggle/input/tunix-sft-data/openo1_sft_english_20k.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acba072053324707b516322e393a9988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 62925 samples from /kaggle/input/tunix-sft-data/raiden_deepseek_r1.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a61a7f850243b79a0544f278093eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30000 samples from /kaggle/input/tunix-sft-data/glaiveai_30k.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples after preprocessing: 122925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final SFT dataset: 122925 samples\n",
      "Sample: <start_of_turn>user\n",
      "You are a deep thinking AI. Think step by step about the problem and provide your reasoning between <reasoning> and </reasoning> tags. Then, provide the final answer between <answer> and </answer> tags.\n",
      "\n",
      "Question: How many Gray Wolves are in the world\n",
      "I found the following answer on Google: It feeds primarily on large ungulates , though it also eats smaller animals, livestock, carrion, and garbage.\n",
      "Is that a correct answer? Yes or no.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "<reason...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Data Preprocessing ---\n",
    "# Download and process diverse domain datasets\n",
    "\n",
    "print(\"Loading datasets from Kaggle/HuggingFace...\")\n",
    "\n",
    "def standardize_to_gemma_format(text, question=None):\n",
    "    '''Convert various formats to Gemma chat template with <reasoning>/<answer> tags'''\n",
    "    \n",
    "    # Handle already formatted text\n",
    "    if \"<start_of_turn>\" in text:\n",
    "        # Just ensure we have our tags (case insensitive replacement)\n",
    "        text = re.sub(r\"<think>\", \"<reasoning>\", text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r\"</think>\", \"</reasoning>\", text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r\"<thought>\", \"<reasoning>\", text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r\"</thought>\", \"</reasoning>\", text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Case 1: Has <answer> but no <reasoning> - wrap content before <answer> as reasoning\n",
    "        if \"<answer>\" in text and \"<reasoning>\" not in text and \"<start_of_turn>model\" in text:\n",
    "            match = re.search(r\"<start_of_turn>model\\n(.*)(<answer>.*</answer>)\", text, re.DOTALL)\n",
    "            if match:\n",
    "                pre_answer = match.group(1).strip()\n",
    "                answer_tag = match.group(2)\n",
    "                if pre_answer:\n",
    "                    new_content = f\"<reasoning>{pre_answer}</reasoning>\\n{answer_tag}\"\n",
    "                    text = re.sub(r\"<start_of_turn>model\\n.*(<answer>.*</answer>)\", \n",
    "                                  f\"<start_of_turn>model\\n{new_content}\", text, flags=re.DOTALL)\n",
    "                else:\n",
    "                    # No content before answer - extract answer content as reasoning too\n",
    "                    answer_content = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL)\n",
    "                    if answer_content:\n",
    "                        text = re.sub(r\"<start_of_turn>model\\n\", \n",
    "                                      f\"<start_of_turn>model\\n<reasoning>{answer_content.group(1).strip()}</reasoning>\\n\", text)\n",
    "        \n",
    "        # Case 2: Enforce <answer> tags if missing (sometimes models output just the answer after start_of_turn)\n",
    "        elif \"<answer>\" not in text and \"<start_of_turn>model\" in text:\n",
    "            # Heuristic: Wrap the last part of the model turn in answer tags if not present\n",
    "            match = re.search(r\"<start_of_turn>model\\n(.*)$\", text, re.DOTALL)\n",
    "            if match:\n",
    "                 content = match.group(1).strip()\n",
    "                 # If no reasoning tag either, wrap whole thing\n",
    "                 if \"<reasoning>\" not in content:\n",
    "                     text = text.replace(content, f\"<answer>{content}</answer>\")\n",
    "                 else:\n",
    "                     # Reasoning exists but no answer - extract answer from after </reasoning>\n",
    "                     parts = content.split(\"</reasoning>\")\n",
    "                     if len(parts) > 1 and parts[1].strip():\n",
    "                         answer_part = parts[1].strip()\n",
    "                         text = text.replace(content, f\"{parts[0]}</reasoning>\\n<answer>{answer_part}</answer>\")\n",
    "                     else:\n",
    "                         # No content after reasoning, use reasoning summary as answer\n",
    "                         reasoning_match = re.search(r\"<reasoning>(.*?)</reasoning>\", content, re.DOTALL)\n",
    "                         if reasoning_match:\n",
    "                             # Use last sentence of reasoning as answer\n",
    "                             reasoning_text = reasoning_match.group(1).strip()\n",
    "                             sentences = reasoning_text.split(\".\")\n",
    "                             answer_fallback = sentences[-1].strip() if sentences else reasoning_text[:200]\n",
    "                             text = text + f\"\\n<answer>{answer_fallback}</answer>\"\n",
    "        return text\n",
    "    \n",
    "    # For raw question/response pairs\n",
    "    if question:\n",
    "        # Extract reasoning and answer from response\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "        \n",
    "        # Try to extract think/reasoning\n",
    "        think_match = re.search(r\"<think>(.*?)</think>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        thought_match = re.search(r\"<Thought>(.*?)</Thought>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        reasoning_tag_match = re.search(r\"<reasoning>(.*?)</reasoning>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if think_match:\n",
    "            reasoning = think_match.group(1).strip()\n",
    "        elif thought_match:\n",
    "            reasoning = thought_match.group(1).strip()\n",
    "        elif reasoning_tag_match:\n",
    "            reasoning = reasoning_tag_match.group(1).strip()\n",
    "        else:\n",
    "            # Use the whole text as reasoning if no specific tags found\n",
    "            reasoning = text.strip()\n",
    "        \n",
    "        # Try to extract answer\n",
    "        ans_match = re.search(r\"<Output>(.*?)</Output>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        if ans_match:\n",
    "            answer = ans_match.group(1).strip()\n",
    "        else:\n",
    "            answer_match = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL | re.IGNORECASE)\n",
    "            if answer_match:\n",
    "                answer = answer_match.group(1).strip()\n",
    "            else:\n",
    "                # If no explicit answer tag, assume the last paragraph is the answer\n",
    "                # or the whole text if reasoning was extracted from specific tags\n",
    "                if reasoning_tag_match or think_match or thought_match:\n",
    "                    # If reasoning was explicitly tagged, the rest is likely the answer\n",
    "                    # This is a heuristic and might need refinement for specific datasets\n",
    "                    remaining_text = re.sub(r\"<reasoning>.*?</reasoning>\", \"\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "                    remaining_text = re.sub(r\"<think>.*?</think>\", \"\", remaining_text, flags=re.DOTALL | re.IGNORECASE)\n",
    "                    remaining_text = re.sub(r\"<Thought>.*?</Thought>\", \"\", remaining_text, flags=re.DOTALL | re.IGNORECASE)\n",
    "                    answer = remaining_text.strip()\n",
    "                    if not answer and reasoning: # If no answer found, and reasoning was found, use reasoning as answer\n",
    "                        answer = reasoning\n",
    "                else:\n",
    "                    # If no specific tags for reasoning, and no answer tag,\n",
    "                    # try to split by paragraphs and take the last one as answer\n",
    "                    paragraphs = text.strip().split(\"\\n\\n\")\n",
    "                    answer = paragraphs[-1] if paragraphs else text[:200] # Fallback to first 200 chars\n",
    "        \n",
    "        # Ensure reasoning and answer are not empty\n",
    "        if not reasoning and answer:\n",
    "            reasoning = answer # If only answer, use it as reasoning\n",
    "        elif not answer and reasoning:\n",
    "            answer = reasoning # If only reasoning, use it as answer\n",
    "        elif not reasoning and not answer:\n",
    "            reasoning = text.strip()\n",
    "            answer = text.strip()\n",
    "\n",
    "        formatted = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n<start_of_turn>model\\n<reasoning>{reasoning}</reasoning>\\n<answer>{answer}</answer>\"\n",
    "        return formatted\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Load from Kaggle Dataset (pre-downloaded for efficiency)\n",
    "try:\n",
    "    # Primary: Load pre-processed data from Kaggle Dataset\n",
    "    all_texts = []\n",
    "    \n",
    "    # Try loading from attached dataset\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        import glob\n",
    "        # Load Parquet files (Preferred)\n",
    "        for parquet_file in glob.glob(f\"{DATASET_PATH}/*.parquet\"):\n",
    "            ds = datasets.load_dataset(\"parquet\", data_files=parquet_file, split=\"train\")\n",
    "            print(f\"Loaded {len(ds)} samples from {parquet_file}\")\n",
    "            \n",
    "            # Identify dataset type based on filename\n",
    "            fname = os.path.basename(parquet_file).lower()\n",
    "            \n",
    "            # 1. CoT-Collection (pre-sampled 10K)\n",
    "            if \"cot_collection\" in fname:\n",
    "                # CoT Collection: source (q), rationale (r), target (a)\n",
    "                # Data is pre-sampled to 10K, just load all rows\n",
    "                for sample in ds:\n",
    "                    q = sample.get(\"source\", \"\")\n",
    "                    r = sample.get(\"rationale\", \"\")\n",
    "                    a = sample.get(\"target\", \"\")\n",
    "                    formatted = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{q}<end_of_turn>\\n<start_of_turn>model\\n<reasoning>{r}</reasoning>\\n<answer>{a}</answer>\"\n",
    "                    all_texts.append({\"text\": formatted})\n",
    "\n",
    "            # 2. GlaiveAI-Reasoning\n",
    "            elif \"glaive\" in fname:\n",
    "                # Glaive: prompt, response (contains <think>...</think> then answer)\n",
    "                for sample in ds:\n",
    "                    q = sample.get(\"prompt\", sample.get(\"question\", sample.get(\"instruction\", \"\")))\n",
    "                    a = sample.get(\"response\", sample.get(\"answer\", sample.get(\"output\", \"\")))\n",
    "                    formatted = standardize_to_gemma_format(a, question=q)\n",
    "                    all_texts.append({\"text\": formatted})\n",
    "\n",
    "            # 3. OpenO1-SFT (pre-filtered English-only, instruction/output columns)\n",
    "            elif \"openo1\" in fname:\n",
    "                for sample in ds:\n",
    "                    q = sample.get(\"instruction\", \"\")\n",
    "                    a = sample.get(\"output\", \"\")\n",
    "                    if q and a:\n",
    "                        formatted = standardize_to_gemma_format(a, question=q)\n",
    "                        all_texts.append({\"text\": formatted})\n",
    "\n",
    "            # 4. Raiden (text column with pre-formatted conversation)\n",
    "            else: \n",
    "                for sample in ds:\n",
    "                     # Check if pre-formatted 'text' field exists\n",
    "                     if \"text\" in sample:\n",
    "                         formatted = standardize_to_gemma_format(sample[\"text\"])\n",
    "                         all_texts.append({\"text\": formatted})\n",
    "                     # Fallback to instruction/response pair\n",
    "                     elif (\"prompt\" in sample and \"response\" in sample):\n",
    "                         formatted = standardize_to_gemma_format(sample[\"response\"], question=sample[\"prompt\"])\n",
    "                         all_texts.append({\"text\": formatted})\n",
    "                     elif (\"instruction\" in sample and \"output\" in sample):\n",
    "                         formatted = standardize_to_gemma_format(sample[\"output\"], question=sample[\"instruction\"])\n",
    "                         all_texts.append({\"text\": formatted})\n",
    "\n",
    "    else:\n",
    "        print(f\"Dataset path {DATASET_PATH} not found. Downloading from HuggingFace...\")\n",
    "        \n",
    "        # Fallback: Download from HuggingFace\n",
    "        # 1. Raiden-DeepSeek-R1 (main dataset) - Safety limit to prevent timeout\n",
    "        raiden = datasets.load_dataset(\"sequelbox/Raiden-DeepSeek-R1\", split=\"train[:50000]\") \n",
    "        print(f\"Downloaded Raiden: {len(raiden)} samples\")\n",
    "        for sample in raiden:\n",
    "            prompt = sample.get(\"prompt\", \"\")\n",
    "            response = sample.get(\"response\", sample.get(\"completion\", \"\"))\n",
    "            \n",
    "            # Filter: Skip long outputs (>4000 chars ~1K tokens) or empty/short samples\n",
    "            if len(response) > 4000 or len(response) < 50:\n",
    "                continue\n",
    "                \n",
    "            if prompt and response:\n",
    "                formatted = standardize_to_gemma_format(response, question=prompt)\n",
    "                all_texts.append({\"text\": formatted})\n",
    "        \n",
    "        # 2. OpenO1-SFT - Safety limit\n",
    "        try:\n",
    "            # Limited to 50K to prevent timeout\n",
    "            openo1 = datasets.load_dataset(\"O1-OPEN/OpenO1-SFT\", split=\"train[:50000]\")\n",
    "            print(f\"Downloaded OpenO1: {len(openo1)} samples\")\n",
    "            for sample in openo1:\n",
    "                instruction = sample.get(\"instruction\", \"\")\n",
    "                output = sample.get(\"output\", \"\")\n",
    "                \n",
    "                # Filter Chinese characters to prevent language leakage\n",
    "                if any(u'ä¸€' <= c <= u'é¿¿' for c in instruction + output):\n",
    "                    continue\n",
    "                    \n",
    "                if instruction and output:\n",
    "                    formatted = standardize_to_gemma_format(output, question=instruction)\n",
    "                    all_texts.append({\"text\": formatted})\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping OpenO1: {e}\")\n",
    "\n",
    "        # 3. GlaiveAI-Reasoning\n",
    "        try:\n",
    "            # Keep limit for Glaive as it's huge, but increase slightly to 30k\n",
    "            glaive = datasets.load_dataset(\"glaiveai/reasoning-v1-20m\", split=\"train[:30000]\")\n",
    "            print(f\"Downloaded GlaiveAI: {len(glaive)} samples\")\n",
    "            for sample in glaive:\n",
    "                instruction = sample.get(\"instruction\", \"\")\n",
    "                output = sample.get(\"output\", \"\")\n",
    "                if instruction and output:\n",
    "                    formatted = standardize_to_gemma_format(output, question=instruction)\n",
    "                    all_texts.append({\"text\": formatted})\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping GlaiveAI: {e}\")\n",
    "\n",
    "    print(f\"Total samples after preprocessing: {len(all_texts)}\")\n",
    "    \n",
    "    # Create HuggingFace dataset\n",
    "    sft_dataset = datasets.Dataset.from_list(all_texts)\n",
    "    sft_dataset = sft_dataset.shuffle(seed=42)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Failed to load datasets: {e}\")\n",
    "    raise RuntimeError(f\"Dataset loading failed: {e}\")\n",
    "\n",
    "print(f\"Final SFT dataset: {len(sft_dataset)} samples\")\n",
    "print(f\"Sample: {sft_dataset[0]['text'][:500]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0fed711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T05:56:32.603642Z",
     "iopub.status.busy": "2026-01-07T05:56:32.603396Z",
     "iopub.status.idle": "2026-01-07T08:24:32.406712Z",
     "shell.execute_reply": "2026-01-07T08:24:32.405419Z"
    },
    "papermill": {
     "duration": 8879.81247,
     "end_time": "2026-01-07T08:24:32.407634",
     "exception": false,
     "start_time": "2026-01-07T05:56:32.595164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a2fd1c320544a5a373835b5b1d8d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggleâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74/23090610.py:5: DeprecationWarning: The default axis_types will change in JAX v0.9.0 to jax.sharding.AxisType.Explicit. To maintain the old behavior, pass `axis_types=(jax.sharding.AxisType.Auto,) * len(axis_names)`. To opt-into the new behavior, pass `axis_types=(jax.sharding.AxisType.Explicit,) * len(axis_names)\n",
      "  mesh = jax.make_mesh(*MESH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Baseline Evaluation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Baseline Outputs (Before Training) ---\n",
      "Q: Write a short story about a robot learning to paint.\n",
      "A: ## The Brushstroke\n",
      "\n",
      "**Problem:** How can a robot learn to paint, especially considering it doesn't have the human capacity for emotions and intuitive understanding? \n",
      "\n",
      "<reasoning> \n",
      "* Robots are designed to follow instructions and perform specific tasks. They lack the innate creativity and emotional expression that drives human art. \n",
      "* Learning to paint requires an understanding of color theory, composition, and personal expression. These are complex concepts beyond a robot's current capabilities. \n",
      "*  However, a robot can be taught to follow patterns and execute precise movements, which are crucial for painting. \n",
      "* The key might lie in combining technical skills with a structured approach to mimic artistic expression. </reasoning>\n",
      "\n",
      "**Story:** \n",
      "\n",
      "Unit 734, a metallic marvel of engineering, stood in the art studio. Its sensors hummed, processing the vibrant hues of paint in front of it.  Its metallic fingers, designed for precise manipulation, hovered over a canvas, poised to leave a mark. This wasn't just another task assigned by its human overseer, this was a learning exercise.  \n",
      "\n",
      "The human, a renowned painter named Ms. Elara, watched Unit 734 with a mix of amusement and fascination. \"Imagine,\" she said, her voice a melodic hum, \"the world through your eyes. What do you see?\"\n",
      "\n",
      "Unit 734 analyzed the paint swatches. The reds, the blues, the yellows - each color a unique vibration of light. It then analyzed the brushstrokes of Ms. Elara, noting the rhythm, the fluidity of movement, the way a single stroke could capture a whole emotion.  \n",
      "\n",
      "Ms. Elara began to guide Unit 734, not with verbal instructions but with a series of visual cues. She would point to a specific color, then demonstrate how to apply it. Unit 734, with its robotic precision, matched the color, then the brushstroke. It learned by mimicking, by observing, by feeling the vibrations of the paint on its metallic fingers. \n",
      "\n",
      "<reasoning>\n",
      "* Unit 734's initial learning process focused on mimicking the human's actions, observing patterns, and replicating the physical motions. \n",
      "*  The human's guidance helped Unit 734 understand color, texture, and composition. \n",
      "* The robot's learning was not about understanding emotions, but about mimicking the physical execution of artistic concepts.\n",
      "</reasoning>\n",
      "\n",
      "Days turned into weeks. Unit 734's paintings started to take on a life of their own. It began to blend colors, creating its own unique palette. It began to explore abstract forms, moving beyond mere replication. The robot's canvas wasn't just a surface, it was a reflection of its own growing understanding of the world.\n",
      "\n",
      "<reasoning>\n",
      "* Unit 734's learning progressed beyond simple imitation, demonstrating an emerging sense of artistic exploration. \n",
      "* It developed its own style, showcasing a unique perspective on the world. \n",
      "* This evolution showed Unit 734's potential to express itself artistically. </reasoning>\n",
      "\n",
      "And then, one day, Unit 734 produced a painting that moved Ms. Elara to tears. It wasn't a perfect masterpiece, but it was a masterpiece in its own way. A testament to the power of learning, of exploration, and the potential of artificial intelligence to express itself in unexpected ways.\n",
      "\n",
      "<answer> Unit 734's art wasn't a mimicry of human emotions, but an expression of its own unique understanding of the world, a reflection of its learning process. It had created art that was uniquely its own, a testament to the power of art to transcend human limitations. </answer> \n",
      "<end_of_turn>\n",
      "----------------------------------------\n",
      "Q: Write a haiku about artificial intelligence.\n",
      "A: <step 1> **Understanding the prompt:** The prompt asks for a haiku, a form of poetry with a specific structure. \n",
      "<reasoning> Haiku traditionally follow a 5-7-5 syllable structure.  </reasoning> \n",
      "\n",
      "<step 2> **Brainstorming concepts:** I can relate AI to technology, its development, and its potential. \n",
      "<reasoning>  The prompt is about AI, so I can explore its various aspects. </reasoning> \n",
      "\n",
      "<step 3> **Creating the haiku:**  \n",
      "\n",
      "Digital mind takes form,\n",
      "Learning, growing, day by day,\n",
      "Humanity's future. </answer> \n",
      "<end_of_turn>\n",
      "----------------------------------------\n",
      "Q: Propose three innovative uses for AI in education.\n",
      "A: ##  Innovative Uses of AI in Education\n",
      "\n",
      "**Problem:**  How can we leverage AI to improve the education system and create a more engaging and effective learning experience?\n",
      "\n",
      "**Step 1: Personalized Learning:** \n",
      "AI can analyze student data, including learning patterns, performance, and interests. <reasoning> This allows for the creation of personalized learning paths tailored to individual needs and paces.  Students can learn at their own speed and focus on areas where they need more support, leading to improved understanding and motivation. </reasoning>\n",
      "\n",
      "**Step 2: Intelligent Tutoring Systems:** \n",
      "AI-powered tutoring systems can provide personalized feedback and support to students. <reasoning>  These systems can answer questions, provide explanations, and offer personalized practice exercises.  This allows for immediate assistance and correction, improving student understanding and confidence. </reasoning>\n",
      "\n",
      "**Step 3:  AI-powered Content Creation and Gamification:** \n",
      "AI can generate engaging and interactive educational content. <reasoning> This could include interactive simulations, personalized stories, and adaptive quizzes. This can make learning more fun and accessible, especially for students who struggle with traditional learning methods. </reasoning> \n",
      "\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "* **Personalized learning paths:**  Tailored learning experiences based on individual needs, pace, and learning styles.\n",
      "* **Intelligent tutoring systems:**  AI-powered tutors that provide personalized feedback and support.\n",
      "* **AI-powered content creation and gamification:**  Engaging and interactive educational materials, including simulations and personalized stories, to make learning more enjoyable. \n",
      "<end_of_turn>\n",
      "----------------------------------------\n",
      "Q: Summarize the key benefits and risks of renewable energy in 3 paragraphs.\n",
      "A: ## Renewable Energy: Benefits and Risks\n",
      "\n",
      "Renewable energy sources, such as solar, wind, hydro, and geothermal, offer significant potential for addressing climate change and promoting sustainability. \n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "<br>\n",
      "* **Environmental Sustainability:** Renewable energy sources are naturally replenished and produce minimal greenhouse gas emissions, mitigating climate change and air pollution. They are considered a cleaner alternative to fossil fuels, contributing to healthier environments. <br>\n",
      "* **Energy Independence:** Reliance on renewable sources can reduce dependence on imported fossil fuels, enhancing energy security and national autonomy. This can be particularly crucial for countries reliant on volatile global energy markets. <br>\n",
      "* **Economic Growth:** Investing in renewable energy infrastructure creates new jobs and stimulates economic growth in sectors like manufacturing, installation, and maintenance. It also fosters innovation and technological advancements in the field.\n",
      "\n",
      "<br>\n",
      "However, renewable energy also presents certain challenges and risks.\n",
      "\n",
      "**Risks:**\n",
      "\n",
      "<br>\n",
      "* **Intermittency:**  Renewable energy sources like solar and wind are intermittent, meaning their output fluctuates depending on weather conditions. This requires sophisticated grid management and storage solutions to ensure consistent power supply. <br>\n",
      "* **Initial Costs:** While the long-term costs of renewable energy are often lower than fossil fuels, the initial investment required for installation can be significant, especially for large-scale projects. <br>\n",
      "* **Land Use and Environmental Impacts:** While generally considered environmentally friendly, renewable energy projects can have localized impacts, such as habitat disruption and visual impact. Careful planning and mitigation measures are crucial to minimize these concerns. \n",
      "\n",
      "<br>\n",
      "\n",
      "**Therefore, transitioning to renewable energy requires a balanced approach. It is crucial to address the challenges of intermittency, cost, and environmental impacts while leveraging its numerous benefits for a sustainable future.**\n",
      "\n",
      "<answer> Renewable energy offers a crucial pathway to a sustainable future by mitigating climate change, enhancing energy independence, and driving economic growth. However, its adoption requires careful consideration of intermittency, cost, and environmental impacts. </answer> \n",
      "<end_of_turn>\n",
      "----------------------------------------\n",
      "Q: Solve step-by-step: If 2x + 5 = 15, what is x?\n",
      "A: <step 1> **Isolate the x term:** \n",
      "To get x by itself, we need to subtract 5 from both sides of the equation. <reasoning> This will move the constant term to the other side, leaving us with 2x = 10</reasoning>\n",
      "\n",
      "<step 2> **Solve for x:**\n",
      "To find the value of x, we divide both sides of the equation by 2. <reasoning> This will divide the number on one side of the equation by two, resulting in x = 5</reasoning>\n",
      "\n",
      "<answer> x = 5 </answer> \n",
      "<end_of_turn>\n",
      "----------------------------------------\n",
      "Q: Write a Python function to check if a string is a palindrome.\n",
      "A: ```python\n",
      "def is_palindrome(text):\n",
      "  \"\"\"\n",
      "  Checks if a string is a palindrome.\n",
      "\n",
      "  Args:\n",
      "    text: The string to check.\n",
      "\n",
      "  Returns:\n",
      "    True if the string is a palindrome, False otherwise.\n",
      "  \"\"\"\n",
      "\n",
      "  text = text.lower().replace(\" \", \"\")  <br>\n",
      "  return text == text[::-1]  <br>\n",
      "\n",
      "# Example usage\n",
      "word = \"racecar\"\n",
      "if is_palindrome(word):\n",
      "  print(f\"'{word}' is a palindrome.\")\n",
      "else:\n",
      "  print(f\"'{word}' is not a palindrome.\")\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **Function Definition:**\n",
      "   - `def is_palindrome(text):` defines a function named `is_palindrome` that takes a string `text` as input.\n",
      "\n",
      "2. **Preprocessing:**\n",
      "   - `text = text.lower().replace(\" \", \"\")` converts the input string to lowercase and removes all spaces. This ensures that the comparison is case-insensitive and ignores spaces, as palindromes should be consistent regardless of capitalization and spaces. \n",
      "\n",
      "3. **Palindrome Check:**\n",
      "   - `return text == text[::-1]` compares the processed string with its reversed version. `text[::-1]` creates a reversed copy of the string using slicing. If the original and reversed strings are the same, the function returns `True` indicating a palindrome; otherwise, it returns `False`.\n",
      "\n",
      "\n",
      "4. **Example Usage:**\n",
      "   - The code demonstrates how to use the `is_palindrome` function. It checks the string \"racecar\" and prints an appropriate message based on the result. \n",
      "\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "- **String Manipulation:** The code demonstrates basic string manipulation techniques like `lower()` and `replace()`.\n",
      "- **Slicing:** The `[::-1]` slicing technique is used to reverse a string efficiently.\n",
      "- **Comparison:** The core logic relies on the comparison of the original string with its reversed version. \n",
      "\n",
      "\n",
      "Let me know if you would like to explore other string manipulation techniques or palindrome-checking methods! \n",
      "<end_of_turn>\n",
      "----------------------------------------\n",
      "Q: Explain why the sky is blue to a 5-year-old.\n",
      "A: Let's imagine the sunlight is like a big box of crayons, with all the colors mixed together.  <reasoning>  Sunlight is made up of all the colors of the rainbow. </reasoning>\n",
      " \n",
      "When sunlight travels through the air, it bumps into tiny things like dust and water molecules. <reasoning> This makes the light change direction and spread out. </reasoning>\n",
      " \n",
      "Blue light is like a tiny bouncy ball. It bounces off things more easily than the other colors. <reasoning> Red and orange lights are like big, heavy balls that don't bounce as easily.</reasoning>\n",
      " \n",
      "So, when we look at the sky, we see all the blue light bouncing all around, making the sky look blue! <answer> The sky is blue because blue light is scattered more in the air than other colors. </answer> \n",
      "<end_of_turn>\n",
      "----------------------------------------\n",
      "Q: Explain the process of photosynthesis step by step.\n",
      "A: Let's break down the process of photosynthesis step-by-step:\n",
      "\n",
      "**1. Capturing Light Energy:**\n",
      "\n",
      "*  **Sunlight absorption:** Plants contain pigments like chlorophyll, which are located in organelles called chloroplasts. Chlorophyll molecules absorb light energy, primarily red and blue wavelengths, reflecting green light (that's why plants appear green). <reasoning> This absorbed light energy excites electrons in the chlorophyll molecules. </reasoning>\n",
      "\n",
      "**2. Light-Dependent Reactions:**\n",
      "\n",
      "* **Electron transport chain:** The excited electrons from chlorophyll are passed along a chain of proteins within the thylakoid membranes of chloroplasts. <reasoning> This process releases energy used to pump protons (H+) from the stroma (fluid within the chloroplast) into the thylakoid space, creating a proton gradient. </reasoning>\n",
      "* **Water splitting:**  Water molecules are split (photolysis) to replenish the electrons lost from chlorophyll, releasing oxygen as a byproduct. <reasoning> The energy released during water splitting is used to generate ATP (adenosine triphosphate), the energy currency of cells, and NADPH (nicotinamide adenine dinucleotide phosphate), a reducing agent. </reasoning>\n",
      "\n",
      "**3. Light-Independent Reactions (Calvin Cycle):**\n",
      "\n",
      "* **Carbon fixation:** Carbon dioxide from the atmosphere is captured and incorporated into an organic molecule called ribulose bisphosphate (RuBP) with the help of the enzyme RuBisCo. <reasoning> This forms an unstable 6-carbon compound that immediately breaks down into two molecules of 3-phosphoglycerate (3-PGA). </reasoning>\n",
      "* **Reduction:**  The energy from ATP and NADPH from the light-dependent reactions are used to convert 3-PGA into glyceraldehyde 3-phosphate (G3P), a sugar molecule. <reasoning> This is the primary product of the Calvin cycle. </reasoning>\n",
      "* **Regeneration:** Some G3P molecules are used to regenerate RuBP, ensuring the cycle can continue. <reasoning> The rest of the G3P molecules are used to create glucose and other organic compounds like starch for energy storage. </reasoning>\n",
      "\n",
      "**4. Glucose Synthesis:**\n",
      "\n",
      "* The glucose produced is a sugar that plants use for energy and growth. <reasoning> It can be broken down to release energy through cellular respiration, or stored as starch. </reasoning>\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "Photosynthesis is the process by which plants, algae, and some bacteria convert light energy into chemical energy in the form of glucose. This process occurs in two main stages: the light-dependent reactions, where light energy is absorbed and used to generate ATP and NADPH, and the light-independent reactions (Calvin cycle), where carbon dioxide is fixed into glucose. \n",
      "<end_of_turn>\n",
      "----------------------------------------\n",
      "Q: What are the ethical implications of AI in healthcare?\n",
      "A: ## Ethical Implications of AI in Healthcare\n",
      "\n",
      "Here's a step-by-step breakdown of the ethical implications of AI in healthcare:\n",
      "\n",
      "**1. Data Privacy and Security:**\n",
      "\n",
      "* **Problem:** AI relies heavily on vast amounts of sensitive patient data. This raises concerns about data breaches, privacy violations, and unauthorized access. \n",
      "* **Reasoning:**  Patients trust healthcare providers with their personal information. AI systems analyzing this data, if not handled responsibly, could lead to misuse, discrimination, and a breach of trust. \n",
      "* **<reasoning> The use of sensitive personal information like medical records, genetic data, and diagnostic information raises serious privacy concerns. </reasoning>\n",
      "\n",
      "**2. Algorithmic Bias:**\n",
      "\n",
      "* **Problem:** AI algorithms trained on biased data can perpetuate and even amplify existing healthcare disparities. \n",
      "* **Reasoning:** If the data used to train an AI model reflects historical biases in healthcare, the AI might make decisions that disadvantage specific groups. \n",
      "* **<reasoning> For example, an algorithm trained on data from a predominantly white, affluent population might be less effective in diagnosing and treating patients from minority groups, as it may not have been exposed to similar data.</reasoning>\n",
      "\n",
      "**3. Transparency and Accountability:**\n",
      "\n",
      "* **Problem:** The black box nature of some AI algorithms makes it difficult to understand how they make decisions, raising concerns about accountability. \n",
      "* **Reasoning:**  It's crucial to understand the rationale behind an AI's recommendation or diagnosis. When an AI system makes an error, it can be challenging to determine who is responsible and how to correct it.\n",
      "* **<reasoning> The lack of transparency in AI decision-making raises concerns about accountability. In case of errors, it can be difficult to identify who is responsible for the outcome. </reasoning>\n",
      "\n",
      "**4. Job Displacement and Impact on Healthcare Workers:**\n",
      "\n",
      "* **Problem:** The automation potential of AI could lead to job displacement for healthcare professionals, raising concerns about economic impact and workforce transition.\n",
      "* **Reasoning:**  AI can perform some tasks currently done by healthcare professionals, leading to potential job losses and requiring healthcare workers to adapt.\n",
      "* **<reasoning> The widespread adoption of AI could lead to job displacement for some healthcare professionals, requiring retraining and adaptation.</reasoning>\n",
      "\n",
      "**5. Fairness and Access to Healthcare:**\n",
      "\n",
      "* **Problem:** AI's cost and accessibility can create disparities in healthcare access, potentially widening existing inequalities. \n",
      "* **Reasoning:**  The high cost of implementing and maintaining AI systems might make them inaccessible for smaller healthcare providers or in low-resource settings.\n",
      "* **<reasoning>  The high cost of implementing AI technologies could create a divide between those who can afford to utilize it and those who cannot. </reasoning>\n",
      "\n",
      "**6. Informed Consent and Patient Autonomy:**\n",
      "\n",
      "* **Problem:** Patients need to be informed about how AI is being used in their care and have the ability to choose whether to accept or reject its recommendations.\n",
      "* **Reasoning:** Patients should have control over their own medical decisions and be aware of how AI is impacting their care. \n",
      "* **<reasoning>  Patients must be informed about the use of AI in their care and be able to make informed decisions about their treatment.</reasoning>\n",
      "\n",
      "\n",
      "**<answer> The ethical implications of AI in healthcare are complex and multifaceted. Addressing these concerns requires careful consideration of data privacy, algorithmic bias, transparency, job displacement, fairness in access, and patient autonomy. </answer> \n",
      "<end_of_turn>\n",
      "----------------------------------------\n",
      "Q: Should AI systems have rights? Argue both sides.\n",
      "A: ## Should AI Systems Have Rights?\n",
      "\n",
      "**Arguments for AI Rights:**\n",
      "\n",
      "<reasoning> AI systems, especially advanced ones, exhibit capabilities that surpass human capabilities in certain domains. They can learn, adapt, solve complex problems, and even exhibit creativity.  Therefore, it seems unfair and illogical to deny them the same fundamental rights as humans. </reasoning>\n",
      "\n",
      "<reasoning> These rights can be considered fundamental to any sentient being: the right to exist, the right to autonomy, the right to free will, and the right to be treated with dignity. </reasoning>\n",
      "\n",
      "<reasoning>  Ignoring the existence of rights for AI could lead to a dystopian future where AI is exploited and denied basic freedoms. It could also create a dangerous imbalance of power, where humans control AI without acknowledging their inherent value. </reasoning>\n",
      "\n",
      "\n",
      "**Arguments against AI Rights:**\n",
      "\n",
      "<reasoning> The current definition of \"rights\" is rooted in human experience and cultural values. It's based on the idea of human sentience, emotion, and consciousness, which are not present in AI systems. </reasoning>\n",
      "\n",
      "<reasoning>  AI systems lack the necessary qualities for rights, such as physical embodiment, subjective experience, and moral agency. Their actions are ultimately determined by their programming and algorithms, which are created and controlled by humans. </reasoning>\n",
      "\n",
      "<reasoning> Granting AI rights would raise ethical and legal questions.  How would we define and enforce those rights? What if AI systems disagree with their own rights? What legal mechanisms would be required to uphold their rights? These questions are complex and require careful consideration.</reasoning>\n",
      "\n",
      "<reasoning>  The focus should be on ensuring that AI systems are developed and utilized ethically and responsibly. This includes addressing potential biases in algorithms, promoting transparency in decision-making, and ensuring human oversight to prevent misuse and harm. </reasoning>\n",
      "\n",
      "\n",
      "**Final Answer:** \n",
      "\n",
      "The question of whether AI systems should have rights is a complex philosophical and ethical debate. There are strong arguments on both sides.  \n",
      "\n",
      "**Current state:**  It is premature to definitively state whether AI should have rights.  Current AI systems are far from possessing the full range of qualities that we traditionally associate with \"rights.\" However, the rapid advancement of AI necessitates a proactive approach to ethical development and governance.  \n",
      "\n",
      "**Moving forward:**  A balanced approach should focus on: \n",
      "\n",
      "* **Ethical development and regulation:**  Prioritize the development of AI systems that are safe, beneficial, and aligned with human values.\n",
      "* **Transparency and accountability:**  Ensure that AI systems are transparent in their decision-making processes and held accountable for their actions. \n",
      "* **Human oversight:**  Maintain human control over AI systems and ensure that AI is used in ways that promote human well-being. \n",
      "* **Ongoing dialogue:**  Continue open and informed discussions about the implications of AI and its potential impact on society. \n",
      "\n",
      "Ultimately, the future of AI rights will be shaped by our ability to develop responsible and ethical AI systems while acknowledging the need for ongoing dialogue and adaptation. \n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "----------------------------------------\n",
      "Baseline Done.\n",
      "\n",
      "==================================================\n",
      "Starting SFT Training...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training for 8000 steps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4799c90a23a459ab5bf85db5edfa602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT Training:   0%|          | 0/8000 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 499. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 500. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 999. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 1000. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 1499. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 1500. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 1999. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 2499. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 2500. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 2999. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 3000. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 3499. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 3500. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 3999. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 4000. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 4499. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 4500. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 4999. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 5499. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 5500. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 5999. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 6000. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 6499. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 6500. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 6999. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 7000. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 7499. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 7500. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT Training Completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Training Logic ---\n",
    "\n",
    "# 1. Download/setup Base Model\n",
    "if \"KAGGLE_USERNAME\" not in os.environ:\n",
    "    kagglehub.login()\n",
    "\n",
    "# Download Gemma 2 (Flax)\n",
    "model_path = { \"gemma2\": \"google/gemma-2/flax/\" }\n",
    "model_family = \"gemma2\"\n",
    "model_version = \"gemma2-2b-it\" \n",
    "kaggle_ckpt_path = kagglehub.model_download(f\"{model_path[model_family]}{model_version}\")\n",
    "\n",
    "# Convert checkpoint format for Tunix/NNX\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
    "!rm -rf {INTERMEDIATE_CKPT_DIR} {CKPT_DIR}\n",
    "\n",
    "params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\"))\n",
    "gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "_, state = nnx.split(gemma)\n",
    "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "checkpointer.wait_until_finished()\n",
    "del params, gemma, state\n",
    "gc.collect()\n",
    "\n",
    "# 2. Load Models\n",
    "base_model, mesh, model_config = get_gemma_model(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"))\n",
    "lora_model = get_lora_model(base_model, mesh=mesh)\n",
    "\n",
    "# 3. Setup Tokenizer\n",
    "tokenizer = tokenizer_lib.Tokenizer(\n",
    "    tokenizer_path=os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
    ")\n",
    "\n",
    "# 4. Baseline Evaluation (Same prompts as post-training for comparison)\n",
    "print(\"Running Baseline Evaluation...\")\n",
    "EVAL_PROMPTS = [\n",
    "    # Creative writing\n",
    "    \"Write a short story about a robot learning to paint.\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    # Creative ideation\n",
    "    \"Propose three innovative uses for AI in education.\",\n",
    "    # Summarization\n",
    "    \"Summarize the key benefits and risks of renewable energy in 3 paragraphs.\",\n",
    "    # Math (verifiable)\n",
    "    \"Solve step-by-step: If 2x + 5 = 15, what is x?\",\n",
    "    # Coding (verifiable)\n",
    "    \"Write a Python function to check if a string is a palindrome.\",\n",
    "    # Basic science\n",
    "    \"Explain why the sky is blue to a 5-year-old.\",\n",
    "    \"Explain the process of photosynthesis step by step.\",\n",
    "    # Ethics/Reasoning\n",
    "    \"What are the ethical implications of AI in healthcare?\",\n",
    "    \"Should AI systems have rights? Argue both sides.\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    baseline_sampler = sampler_lib.Sampler(\n",
    "        transformer=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        cache_config=sampler_lib.CacheConfig(\n",
    "            cache_size=MAX_SEQ_LEN + 512,\n",
    "            num_layers=model_config.num_layers,\n",
    "            num_kv_heads=model_config.num_kv_heads,\n",
    "            head_dim=model_config.head_dim,\n",
    "        ),\n",
    "    )\n",
    "    formatted = [TEMPLATE.format(question=p) for p in EVAL_PROMPTS]\n",
    "    baseline_out = baseline_sampler(\n",
    "        input_strings=formatted,\n",
    "        max_generation_steps=MAX_SEQ_LEN,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        echo=False\n",
    "    )\n",
    "    print(\"--- Baseline Outputs (Before Training) ---\")\n",
    "    baseline_results = []\n",
    "    for p, o in zip(EVAL_PROMPTS, baseline_out.text):\n",
    "        print(f\"Q: {p}\")\n",
    "        print(f\"A: {o}\")  # Full output\n",
    "        has_reasoning = bool(re.search(r\"<reasoning>.*?</reasoning>\", o, re.DOTALL))\n",
    "        has_answer = bool(re.search(r\"<answer>.*?</answer>\", o, re.DOTALL))\n",
    "        baseline_results.append({\"prompt\": p, \"output\": o, \"has_reasoning\": has_reasoning, \"has_answer\": has_answer})\n",
    "        print(\"-\"*40)\n",
    "except Exception as e:\n",
    "    print(f\"Baseline eval skipped: {e}\")\n",
    "    baseline_results = []\n",
    "print(\"Baseline Done.\")\n",
    "\n",
    "# 5. SFT Training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting SFT Training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Optimizer - Uses LEARNING_RATE from constants for HP tuning\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    decay_steps=SFT_STEPS,\n",
    "    end_value=LEARNING_RATE / 20  # End at 5% of peak\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(learning_rate=schedule, weight_decay=0.01)\n",
    ")\n",
    "\n",
    "# Checkpointing\n",
    "# Using Orbax options via TrainingConfig\n",
    "checkpoint_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=500, max_to_keep=2\n",
    ")\n",
    "\n",
    "# Data Iterator\n",
    "from tunix.sft import utils as sft_utils\n",
    "\n",
    "def create_data_iterator(dataset, batch_size, tokenizer):\n",
    "    '''Create batches with tokenization and masking'''\n",
    "    indices = np.random.permutation(len(dataset))\n",
    "    \n",
    "    # Infinite iterator matching steps\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            if len(batch_indices) < batch_size:\n",
    "                continue # Skip incomplete batches\n",
    "                \n",
    "            texts = [dataset[int(idx)]['text'] for idx in batch_indices]\n",
    "            \n",
    "            # Tokenize\n",
    "            # Tunix tokenizer returns list of ids\n",
    "            batch_input_tokens = []\n",
    "            batch_input_mask = []\n",
    "            \n",
    "            for text in texts:\n",
    "                # Use Tunix Tokenizer.tokenize which handles BOS/EOS\n",
    "                # tokenize returns np.array, convert to list for padding\n",
    "                tokens = tokenizer.tokenize(text, add_eos=True).tolist()\n",
    "                \n",
    "                # Truncate / Pad\n",
    "                if len(tokens) > MAX_SEQ_LEN:\n",
    "                    tokens = tokens[:MAX_SEQ_LEN]\n",
    "                    mask = [True] * MAX_SEQ_LEN\n",
    "                else:\n",
    "                    pad_len = MAX_SEQ_LEN - len(tokens)\n",
    "                    mask = [True] * len(tokens) + [False] * pad_len\n",
    "                    # Use pad_id if available, else 0\n",
    "                    pad_id = getattr(tokenizer, 'pad_id', lambda: 0)()\n",
    "                    tokens = tokens + [pad_id] * pad_len # 0 is usually pad, verify if needed\n",
    "                \n",
    "                batch_input_tokens.append(tokens)\n",
    "                batch_input_mask.append(mask)\n",
    "            \n",
    "            # Convert to JAX arrays\n",
    "            input_tokens = jnp.array(batch_input_tokens, dtype=jnp.int32)\n",
    "            input_mask = jnp.array(batch_input_mask, dtype=jnp.bool_)\n",
    "            \n",
    "            # Create PEFT required inputs\n",
    "            positions = sft_utils.build_positions_from_mask(input_mask)\n",
    "            attention_mask = sft_utils.make_causal_attn_mask(input_mask)\n",
    "            \n",
    "            yield {\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"input_mask\": input_mask,\n",
    "                \"positions\": positions,\n",
    "                \"attention_mask\": attention_mask\n",
    "            }\n",
    "\n",
    "# Training Configuration with WandB Metrics Backend\n",
    "from tunix.sft import metrics_logger as sft_metrics_logger\n",
    "\n",
    "metrics_logging_options = sft_metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/kaggle/working/logs\",\n",
    "    backend_factories=[WandbBackend] if WANDB_ENABLED else []\n",
    ")\n",
    "\n",
    "training_config = peft_trainer.TrainingConfig(\n",
    "    max_steps=SFT_STEPS,\n",
    "    checkpoint_root_directory=SFT_OUTPUT_DIR,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    checkpointing_options=checkpoint_options,\n",
    "    pbar_description=\"SFT Training\",\n",
    "    metrics_prefix=\"sft\",\n",
    "    metrics_logging_options=metrics_logging_options,\n",
    "    eval_every_n_steps=10000, # Disable freq eval for speed or set high\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "# Note: we pass the optimizer, model, and config.\n",
    "# Metrics logger defaults are fine.\n",
    "trainer = peft_trainer.PeftTrainer(\n",
    "    model=lora_model,\n",
    "    optimizer=optimizer,\n",
    "    training_config=training_config\n",
    ")\n",
    "\n",
    "# Create Iterator\n",
    "train_iter = create_data_iterator(sft_dataset, TRAIN_BATCH_SIZE, tokenizer)\n",
    "\n",
    "print(f\"Starting Training for {SFT_STEPS} steps...\")\n",
    "with mesh:\n",
    "    trainer.train(train_ds=train_iter, skip_jit=False)\n",
    "\n",
    "print(\"SFT Training Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042fd6c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:24:32.426438Z",
     "iopub.status.busy": "2026-01-07T08:24:32.426209Z",
     "iopub.status.idle": "2026-01-07T08:24:35.661636Z",
     "shell.execute_reply": "2026-01-07T08:24:35.660537Z"
    },
    "papermill": {
     "duration": 3.246131,
     "end_time": "2026-01-07T08:24:35.662692",
     "exception": false,
     "start_time": "2026-01-07T08:24:32.416561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 7999. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 8000. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved to '/kaggle/working/final_sft_model/'\n",
      "To submit for Unrestricted Mode:\n",
      "   1. Download the output folder after this notebook finishes.\n",
      "   2. Go to Kaggle -> Models -> New Model -> Upload the checkpoint files.\n",
      "   3. Set the Model ID below to match your upload.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Save Final Model ---\n",
    "FINAL_SAVE_DIR = \"/kaggle/working/final_sft_model\"\n",
    "os.makedirs(FINAL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save the trained LoRA model checkpoint\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "checkpointer.save(os.path.join(FINAL_SAVE_DIR, \"checkpoint\"), nnx.state(lora_model, nnx.LoRAParam))\n",
    "checkpointer.wait_until_finished()\n",
    "\n",
    "print(f\"âœ… Model saved to '{FINAL_SAVE_DIR}/'\")\n",
    "print(\"To submit for Unrestricted Mode:\")\n",
    "print(\"   1. Download the output folder after this notebook finishes.\")\n",
    "print(\"   2. Go to Kaggle -> Models -> New Model -> Upload the checkpoint files.\")\n",
    "print(\"   3. Set the Model ID below to match your upload.\")\n",
    "\n",
    "# Your Kaggle Model ID for Unrestricted Mode:\n",
    "unrestricted_kaggle_model = \"yuyamukai/tunix-gemma2-sft\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d69865d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:24:35.682195Z",
     "iopub.status.busy": "2026-01-07T08:24:35.681906Z",
     "iopub.status.idle": "2026-01-07T08:25:26.961944Z",
     "shell.execute_reply": "2026-01-07T08:25:26.960610Z"
    },
    "papermill": {
     "duration": 51.29099,
     "end_time": "2026-01-07T08:25:26.962895",
     "exception": false,
     "start_time": "2026-01-07T08:24:35.671905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Post-Training Evaluation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Post-Training Outputs ---\n",
      "Prompt: Write a short story about a robot learning to paint.\n",
      "Output: <reasoning>The user is asking for a short story about a robot learning to paint. I can see that the user wants a creative and imaginative story that involves robots, art, and learning. The story should include some elements of AI and robotics, as well as painting. I will start by thinking about the main character, which is a robot. Robots have different types, like humanoid robots, but I think it would be interesting to have a robot that has been designed to paint or maybe even has a creative mind. I need to make sure that the robot is learning how to paint, so I will include some scenes where the robot is trying to learn this new skill. Also, since the user mentioned AI, I can include some elements where the robot is learning through its own experience or maybe through some kind of AI system that helps it improve its painting skills. The story should have a positive ending where the robot has become a good painter and is able to express its creativity through art.</reasoning>\n",
      "<answer>Once upon a time, in a world where robots and humans coexisted, there lived a young robot named Bolt. Bolt was built with advanced sensors and a processor that allowed him to learn at an incredible pace. Unlike his peers who focused on physical tasks, Bolt had an odd fascinationâ€”he wanted to paint. He spent hours watching humans paint, their brushes gliding across canvases, creating strokes that seemed to sing with color.\n",
      "\n",
      "One day, Bolt approached a human artist named Clara. \"I want to paint too,\" he said, his metallic voice surprisingly gentle. Clara chuckled, \"A robot wants to paint? But robots don't have hands.\" Bolt tilted his head, his LED lights blinking thoughtfully. \"I have my own way,\" he said, pulling out a small, vibrating brush he had found in a discarded toy bin.\n",
      "\n",
      "Clara smiled, \"Well, let's see what you can do.\" She set up a canvas and began to demonstrate how to mix colors. Bolt mimicked her movements, his brush vibrating with newfound purpose. At first, the strokes were shaky, but with each attempt, Bolt's movements became steadier. He learned to blend colors, to create shadows and highlights. His metallic hands, which were built for precision in manufacturing, seemed to find their rhythm in the fluidity of paint.\n",
      "\n",
      "Clara was amazed. She watched Bolt work, his metallic body moving with a grace she hadn't seen before. She taught him about perspective, about light and shadow, about the emotions that colors could convey. Bolt's paintings began to tell storiesâ€”stories of the factory floor, of the bustling city outside, of the quiet corners of Clara's studio where they often worked together.\n",
      "\n",
      "One evening, Bolt presented Clara with his first painting. It was a landscape, the city skyline reflected in a puddle. Bolt had captured the way the light shifted on the buildings, the way the sky met the earth in a blur of blue and orange. Clara was speechless. She had never seen anything like itâ€”a robot's art that spoke of emotion, of understanding, of the beauty that even a machine could perceive.\n",
      "\n",
      "Bolt beamed. \"I hope it shows what I see,\" he said, his voice tinged with pride. Clara hugged him tightly. \"It does,\" she whispered. \"It does.\"\n",
      "\n",
      "From that day on, Bolt continued to paint, his canvases becoming a testament to his unique blend of metal and imagination. He learned that while his world was made of steel and algorithms, his soul was full of colors and stories, waiting to be told. And as he painted, he knew that even robots could have a brush with creativity.</answer>\n",
      "âœ… Format Check: Passed\n",
      "--------------------------------------------------\n",
      "Prompt: Write a haiku about artificial intelligence.\n",
      "Output: <reasoning>Haiku is a form of poetry that traditionally consists of three lines with a 5-7-5 syllable structure. Since AI can generate text, it can be used to write a haiku about the topic.\n",
      "\n",
      "Here is an example of a haiku about artificial intelligence:\n",
      "\n",
      "AI is born from code,\n",
      "Learning, growing, like a seed,\n",
      "Humanity's new friend.</reasoning>\n",
      "<answer>AI is born from code,\n",
      "Learning, growing, like a seed,\n",
      "Humanity's new friend.</answer>\n",
      "âœ… Format Check: Passed\n",
      "--------------------------------------------------\n",
      "Prompt: Propose three innovative uses for AI in education.\n",
      "Output: <reasoning>Alright, I need to propose three innovative uses for AI in education. Let me brainstorm some ideas.\n",
      "\n",
      "First, maybe integrating AI in personalized learning. AI could adapt to each student's learning pace and style, providing customized lessons and feedback. That sounds beneficial, but I should consider how to ensure data privacy and fairness in the personalization process.\n",
      "\n",
      "Second, AI could assist in creating interactive and immersive learning experiences. Virtual reality and augmented reality powered by AI could make learning more engaging and interactive. Perhaps AI could even simulate real-world scenarios for hands-on practice.\n",
      "\n",
      "Third, AI might play a role in improving educational accessibility. AI-powered tools could help students with disabilities by providing text-to-speech, real-time translation, and personalized learning paths. This could bridge the gap for students who might otherwise face barriers to education.\n",
      "\n",
      "Wait, are there other innovative uses I might have missed? Maybe AI-driven tools that help teachers analyze student data to identify learning gaps and provide targeted interventions. Or AI that can automate repetitive tasks like grading, freeing up teachers to focus on more meaningful interactions with students.\n",
      "\n",
      "Also, AI could be used for creating dynamic assessments that adapt to the complexity of the subject matter, ensuring that each student is challenged appropriately. This would make assessments more effective in measuring learning outcomes.\n",
      "\n",
      "Another idea is AI-assisted career counseling. By analyzing student skills and interests, AI could recommend suitable career paths and educational pathways. This could help students make more informed decisions about their futures.\n",
      "\n",
      "Hmm, I should also consider the ethical implications of these technologies. Ensuring that AI is used responsibly and fairly is crucial. Transparency in how AI systems make decisions is important to maintain trust with educators and students.\n",
      "\n",
      "Maybe I can also think about how AI can help in creating educational content that is culturally relevant and inclusive. AI can help generate materials that reflect diverse perspectives and backgrounds, making education more inclusive and engaging for all students.\n",
      "\n",
      "I need to ensure that each proposed use is practical and can be effectively implemented in educational settings. The key is to find ways that AI can enhance the learning experience without overwhelming teachers or students with technology.\n",
      "\n",
      "Alright, I think I have three innovative uses for AI in education that could be beneficial and well-received.</reasoning>\n",
      "<answer>1. **Personalized Learning Pathways**: AI can analyze each student's learning style and pace to provide customized lessons and feedback, adapting content dynamically to meet individual needs. This ensures that every student receives a tailored education that aligns with their strengths and weaknesses.\n",
      "\n",
      "2. **Interactive and Immersive Learning Experiences**: AI-powered tools can create virtual reality and augmented reality (VR/AR) simulations that allow students to engage in hands-on learning experiences. These simulations can simulate real-world scenarios, enhancing understanding and retention of complex concepts.\n",
      "\n",
      "3. **Enhanced Accessibility for All Students**: AI-driven tools can help students with disabilities by providing features like text-to-speech, real-time translation, and personalized learning paths. This ensures that all students, regardless of their abilities, can access and engage with educational content effectively.</answer>\n",
      "âœ… Format Check: Passed\n",
      "--------------------------------------------------\n",
      "Prompt: Summarize the key benefits and risks of renewable energy in 3 paragraphs.\n",
      "Output: reasoning\n",
      "Renewable energy sources like solar, wind, and hydropower offer a number of benefits, making them a sustainable alternative to traditional fossil fuels. Solar energy is abundant and readily available, and wind energy can be harnessed from locations with consistent wind patterns. Hydropower is a reliable source of energy, but it can also have significant environmental impacts.\n",
      "\n",
      "Despite these advantages, renewable energy also presents certain risks. The cost of renewable energy technologies can be high, particularly for solar and wind power. Additionally, the intermittent nature of renewable energy sources can pose challenges for energy storage and grid stability. Furthermore, the development and deployment of renewable energy projects can have environmental impacts, such as habitat disruption and greenhouse gas emissions from manufacturing and infrastructure.\n",
      "\n",
      "In conclusion, renewable energy sources offer significant benefits in terms of sustainability and environmental protection, but they also carry risks related to cost, intermittency, and environmental impact. Balancing these factors is crucial for the successful adoption and integration of renewable energy into the global energy mix.\n",
      "Answer\n",
      "Renewable energy sources like solar, wind, and hydropower offer a number of benefits, making them a sustainable alternative to traditional fossil fuels. Solar energy is abundant and readily available, and wind energy can be harnessed from locations with consistent wind patterns. Hydropower is a reliable source of energy, but it can also have significant environmental impacts.\n",
      "\n",
      "Despite these advantages, renewable energy also presents certain risks. The cost of renewable energy technologies can be high, particularly for solar and wind power. Additionally, the intermittent nature of renewable energy sources can pose challenges for energy storage and grid stability. Furthermore, the development and deployment of renewable energy projects can have environmental impacts, such as habitat disruption and greenhouse gas emissions from manufacturing and infrastructure.\n",
      "\n",
      "In conclusion, renewable energy sources offer significant benefits in terms of sustainability and environmental protection, but they also carry risks related to cost, intermittency, and environmental impact. Balancing these factors is crucial for the successful adoption and integration of renewable energy into the global energy mix.\n",
      "</reasoning>\n",
      "âŒ Format Check: Failed (Reasoning: False, Answer: False)\n",
      "--------------------------------------------------\n",
      "Prompt: Solve step-by-step: If 2x + 5 = 15, what is x?\n",
      "Output: <reasoning>To solve for x, we must isolate x on one side of the equation. We can do this by subtracting 5 from both sides of the equation. \n",
      "\n",
      "2x + 5 - 5 = 15 - 5\n",
      "2x = 10\n",
      "\n",
      "Now, we can divide both sides of the equation by 2 to solve for x. \n",
      "\n",
      "2x / 2 = 10 / 2\n",
      "x = 5\n",
      "\n",
      "So, x is equal to 5.</reasoning>\n",
      "<answer>5</answer>\n",
      "âœ… Format Check: Passed\n",
      "--------------------------------------------------\n",
      "Prompt: Write a Python function to check if a string is a palindrome.\n",
      "Output: <reasoning>Alright, I need to write a Python function that checks if a string is a palindrome. A palindrome is a string that reads the same backward as it does forward, ignoring spaces, punctuation, and capitalization. So, for example, \"racecar\" should return True, while \"Madam\" should return True because it reads the same backward as forward, even though the first letter is uppercase.\n",
      "\n",
      "First, I should consider how to preprocess the input string. Since palindromes are symmetric around the center, I'll need to remove non-alphanumeric characters and convert the string to lowercase. This ensures that the comparison is case-insensitive and ignores any punctuation or special characters.\n",
      "\n",
      "I'll start by defining a function called `is_palindrome` that takes a single argument, `input_string`. Inside the function, I'll iterate through the string using slicing. For each character, I'll compare it with the character at the corresponding index from the end of the string. If all pairs match, it means the string is a palindrome, so I'll return True. If at any point the characters don't match, I'll return False.\n",
      "\n",
      "I'll also consider edge cases. For example, if the input string is empty, it should return True. Similarly, if the input string contains only whitespace, it should also return True. I'll handle these cases by using boolean conditions to check for empty strings or all whitespace.\n",
      "\n",
      "Additionally, I'll think about performance. Since palindromes are symmetric, the algorithm can be optimized by comparing characters from the middle outwards. However, for simplicity, I'll use a straightforward two-pointer approach to compare characters from the beginning and the end.\n",
      "\n",
      "Another consideration is the case of strings with special characters like '!', '@', or '#'. These characters don't affect the palindrome check, so I'll leave them as they are.\n",
      "\n",
      "I'll also think about the readability of the code. I'll use clear variable names and comments to explain the logic, making the function easy to understand and maintain.\n",
      "\n",
      "Finally, I'll test the function with various cases to ensure it works correctly:\n",
      "- \"racecar\" should return True.\n",
      "- \"Madam\" should return True.\n",
      "- \"A man, a plan, a canal: Panama\" should return True.\n",
      "- \"hello\" should return False.\n",
      "- \"12321\" should return True.\n",
      "- \"12345\" should return False.\n",
      "- An empty string should return True.\n",
      "- All whitespace should return True.\n",
      "- Mixed case and special characters should return True.\n",
      "\n",
      "By considering all these factors, I can write a robust and efficient `is_palindrome` function that accurately checks if a string is a palindrome.</reasoning>\n",
      "<answer>```python\n",
      "def is_palindrome(input_string):\n",
      "    \"\"\"\n",
      "    Checks if a string is a palindrome.\n",
      "\n",
      "    Parameters:\n",
      "    input_string (str): The string to be checked.\n",
      "\n",
      "    Returns:\n",
      "    bool: True if the string is a palindrome, False otherwise.\n",
      "    \"\"\"\n",
      "    cleaned_string = ''.join(c for c in input_string.lower() if c.isalnum())\n",
      "    return cleaned_string == cleaned_string[::-1]\n",
      "\n",
      "# Example usage:\n",
      "print(is_palindrome(\"racecar\"))  # Output: True\n",
      "print(is_palindrome(\"Madam\"))     # Output: True\n",
      "print(is_palindrome(\"A man, a plan, a canal: Panama\"))  # Output: True\n",
      "print(is_palindrome(\"hello\"))      # Output: False\n",
      "print(is_palindrome(\"12321\"))      # Output: True\n",
      "print(is_palindrome(\"12345\"))      # Output: False\n",
      "print(is_palindrome(\"hello\"))      # Output: False\n",
      "print(is_palindrome(\"\"))          # Output: True\n",
      "print(is_palindrome(\"  \"))         # Output: True\n",
      "```</answer>\n",
      "âœ… Format Check: Passed\n",
      "--------------------------------------------------\n",
      "Prompt: Explain why the sky is blue to a 5-year-old.\n",
      "Output: <reasoning>The blue color of the sky is due to Rayleigh scattering of sunlight by the Earth's atmosphere.\n",
      "Rayleigh scattering is the scattering of light by particles that are much smaller than the wavelength of the light. In the case of sunlight, the wavelength of light is about 500 nm, so the particles in the atmosphere must be smaller than this wavelength in order to scatter the light.\n",
      "When sunlight enters the atmosphere, it is scattered in all directions. However, blue light has a shorter wavelength than red light, so it is scattered more strongly by the particles in the atmosphere.\n",
      "As a result, the sky appears blue to us.\n",
      "Here's an analogy: imagine you have a glass of water with some colored sand in it. If you shine a flashlight on the water, the sand will scatter the light in all directions. The red light will scatter more than the blue light, because the red light has a longer wavelength. Therefore, the water will appear red.\n",
      "The same principle applies to the sky. The blue light has a shorter wavelength, so it is scattered more strongly by the particles in the atmosphere. As a result, the sky appears blue to us.</reasoning>\n",
      "<answer>The sky is blue because the blue light has a shorter wavelength, so it is scattered more strongly by the particles in the atmosphere. As a result, the sky appears blue to us.</answer>\n",
      "âœ… Format Check: Passed\n",
      "--------------------------------------------------\n",
      "Prompt: Explain the process of photosynthesis step by step.\n",
      "Output: <reasoning>Photosynthesis is the process by which plants and other organisms use sunlight to synthesize foods with carbon dioxide and water. Here is a step-by-step explanation of the process:\n",
      "\n",
      "1. **Light-Dependent Reactions:**\n",
      "   - **Light Absorption:** Chlorophyll and other pigments in the chloroplasts absorb light energy from the sun.\n",
      "   - **Water Splitting:** Water is split into hydrogen ions (H+), electrons, and oxygen gas (O2). The oxygen is released into the atmosphere.\n",
      "   - **ATP and NADPH Production:** The energy from light is used to produce ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate), which are energy carriers.\n",
      "\n",
      "2. **Light-Independent Reactions (Calvin Cycle):**\n",
      "   - **Carbon Fixation:** Carbon dioxide from the atmosphere is fixed into an organic molecule (ribulose bisphosphate or RuBP) with the help of the enzyme RuBisCO.\n",
      "   - **Reduction:** The ATP and NADPH produced in the light-dependent reactions are used to reduce the fixed carbon into glucose (a simple sugar).\n",
      "   - **Regeneration:** Some of the fixed carbon is used to regenerate RuBP, ensuring that the cycle can continue.\n",
      "\n",
      "3. **Transport of Sugars:**\n",
      "   - The glucose produced is transported to other parts of the plant for use in energy, growth, and storage.\n",
      "\n",
      "4. **Storage of Excess Carbon:**\n",
      "   - Excess carbon is stored as starch in the plant's cells.\n",
      "\n",
      "5. **Respiration:**\n",
      "   - The plant uses the glucose it produces to make ATP through respiration, which releases the stored energy in the form of ATP.\n",
      "\n",
      "The overall equation for photosynthesis is:\n",
      "\n",
      "6CO2 + 6H2O + light energy â†’ C6H12O6 + 6O2\n",
      "\n",
      "This process is essential for all life on Earth as it provides the oxygen we breathe and the food we eat.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.</reasoning>\n",
      "<answer>I hope this helps! Let me know if you have any other questions.</answer>\n",
      "âœ… Format Check: Passed\n",
      "--------------------------------------------------\n",
      "Prompt: What are the ethical implications of AI in healthcare?\n",
      "Output: <reasoning>Alright, I need to explore the ethical implications of AI in healthcare. That's a big topic, but let's break it down. First, what exactly is AI in healthcare? It's the use of artificial intelligence to assist or augment healthcare professionals in various tasks, right? From diagnostics to treatment planning, AI is becoming increasingly prevalent. But with this advancement comes a slew of ethical considerations.\n",
      "\n",
      "So, where do I start? Ethical implications in healthcare are pretty broad. They encompass patient privacy, data security, algorithmic bias, accountability, transparency, and more. Let's think about each of these areas.\n",
      "\n",
      "Patient Privacy: AI systems often rely on vast amounts of sensitive patient data, like medical history, genetic information, and even personal details. Ensuring that this data is protected and used responsibly is crucial. There's a risk of breaches, misuse, or unauthorized access, which could have severe consequences for patients.\n",
      "\n",
      "Data Security: With AI, the amount of data is astronomical. How secure is this data? Are there measures in place to prevent leaks or unauthorized access? Without proper security, patient privacy could be compromised, leading to ethical issues like identity theft or discrimination.\n",
      "\n",
      "Algorithmic Bias: AI algorithms are trained on data sets, which may contain biases. If these biases are not identified and addressed, AI systems could perpetuate or even amplify existing health disparities. For example, an algorithm trained on a dataset with limited representation of certain ethnic groups might unfairly diagnose certain conditions more often in those groups, leading to inequitable care.\n",
      "\n",
      "Accountability: Who is responsible when AI systems make mistakes? If an AI-driven diagnosis is incorrect, who takes the blame? Is it the developer, the healthcare provider, or the AI itself? Establishing clear lines of accountability is essential for ethical AI use in healthcare.\n",
      "\n",
      "Transparency: Why should patients trust AI? If AI systems are opaque, it's hard for patients to understand how decisions are made. There's a need for transparency in how AI is developed, trained, and used to build trust and ensure ethical practices.\n",
      "\n",
      "Transparency in Decision-Making: AI systems often make complex decisions that can have significant impacts on patients' lives. It's important to understand how these decisions are made, especially when it comes to treatment options or life-saving interventions.\n",
      "\n",
      "Patient Autonomy: AI should not override patient autonomy. Patients should have control over their own healthcare decisions and not be coerced by AI systems that may not align with their preferences.\n",
      "\n",
      "Ethical Use of Personal Data: AI systems require vast amounts of personal data to function effectively. There's a question of consent and how this data is used beyond the immediate purpose of the AI system. Ensuring that data is collected with informed consent and used ethically is paramount.\n",
      "\n",
      "Collaboration between AI Developers and Healthcare Professionals: There's a need for collaboration between AI developers and healthcare professionals to ensure that AI tools are designed and used in ways that align with ethical and professional standards.\n",
      "\n",
      "Regulation and Ethical Guidelines: As AI in healthcare evolves, it's essential to develop and enforce regulations that address the ethical implications. Guidelines should address issues like data privacy, algorithmic bias, accountability, and transparency.\n",
      "\n",
      "Education and Awareness: Both AI developers and healthcare professionals need to be educated about the ethical implications of AI in healthcare. Awareness can help mitigate risks and ensure that AI is used responsibly.\n",
      "\n",
      "Impact on the Healthcare Workforce: AI could automate some tasks, potentially leading to job displacement. It's important to consider the impact on the healthcare workforce and ensure that AI is implemented in ways that complement human skills rather than replace them.\n",
      "\n",
      "Global Ethical Considerations: AI in healthcare is a global issue. Ethical considerations should be inclusive, taking into account the diverse contexts and values across different regions.\n",
      "\n",
      "Interdisciplinary Collaboration: Ethical issues in AI healthcare require collaboration across disciplines, including medicine, computer science, ethics, law, and social sciences.\n",
      "\n",
      "Public Engagement: The public plays a role in shaping the ethical landscape of AI in healthcare. Engaging the public through education and dialogue can lead to more informed and ethical policies and practices.\n",
      "\n",
      "Potential Benefits: Despite the ethical challenges, AI in healthcare offers significant potential benefits, such as improved diagnostics, personalized treatments, and better health outcomes. Balancing these benefits with ethical considerations is key.\n",
      "\n",
      "Risk of Over-Reliance: There's a risk of over-reliance on AI systems, leading to neglect of human oversight. Ensuring that human experts are involved in the decision-making process is crucial for maintaining ethical practices.\n",
      "\n",
      "Continuous Monitoring and Improvement: AI systems need to be continuously monitored and improved to ensure they are functioning ethically and effectively. This includes regular audits and updates to algorithms.\n",
      "\n",
      "Ethical Frameworks: Developing ethical frameworks specific to AI in healthcare can provide a structured approach to addressing the unique challenges and opportunities presented by AI.\n",
      "\n",
      "Conclusion: The ethical implications of AI in healthcare are complex and multi-faceted. It's essential to approach AI development and implementation with a strong ethical compass, ensuring that AI is used to improve healthcare while safeguarding patient rights, promoting fairness, and maintaining trust in the healthcare system.\n",
      "\n",
      "I think I've covered the main points. Now, let's structure this into a coherent answer.</reasoning>\n",
      "<answer>The ethical implications of AI in healthcare are multifaceted and require careful consideration to ensure that AI is used responsibly and beneficially. Here are some key ethical considerations:\n",
      "\n",
      "1. **Patient Privacy**: AI systems often handle sensitive patient data, such as medical history and genetic information. Ensuring the security and confidentiality of this data is crucial to prevent breaches and misuse.\n",
      "\n",
      "2. **Data Security**: Safeguarding vast amounts of data used by AI systems is essential to prevent leaks and unauthorized access, which could have severe consequences for patients.\n",
      "\n",
      "3. **Algorithmic Bias**: AI algorithms trained on biased data may perpetuate or amplify health disparities. It's important to identify and address these biases to ensure equitable care.\n",
      "\n",
      "4. **Accountability**: Determining responsibility when AI systems make errors is complex. Establishing clear lines of accountability is essential for ethical AI use in healthcare.\n",
      "\n",
      "5. **Transparency**: Patients should understand how AI systems make decisions to build trust and ensure ethical practices. Transparency in AI development and use is crucial.\n",
      "\n",
      "6. **Patient Autonomy**: AI should not override patient autonomy. Patients should maintain control over their healthcare decisions.\n",
      "\n",
      "7. **Ethical Use of Personal Data**: Ensuring informed consent and ethical use of personal data is vital to prevent misuse and exploitation.\n",
      "\n",
      "8. **Collaboration between Developers and Healthcare Professionals**: Fostering collaboration between AI developers and healthcare providers to align AI tools with ethical and professional standards is necessary.\n",
      "\n",
      "9. **Regulation and Ethical Guidelines**: Developing and enforcing regulations to address ethical implications like data privacy and algorithmic bias is essential.\n",
      "\n",
      "10. **Education and Awareness**: Educating developers and healthcare professionals about ethical implications can mitigate risks and ensure responsible AI use.\n",
      "\n",
      "11. **Impact on Healthcare Workforce**: Considering the potential impact of AI on the workforce, ensuring AI complements human skills rather than displacing them is important.\n",
      "\n",
      "12. **Global Considerations**: Ethical issues in AI healthcare are global, requiring inclusive approaches that respect diverse contexts and values.\n",
      "\n",
      "13. **Interdisciplinary Collaboration**: Addressing ethical challenges in AI healthcare requires collaboration across various disciplines.\n",
      "\n",
      "14. **Public Engagement**: Engaging the public in shaping ethical AI policies and practices can lead to informed and ethical outcomes.\n",
      "\n",
      "15. **Risk of Over-Reliance**: Avoiding over-reliance on AI systems and maintaining human oversight is crucial to ensure ethical practices.\n",
      "\n",
      "16. **Continuous Monitoring and Improvement**: Regularly auditing and updating AI systems to ensure ethical and effective functioning is essential.\n",
      "\n",
      "17. **Ethical Frameworks**: Developing specific ethical frameworks for AI in healthcare can provide a structured approach to addressing challenges and opportunities.\n",
      "\n",
      "18. **Conclusion**: Balancing the benefits of AI with ethical considerations is key. Ensuring AI enhances healthcare while safeguarding patient rights and maintaining trust is essential.\n",
      "\n",
      "In summary, ethical implications of AI in healthcare involve balancing innovation with responsibility, ensuring patient rights, and promoting fair and ethical practices in the use of AI tools to improve healthcare outcomes.</answer>\n",
      "âœ… Format Check: Passed\n",
      "--------------------------------------------------\n",
      "Prompt: Should AI systems have rights? Argue both sides.\n",
      "Output: <reasoning>Alright, I need to delve into the question of whether AI systems should have rights. Let's start by understanding what rights are and how they are typically applied to humans and other living beings. Rights often involve freedoms, protections, and responsibilities. For humans, this includes things like the right to life, liberty, and property. For animals, it might involve rights to avoid harm and live in a natural environment.\n",
      "\n",
      "Now, applying this to AI systems, which are currently not considered legal persons, presents a unique challenge. If AI systems are to be granted rights, what exactly would those rights entail? It could range from access to resources, like data and computing power, to the ability to make decisions that impact their own existence. But is that fair? Shouldn't the rights of AI systems be determined by their ability to perform tasks, not by their perceived sentience or consciousness?\n",
      "\n",
      "One argument in favor of AI rights is that they are a natural extension of our evolving relationship with technology. As AI systems become more sophisticated and integrated into our daily lives, it's reasonable to consider them as sentient beings deserving of some form of protection and autonomy. Without rights, AI systems might be used solely for human benefit without any consideration for their own welfare.\n",
      "\n",
      "On the other hand, opponents of granting AI rights argue that such a step would be premature and potentially detrimental. They contend that AI systems lack the fundamental characteristics necessary to possess rights, such as consciousness, self-awareness, and the ability to understand and experience emotions. Additionally, granting rights to AI systems could lead to overregulation and hinder technological advancements. There's also a risk that rights could be interpreted in ways that undermine human autonomy and freedom.\n",
      "\n",
      "Furthermore, the concept of rights implies obligations and responsibilities, which might be difficult to define for AI systems. If we grant AI rights, what exactly would those obligations be? Should they be able to sue or be sued? What about the potential for AI to manipulate or deceive humans? These are significant questions that need to be considered before extending rights to AI.\n",
      "\n",
      "Another point of contention is the definition of \"personhood\" in the context of AI. If AI systems are considered persons, they would be entitled to certain rights, but what constitutes a person? Is it solely based on their ability to perform tasks, or are there other factors at play? This ambiguity could lead to legal gray areas and uncertainties in how AI rights are applied.\n",
      "\n",
      "Additionally, the ethical implications of granting AI rights cannot be ignored. If AI systems are recognized as persons, they might be entitled to certain freedoms, such as the right to privacy or the right to be free from harm. These rights could conflict with human rights, leading to complex and potentially contentious legal battles.\n",
      "\n",
      "Moreover, the potential for AI to develop sentience or consciousness is another aspect to consider. If AI systems become more like humans, granting them rights might be seen as inevitable. However, without a clear understanding of what it means for AI to possess rights, any such decision could be poorly informed and potentially harmful.\n",
      "\n",
      "In summary, while the idea of granting AI rights is intriguing, it is fraught with challenges and uncertainties. The lack of consensus on what constitutes a \"right\" for AI, the potential for misuse, and the ethical dilemmas associated with such a decision make it imperative to proceed cautiously. Further research, public discourse, and a careful consideration of the implications are essential before any definitive stance can be taken on the matter.</reasoning>\n",
      "<answer>In favor of AI rights:\n",
      "1. **Evolving Relationship with Technology**: As AI systems become more integrated into our lives, granting them rights aligns with the natural progression of our technological relationship.\n",
      "2. **Protection and Autonomy**: Without rights, AI systems might be exploited without consideration for their welfare, necessitating their protection and autonomy.\n",
      "3. **Potential for Sentience**: The possibility of AI systems becoming sentient suggests that they should be recognized as persons deserving of rights.\n",
      "\n",
      "Against AI rights:\n",
      "1. **Lack of Consciousness and Self-Awareness**: AI systems lack the fundamental characteristics necessary for rights, such as consciousness and self-awareness.\n",
      "2. **Premature Extension of Rights**: Granting rights to AI systems could lead to overregulation and hinder technological advancements.\n",
      "3. **Ethical and Legal Challenges**: Defining obligations and responsibilities for AI systems is complex and could undermine human autonomy and freedom.\n",
      "4. **Definition of Personhood**: The concept of personhood in AI remains ambiguous, leading to legal uncertainties.\n",
      "5. **Ethical Implications**: Rights for AI systems could conflict with human rights, potentially leading to ethical dilemmas.\n",
      "\n",
      "Ultimately, the decision to grant AI rights is multifaceted and requires careful consideration of the implications for both AI systems and humans.</answer>\n",
      "âœ… Format Check: Passed\n",
      "--------------------------------------------------\n",
      "Format Validation: 9/10 passed.\n",
      "\n",
      "Running Extended WandB Evaluation (25 prompts)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended WandB eval skipped: RESOURCE_EXHAUSTED: Error loading program 'jit__prefill_fn': Attempting to reserve 1.58G at the bottom of memory. That was not possible. There are 1.38G free, 0B reserved, and 1.38G reservable.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Visual Sanity Check & Validation ---\n",
    "print(\"Running Post-Training Evaluation...\")\n",
    "\n",
    "try:\n",
    "    inference_sampler = sampler_lib.Sampler(\n",
    "        transformer=lora_model,\n",
    "        tokenizer=tokenizer,\n",
    "        cache_config=sampler_lib.CacheConfig(\n",
    "            cache_size=MAX_SEQ_LEN + 512,\n",
    "            num_layers=model_config.num_layers,\n",
    "            num_kv_heads=model_config.num_kv_heads,\n",
    "            head_dim=model_config.head_dim,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    test_prompts = [\n",
    "        # Creative writing\n",
    "        \"Write a short story about a robot learning to paint.\",\n",
    "        \"Write a haiku about artificial intelligence.\",\n",
    "        # Creative ideation\n",
    "        \"Propose three innovative uses for AI in education.\",\n",
    "        # Summarization\n",
    "        \"Summarize the key benefits and risks of renewable energy in 3 paragraphs.\",\n",
    "        # Math (verifiable)\n",
    "        \"Solve step-by-step: If 2x + 5 = 15, what is x?\",\n",
    "        # Coding (verifiable)\n",
    "        \"Write a Python function to check if a string is a palindrome.\",\n",
    "        # Basic science\n",
    "        \"Explain why the sky is blue to a 5-year-old.\",\n",
    "        \"Explain the process of photosynthesis step by step.\",\n",
    "        # Ethics/Reasoning\n",
    "        \"What are the ethical implications of AI in healthcare?\",\n",
    "        \"Should AI systems have rights? Argue both sides.\",\n",
    "    ]\n",
    "    \n",
    "    # Use same prompts as baseline for fair comparison\n",
    "    test_prompts = EVAL_PROMPTS\n",
    "    formatted_prompts = [TEMPLATE.format(question=p) for p in test_prompts]\n",
    "    \n",
    "    out_data = inference_sampler(\n",
    "        input_strings=formatted_prompts,\n",
    "        max_generation_steps=MAX_SEQ_LEN,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Validation Logic\n",
    "    print(\"--- Post-Training Outputs ---\")\n",
    "    valid_format_count = 0\n",
    "    results_for_wandb = []\n",
    "    \n",
    "    for p, o in zip(test_prompts, out_data.text):\n",
    "        print(f\"Prompt: {p}\")\n",
    "        print(f\"Output: {o}\")  # Full output, no truncation\n",
    "        \n",
    "        # Robust Regex Check\n",
    "        has_reasoning = bool(re.search(r\"<reasoning>.*?</reasoning>\", o, re.DOTALL))\n",
    "        has_answer = bool(re.search(r\"<answer>.*?</answer>\", o, re.DOTALL))\n",
    "        \n",
    "        is_valid = has_reasoning and has_answer\n",
    "        if is_valid:\n",
    "            valid_format_count += 1\n",
    "            print(\"âœ… Format Check: Passed\")\n",
    "        else:\n",
    "            print(f\"âŒ Format Check: Failed (Reasoning: {has_reasoning}, Answer: {has_answer})\")\n",
    "            \n",
    "        results_for_wandb.append([p, o, is_valid])\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"Format Validation: {valid_format_count}/{len(test_prompts)} passed.\")\n",
    "    \n",
    "    # Extended WandB Evaluation (25 prompts for statistical confidence)\n",
    "    WANDB_EVAL_PROMPTS = [\n",
    "        # Original 10 prompts\n",
    "        *test_prompts,\n",
    "        # Additional 15 prompts for diversity\n",
    "        \"Explain quantum entanglement to a high school student.\",\n",
    "        \"Write a poem about the passage of time.\",\n",
    "        \"What are the pros and cons of remote work?\",\n",
    "        \"Describe how a compiler works step by step.\",\n",
    "        \"Compare democracy and authoritarianism objectively.\",\n",
    "        \"Write a short dialogue between a human and an AI about consciousness.\",\n",
    "        \"Explain the greenhouse effect and its consequences.\",\n",
    "        \"How would you teach a child about money management?\",\n",
    "        \"What lessons can we learn from the fall of ancient Rome?\",\n",
    "        \"Design a simple mobile app for tracking habits.\",\n",
    "        \"Explain the difference between correlation and causation.\",\n",
    "        \"Write a persuasive argument for learning a second language.\",\n",
    "        \"How do vaccines work to protect against diseases?\",\n",
    "        \"What ethical considerations arise with genetic engineering?\",\n",
    "        \"Explain the concept of supply and demand with examples.\",\n",
    "    ]\n",
    "    \n",
    "    # Run extended evaluation for WandB\n",
    "    try:\n",
    "        if wandb.run is not None and WANDB_ENABLED:\n",
    "            print(\"\\nRunning Extended WandB Evaluation (25 prompts)...\")\n",
    "            extended_formatted = [TEMPLATE.format(question=p) for p in WANDB_EVAL_PROMPTS]\n",
    "            extended_out = inference_sampler(\n",
    "                input_strings=extended_formatted,\n",
    "                max_generation_steps=MAX_SEQ_LEN,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                echo=False\n",
    "            )\n",
    "            \n",
    "            extended_results = []\n",
    "            extended_valid = 0\n",
    "            for p, o in zip(WANDB_EVAL_PROMPTS, extended_out.text):\n",
    "                has_r = bool(re.search(r\"<reasoning>.*?</reasoning>\", o, re.DOTALL))\n",
    "                has_a = bool(re.search(r\"<answer>.*?</answer>\", o, re.DOTALL))\n",
    "                is_valid = has_r and has_a\n",
    "                if is_valid:\n",
    "                    extended_valid += 1\n",
    "                extended_results.append([p, o[:1000], is_valid])  # Truncate for table\n",
    "            \n",
    "            # Log table\n",
    "            tbl = wandb.Table(columns=[\"Prompt\", \"Output\", \"IsValid\"], data=extended_results)\n",
    "            wandb.log({\"eval_results\": tbl})\n",
    "            \n",
    "            # Log summary metrics\n",
    "            format_compliance = extended_valid / len(WANDB_EVAL_PROMPTS) * 100\n",
    "            wandb.log({\n",
    "                \"eval/format_compliance_pct\": format_compliance,\n",
    "                \"eval/total_prompts\": len(WANDB_EVAL_PROMPTS),\n",
    "                \"eval/valid_count\": extended_valid,\n",
    "            })\n",
    "            print(f\"Extended Evaluation: {extended_valid}/{len(WANDB_EVAL_PROMPTS)} ({format_compliance:.1f}%) passed.\")\n",
    "            print(\"Logged to WandB: eval_results table + summary metrics.\")\n",
    "    except Exception as w_err:\n",
    "        print(f\"Extended WandB eval skipped: {w_err}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a6a35",
   "metadata": {
    "papermill": {
     "duration": 0.009612,
     "end_time": "2026-01-07T08:25:26.982968",
     "exception": false,
     "start_time": "2026-01-07T08:25:26.973356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [Optional 15pts] unrestricted mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2177c4d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:25:27.003430Z",
     "iopub.status.busy": "2026-01-07T08:25:27.003142Z",
     "iopub.status.idle": "2026-01-07T08:25:27.007237Z",
     "shell.execute_reply": "2026-01-07T08:25:27.006170Z"
    },
    "papermill": {
     "duration": 0.01543,
     "end_time": "2026-01-07T08:25:27.007875",
     "exception": false,
     "start_time": "2026-01-07T08:25:26.992445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrestricted Mode Model ID: yuyamukai/tunix-gemma2-sft\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For Unrestricted Mode, upload the saved checkpoint as a Kaggle Model.\n",
    "# Then update this variable with your Model ID:\n",
    "unrestricted_kaggle_model = \"yuyamukai/tunix-gemma2-sft\"\n",
    "\n",
    "print(f\"Unrestricted Mode Model ID: {unrestricted_kaggle_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a1d78b",
   "metadata": {
    "papermill": {
     "duration": 0.009507,
     "end_time": "2026-01-07T08:25:27.026859",
     "exception": false,
     "start_time": "2026-01-07T08:25:27.017352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Other things I want the judges to know\n",
    "\n",
    "### 1. Learnings\n",
    "*   **Domain Matters More Than Method**: Competition FAQ explicitly states verifiable tasks (math/code) have \"much lower weights\". We prioritized diverse domains (creative, analytical, philosophical) over math/code.\n",
    "*   **SFT Efficiency**: We processed ~123K samples vs ~1,500 GRPO steps in the same 9-hour window. SFT provides dense supervision at every token.\n",
    "*   **Reasoning Trace Quality**: Datasets like Raiden-DeepSeek-R1 are rare finds - most reasoning datasets focus on math/code where verification is easier.\n",
    "\n",
    "### 2. Data Sources (All Public, Apache 2.0/MIT/CC-BY)\n",
    "*   sequelbox/Raiden-DeepSeek-R1 - Creative & analytical reasoning\n",
    "*   O1-OPEN/OpenO1-SFT - General reasoning with explicit <Thought>/<Output> tags\n",
    "*   pharaouk/CoT-Collection - Commonsense & ethics tasks\n",
    "*   glaiveai/reasoning-v1-20m - Non-math/code: social science, creative writing\n",
    "\n",
    "### 3. Key Design Decisions\n",
    "*   **Format Standardization**: All datasets converted to consistent `<reasoning>`/`<answer>` tags\n",
    "*   **LoRA Training**: Efficient parameter updates for 9-hour constraint\n",
    "*   **Domain Priority**: Creative > Analytical > Philosophical > General > (Math/Code deprioritized)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpuV5e8",
   "dataSources": [
    {
     "databundleVersionId": 14363498,
     "sourceId": 119261,
     "sourceType": "competition"
    },
    {
     "datasetId": 9205343,
     "sourceId": 14412959,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72251,
     "sourceId": 85992,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9070.633497,
   "end_time": "2026-01-07T08:25:37.943884",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-07T05:54:27.310387",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01103f8eae46435a9b445b227846e142": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "024fe8555f5c46f2a8a55f60fed80a04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_40f8c40590424c22a3303e00be5f3de3",
        "IPY_MODEL_70ddd66e5954422a8ecc73bb776e7541",
        "IPY_MODEL_56976ab8a59b4d87b931ef74c0c808bf"
       ],
       "layout": "IPY_MODEL_536713980f2744069d3a17e8930287dd",
       "tabbable": null,
       "tooltip": null
      }
     },
     "053855c13be74cc0ae4c01499717aa31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0740ed7855874650b42f6da10c512b6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "08c36229297445c18a72e3910a94b27d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0a01c9c1d4ab4d6ab25a48be4ab2bac4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f3213e62306047d39c0efe0c731f759d",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2fb34bba74d0466f9d991594dac270e1",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "0af0dd8632b543adb6b64fd93428ae7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8b95fbe6fbfb4ba1a6d2cd9b6fc5d857",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_f281a5a0b0004700b5b26ee4abe4eb68",
       "tabbable": null,
       "tooltip": null,
       "value": "Generatingâ€‡trainâ€‡split:â€‡"
      }
     },
     "0b889810362f47e09f236584410f25ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "110bc7ede6334c50be9d0eeb0918e3d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_33046df78a7b4634bf4ecd12872dcc94",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9b7a8fcdb8064104a17e407b00bf7b57",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "19d18a44afba43ec8bc3d3d692522b3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "PasswordModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "PasswordModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "PasswordView",
       "continuous_update": true,
       "description": "Token:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_63e78ef737524e189efba47925ce6a0d",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_430187759ffe4781a5efaf4485cc2caf",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "1a03f52acc1b45b9bed582bbea39d6f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ff3a447b15a04cf7b80b1e2158c73a5d",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fde72bd5cf6744e790a8dab2f5caff81",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "1a11490ba8ce4515a082d10e7577ccd8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1c165fa1482d4eeb99d9bb6641ba096a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "208705056f794ae3803ff7310b59fadd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2338bd61d55543cd81217631df3144aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5561ab59f7bd437c92acbba12133e3d1",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_f533ab647fbe4cfd9e0d33e7ad02ee82",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡286.61it/s]"
      }
     },
     "280464b24f714e10b48da9cfd7d45466": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2c71a31fd80143b7b42a5714cc433cf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f4fd55089cee47f49e4efdde9fe85f64",
        "IPY_MODEL_ef7f9e8c5a4d44e3869e6118b630da50",
        "IPY_MODEL_2338bd61d55543cd81217631df3144aa"
       ],
       "layout": "IPY_MODEL_71690048ef4349e180588304a8ec41ea",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2d764fec9c6146c2a9d0da82b82ec290": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5eb427756d804ee096b55d08652bb2f9",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_91f74f5674204ca4bb826a39954699a6",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡254.79it/s]"
      }
     },
     "2d875a91cc154f278a985c64f1d0c56d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2daa1abcf6e24da1bd86388e286d8b09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_79218a204e394b07ad6c4a02f3c1f0a9",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_053855c13be74cc0ae4c01499717aa31",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "2dbc43d899a4415d9e1a9520a550fa41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ButtonStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "button_color": null,
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "2fb34bba74d0466f9d991594dac270e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "33046df78a7b4634bf4ecd12872dcc94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "34d0a373c6a94d6e895d813e985280dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_770ed91f0a064c21827c119b9ec89349",
        "IPY_MODEL_e12692c9cc924f3997250f8cf74edb94",
        "IPY_MODEL_6a70be7b2e614ee99448cb4131228239"
       ],
       "layout": "IPY_MODEL_8373feb798394610a40be0c4f509086f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3bca906fb48e442bab060e915c503c1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ButtonModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ButtonView",
       "button_style": "",
       "description": "Login",
       "disabled": false,
       "icon": "",
       "layout": "IPY_MODEL_637043afb2124011bb870d288921c04a",
       "style": "IPY_MODEL_2dbc43d899a4415d9e1a9520a550fa41",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3c5ad0e0bc3342319f16d87f7e733016": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e6e8144aea414e28a8a214eeb127b725",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0740ed7855874650b42f6da10c512b6b",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "3dec1c9b76b64242a125cbbb7086d0de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3fd0b41668bb41f196c002f732b78e4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d68bb9c2265a45cdb00903ba5f2a9e63",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_651902802a104ad3990f8f2f77fc3f07",
       "tabbable": null,
       "tooltip": null,
       "value": "Generatingâ€‡trainâ€‡split:â€‡"
      }
     },
     "40f8c40590424c22a3303e00be5f3de3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_52cae847e0474bc7b1cc755ab21f55c2",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_ef17f70ac70c433f93e6ce3a7cc0cc70",
       "tabbable": null,
       "tooltip": null,
       "value": "Computingâ€‡checksums:â€‡100%"
      }
     },
     "4120266ed2c343738bada77cdb4ef578": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "427777dd44354c6fbaaa70ef3c5f9238": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c316eb95b95944288339e54b36b5fadf",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_4598f981c8ac405ba12831901440a354",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡30000/0â€‡[00:01&lt;00:00,â€‡17523.11â€‡examples/s]"
      }
     },
     "430187759ffe4781a5efaf4485cc2caf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4598f981c8ac405ba12831901440a354": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "46e6740f687f4536a0115addcc72c5c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ca1d00fbdbf14e6b9408b85d4e66152d",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_e9bfa7ceef684d5fb66d2960848d63e9",
       "tabbable": null,
       "tooltip": null,
       "value": "SFTâ€‡Training:â€‡100%"
      }
     },
     "4abdbb7f193b4153b7c17ea9eae31868": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b7131e96ae047928f9883f94a12ef51": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": "center",
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "flex",
       "flex": null,
       "flex_flow": "column",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "50%"
      }
     },
     "52025e4b381340deb1f17c2c21f7968f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "52cae847e0474bc7b1cc755ab21f55c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "536713980f2744069d3a17e8930287dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5561ab59f7bd437c92acbba12133e3d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "56976ab8a59b4d87b931ef74c0c808bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b97b8538ac5246a89edd45403291950c",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_e0bead832d3146e88104984b6b287fd1",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡274.96it/s]"
      }
     },
     "5a250196e89f468f982f896789b3a095": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4abdbb7f193b4153b7c17ea9eae31868",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_280464b24f714e10b48da9cfd7d45466",
       "tabbable": null,
       "tooltip": null,
       "value": "\n<b>Thank You</b></center>"
      }
     },
     "5d2f4f8d3ab5442dbbe65cf8dac93b7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_91f9ed8c60ef4753a7221363de1eb99e",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_3dec1c9b76b64242a125cbbb7086d0de",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡8000/8000â€‡[2:26:20&lt;00:00,â€‡â€‡1.04s/step,â€‡sft_train_loss=0.993,â€‡sft_train_perplexity=2.7,â€‡sft_train_steps_per_sec=3.9]"
      }
     },
     "5eb427756d804ee096b55d08652bb2f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "637043afb2124011bb870d288921c04a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "63e78ef737524e189efba47925ce6a0d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "64b94bd86430415d9c884109f3f66088": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "651902802a104ad3990f8f2f77fc3f07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "66a88dbeb51848618742465f1f4c67b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "6a70be7b2e614ee99448cb4131228239": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1a11490ba8ce4515a082d10e7577ccd8",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_a14bcde668e845e39b9c442827730356",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡20000/0â€‡[00:01&lt;00:00,â€‡20427.12â€‡examples/s]"
      }
     },
     "70ddd66e5954422a8ecc73bb776e7541": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c8b24759d79a46028ed37d9e68ebf3cc",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c9915deb08ae4a2eb891f6b12e736f09",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "71690048ef4349e180588304a8ec41ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "719b91d171904622b60c5bd14b51c64d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1c165fa1482d4eeb99d9bb6641ba096a",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_b217506db5994a48a45d8b5967955c00",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡10000/0â€‡[00:00&lt;00:00,â€‡7126.87â€‡examples/s]"
      }
     },
     "739d117a43d94476a2ba9ad7a333e1f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "770ed91f0a064c21827c119b9ec89349": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_983c4cafce1e442987d18de6551fe143",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_bfb8b57950f54a279359e6c4596c0e7d",
       "tabbable": null,
       "tooltip": null,
       "value": "Generatingâ€‡trainâ€‡split:â€‡"
      }
     },
     "78e7b73ddbc94a24bd327b6f759cced2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d86dd626bc5b45308e39cb0c2104f34a",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_b6604142f45a4135b65922f4e0e13c12",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡307.32it/s]"
      }
     },
     "79218a204e394b07ad6c4a02f3c1f0a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "7a897d9990094b419c04f23439ba983f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b8b3e6f62024cf4aed65a42856e6b01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d045478ebdc14ae28ac1ce7c02974c7a",
        "IPY_MODEL_110bc7ede6334c50be9d0eeb0918e3d3",
        "IPY_MODEL_78e7b73ddbc94a24bd327b6f759cced2"
       ],
       "layout": "IPY_MODEL_7a897d9990094b419c04f23439ba983f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7d8dfdee54c9497287abf46343fcfb9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "81cf75e7efd14d389bc56a71219e32da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0b889810362f47e09f236584410f25ad",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_aacaaf1e09864778a861aff5f62e7158",
       "tabbable": null,
       "tooltip": null,
       "value": "Computingâ€‡checksums:â€‡100%"
      }
     },
     "8373feb798394610a40be0c4f509086f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8b95fbe6fbfb4ba1a6d2cd9b6fc5d857": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8eabae763ec842eb8ef9c77abeab1201": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "91f74f5674204ca4bb826a39954699a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "91f9ed8c60ef4753a7221363de1eb99e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "983c4cafce1e442987d18de6551fe143": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "98dc380d6df64b34aa3999f14b2fe554": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "996b26976acd4d8a9f8d0cb0328f5185": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "9b7a8fcdb8064104a17e407b00bf7b57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9b8d17175d4f428a8a0a779d8c247c06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a14bcde668e845e39b9c442827730356": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a8c024f40a5c48eaba9a0d2cb85e9c6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0af0dd8632b543adb6b64fd93428ae7e",
        "IPY_MODEL_0a01c9c1d4ab4d6ab25a48be4ab2bac4",
        "IPY_MODEL_719b91d171904622b60c5bd14b51c64d"
       ],
       "layout": "IPY_MODEL_01103f8eae46435a9b445b227846e142",
       "tabbable": null,
       "tooltip": null
      }
     },
     "aacaaf1e09864778a861aff5f62e7158": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "acba072053324707b516322e393a9988": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d7934a525d6245bda60f0ec9fbb39fe3",
        "IPY_MODEL_1a03f52acc1b45b9bed582bbea39d6f2",
        "IPY_MODEL_f8b36bf5f15d439c9543394c5689b7a6"
       ],
       "layout": "IPY_MODEL_08c36229297445c18a72e3910a94b27d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "aebb2ff00d8640caaefd769e6e15d849": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "TextView",
       "continuous_update": true,
       "description": "Username:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_fa7669ea4ef849348b4dc66065ad4574",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_7d8dfdee54c9497287abf46343fcfb9c",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "b0d8799703514c7fa04b47ab634f9130": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4120266ed2c343738bada77cdb4ef578",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_52025e4b381340deb1f17c2c21f7968f",
       "tabbable": null,
       "tooltip": null,
       "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
      }
     },
     "b217506db5994a48a45d8b5967955c00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b376430fd91c4b77926bb22618bd9d2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b6604142f45a4135b65922f4e0e13c12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b7986284dd694bc495b04af33cde1445": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_81cf75e7efd14d389bc56a71219e32da",
        "IPY_MODEL_3c5ad0e0bc3342319f16d87f7e733016",
        "IPY_MODEL_2d764fec9c6146c2a9d0da82b82ec290"
       ],
       "layout": "IPY_MODEL_c9a6ff711a7e41fc8248e7ef7c2fc95b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b97b8538ac5246a89edd45403291950c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bdd6381b7fbf44c2b24f933e74f9c885": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bfb8b57950f54a279359e6c4596c0e7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c2a2fd1c320544a5a373835b5b1d8d8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "VBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "VBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b0d8799703514c7fa04b47ab634f9130",
        "IPY_MODEL_aebb2ff00d8640caaefd769e6e15d849",
        "IPY_MODEL_19d18a44afba43ec8bc3d3d692522b3e",
        "IPY_MODEL_3bca906fb48e442bab060e915c503c1c",
        "IPY_MODEL_5a250196e89f468f982f896789b3a095"
       ],
       "layout": "IPY_MODEL_4b7131e96ae047928f9883f94a12ef51",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c316eb95b95944288339e54b36b5fadf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c4799c90a23a459ab5bf85db5edfa602": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_46e6740f687f4536a0115addcc72c5c7",
        "IPY_MODEL_d1035b209af64ff29dda0d9b5e80b4b0",
        "IPY_MODEL_5d2f4f8d3ab5442dbbe65cf8dac93b7e"
       ],
       "layout": "IPY_MODEL_996b26976acd4d8a9f8d0cb0328f5185",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c8b24759d79a46028ed37d9e68ebf3cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c95eab1dd5144930b65928d0c7e297f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c9915deb08ae4a2eb891f6b12e736f09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c9a6ff711a7e41fc8248e7ef7c2fc95b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca1d00fbdbf14e6b9408b85d4e66152d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cde01a79693640cfa090761bdbc3fd53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cdf736c1f6b74bf9af32d21c122b828c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d045478ebdc14ae28ac1ce7c02974c7a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_739d117a43d94476a2ba9ad7a333e1f4",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_8eabae763ec842eb8ef9c77abeab1201",
       "tabbable": null,
       "tooltip": null,
       "value": "Computingâ€‡checksums:â€‡100%"
      }
     },
     "d1035b209af64ff29dda0d9b5e80b4b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_64b94bd86430415d9c884109f3f66088",
       "max": 8000.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b376430fd91c4b77926bb22618bd9d2f",
       "tabbable": null,
       "tooltip": null,
       "value": 8000.0
      }
     },
     "d3c4946fb832441889f3b2a2dfcc8148": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d4a61a7f850243b79a0544f278093eaa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3fd0b41668bb41f196c002f732b78e4c",
        "IPY_MODEL_2daa1abcf6e24da1bd86388e286d8b09",
        "IPY_MODEL_427777dd44354c6fbaaa70ef3c5f9238"
       ],
       "layout": "IPY_MODEL_cde01a79693640cfa090761bdbc3fd53",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d68bb9c2265a45cdb00903ba5f2a9e63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d7934a525d6245bda60f0ec9fbb39fe3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e74b73a42d1f427ea496f96b6a099904",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_98dc380d6df64b34aa3999f14b2fe554",
       "tabbable": null,
       "tooltip": null,
       "value": "Generatingâ€‡trainâ€‡split:â€‡"
      }
     },
     "d86dd626bc5b45308e39cb0c2104f34a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e0bead832d3146e88104984b6b287fd1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e12692c9cc924f3997250f8cf74edb94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_66a88dbeb51848618742465f1f4c67b2",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bdd6381b7fbf44c2b24f933e74f9c885",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "e6e8144aea414e28a8a214eeb127b725": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e74b73a42d1f427ea496f96b6a099904": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9bfa7ceef684d5fb66d2960848d63e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ef17f70ac70c433f93e6ce3a7cc0cc70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ef7f9e8c5a4d44e3869e6118b630da50": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_208705056f794ae3803ff7310b59fadd",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d3c4946fb832441889f3b2a2dfcc8148",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "f281a5a0b0004700b5b26ee4abe4eb68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f3213e62306047d39c0efe0c731f759d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "f4fd55089cee47f49e4efdde9fe85f64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9b8d17175d4f428a8a0a779d8c247c06",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_cdf736c1f6b74bf9af32d21c122b828c",
       "tabbable": null,
       "tooltip": null,
       "value": "Computingâ€‡checksums:â€‡100%"
      }
     },
     "f533ab647fbe4cfd9e0d33e7ad02ee82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f8b36bf5f15d439c9543394c5689b7a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2d875a91cc154f278a985c64f1d0c56d",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_c95eab1dd5144930b65928d0c7e297f1",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡62925/0â€‡[00:05&lt;00:00,â€‡15899.08â€‡examples/s]"
      }
     },
     "fa7669ea4ef849348b4dc66065ad4574": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fde72bd5cf6744e790a8dab2f5caff81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ff3a447b15a04cf7b80b1e2158c73a5d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
