{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":14367199,"sourceType":"datasetVersion","datasetId":9174389}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tunix Zero-Cost Training Pipeline\nSFT -> GRPO on Public Datasets","metadata":{}},{"cell_type":"code","source":"\n# Install Tunix and dependencies\n# In Kaggle, we might need to install from a dataset or git\n!pip install -q git+https://github.com/google-deepmind/tunix.git\n!pip install -q flax==0.12.0 optax==0.2.4 chex==0.1.88\n!pip install -q transformers==4.47.0 datasets==3.2.0\n\nimport os\nimport jax\nimport flax\nimport optax\nfrom tunix.sft import peft_trainer\nfrom tunix.rl.grpo import grpo_learner\nfrom transformers import AutoTokenizer\nimport datasets\n\nprint(f\"JAX devices: {jax.devices()}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Configuration\nMODEL_ID = \"google/gemma-2-2b-it\"\nDATASET_PATH = \"/kaggle/input/tunix-public-data\" # Placeholder for uploaded dataset\nSFT_OUTPUT_DIR = \"sft_checkpoint\"\nGRPO_OUTPUT_DIR = \"grpo_checkpoint\"\n\n# Hyperparameters\nSFT_STEPS = 500  # Adjust for 1.5h\nGRPO_STEPS = 500 # Adjust for 5h\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport re\nimport sympy\n\n# --- Reward Functions ---\nreasoning_pattern = re.compile(r\"<reasoning>(.*?)</reasoning>\", re.DOTALL)\nanswer_pattern = re.compile(r\"<answer>(.*?)</answer>\", re.DOTALL)\n\ndef structure_reward(prompts, completions, **kwargs):\n    scores = []\n    for completion in completions:\n        has_reasoning = \"<reasoning>\" in completion and \"</reasoning>\" in completion\n        has_answer = \"<answer>\" in completion and \"</answer>\" in completion\n        score = 0.5 * has_reasoning + 0.5 * has_answer\n        scores.append(score)\n    return scores\n\ndef math_correctness_reward(prompts, completions, answer, **kwargs):\n    scores = []\n    for completion, true_ans in zip(completions, answer):\n        match = answer_pattern.search(completion)\n        if not match:\n            scores.append(0.0)\n            continue\n        pred = match.group(1).strip()\n        \n        # Simple match for now, SymPy can be added if compatible\n        if pred == true_ans.strip():\n            scores.append(1.0)\n        else:\n            scores.append(0.0)\n    return scores\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --- SFT Phase: Format Learning ---\nprint(\"Starting SFT Phase...\")\n\n# Load Data\n# Assuming data is pre-formatted as 'text' column\nsft_dataset = datasets.load_dataset(\"json\", data_files=f\"{DATASET_PATH}/sft_magpie.jsonl\", split=\"train\")\n\n# Define Trainer\n# Note: Using Tunix PeftTrainer APIs (Mock usage based on repo)\ntrainer = peft_trainer.PeftTrainer(\n    model_name=MODEL_ID,\n    train_dataset=sft_dataset,\n    max_steps=SFT_STEPS,\n    output_dir=SFT_OUTPUT_DIR,\n    per_device_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    use_lora=True,\n)\n\n# Train\n# trainer.train() \n# trainer.save_model(SFT_OUTPUT_DIR)\nprint(\"SFT Completed (Mock)\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --- GRPO Phase: Reinforcement ---\nprint(\"Starting GRPO Phase...\")\n\n# Load Data (GSM8K)\n# Assuming 'prompt' and 'answer' columns\ngrpo_dataset = datasets.load_dataset(\"json\", data_files=f\"{DATASET_PATH}/grpo_gsm8k_train.jsonl\", split=\"train\")\n\n# Initialize GRPO Learner\n# We load the SFT checkpoint as the starting point\ngrpo_config = grpo_learner.GRPOConfig(\n    num_generations=4,\n    beta=0.04,\n    learning_rate=1e-6,\n    max_prompt_length=256,\n    max_completion_length=512,\n)\n\nlearner = grpo_learner.GRPOLearner(\n    config=grpo_config,\n    model_name_or_path=SFT_OUTPUT_DIR, # Load from SFT\n    reward_functions=[structure_reward, math_correctness_reward],\n    train_dataset=grpo_dataset,\n)\n\n# Train\n# learner.train(steps=GRPO_STEPS)\n# learner.save_model(GRPO_OUTPUT_DIR)\nprint(\"GRPO Completed (Mock)\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --- Final Inference Check ---\n# Load final model and run a test prompt\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(GRPO_OUTPUT_DIR)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\nprompt = \"User: Solve 2x + 5 = 15.\\nModel:\"\ninputs = tokenizer(prompt, return_tensors=\"jax\")\n# outputs = model.generate(**inputs, max_new_tokens=200)\n# print(tokenizer.decode(outputs[0]))\n","metadata":{},"outputs":[],"execution_count":null}]}