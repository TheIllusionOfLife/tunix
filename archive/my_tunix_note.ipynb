{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":119261,"databundleVersionId":14363498,"sourceType":"competition"},{"sourceId":7045423,"sourceType":"datasetVersion","datasetId":4054119,"isSourceIdPinned":false},{"sourceId":11350,"sourceType":"modelInstanceVersion","modelInstanceId":8367,"modelId":3301},{"sourceId":85992,"sourceType":"modelInstanceVersion","modelInstanceId":72251,"modelId":76277}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:59:18.320689Z","iopub.execute_input":"2025-11-14T21:59:18.320933Z","iopub.status.idle":"2025-11-14T21:59:20.822639Z","shell.execute_reply.started":"2025-11-14T21:59:18.320915Z","shell.execute_reply":"2025-11-14T21:59:20.821841Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"/kaggle/input/gemma-2/flax/gemma2-2b-it/1/tokenizer.model\n/kaggle/input/gemma-2/flax/gemma2-2b-it/1/gemma2-2b-it/_METADATA\n/kaggle/input/gemma-2/flax/gemma2-2b-it/1/gemma2-2b-it/manifest.ocdbt\n/kaggle/input/gemma-2/flax/gemma2-2b-it/1/gemma2-2b-it/checkpoint\n/kaggle/input/gemma-2/flax/gemma2-2b-it/1/gemma2-2b-it/_CHECKPOINT_METADATA\n/kaggle/input/gemma-2/flax/gemma2-2b-it/1/gemma2-2b-it/ocdbt.process_0/manifest.ocdbt\n/kaggle/input/gemma-2/flax/gemma2-2b-it/1/gemma2-2b-it/ocdbt.process_0/d/834bb4bf1e3854eb09f6208c95c071b2\n/kaggle/input/gemma-2/flax/gemma2-2b-it/1/gemma2-2b-it/ocdbt.process_0/d/bf69258061ae5f35eb7a5669fe6877d4\n/kaggle/input/gemma-2/flax/gemma2-2b-it/1/gemma2-2b-it/ocdbt.process_0/d/fc20151969d7ca91ea9d8275bda0e219\n/kaggle/input/gemma-2/flax/gemma2-2b-it/1/gemma2-2b-it/descriptor/descriptor.pbtxt\n/kaggle/input/gemma-2/flax/gemma2-2b-it/1/gemma2-2b-it/d/b5a4695f4be0a2f41ec1e25616ebd7e7\n/kaggle/input/google-tunix-hackathon/Hackathon dataset.txt\n/kaggle/input/gemma/flax/2b-it/2/tokenizer.model\n/kaggle/input/gemma/flax/2b-it/2/2b-it/manifest.0000000000000002\n/kaggle/input/gemma/flax/2b-it/2/2b-it/_METADATA\n/kaggle/input/gemma/flax/2b-it/2/2b-it/manifest.ocdbt\n/kaggle/input/gemma/flax/2b-it/2/2b-it/checkpoint\n/kaggle/input/gemma/flax/2b-it/2/2b-it/manifest.0000000000000001\n/kaggle/input/gemma/flax/2b-it/2/2b-it/ocdbt.process_0/manifest.0000000000000002\n/kaggle/input/gemma/flax/2b-it/2/2b-it/ocdbt.process_0/manifest.0000000000000003\n/kaggle/input/gemma/flax/2b-it/2/2b-it/ocdbt.process_0/manifest.ocdbt\n/kaggle/input/gemma/flax/2b-it/2/2b-it/ocdbt.process_0/manifest.0000000000000001\n/kaggle/input/gemma/flax/2b-it/2/2b-it/ocdbt.process_0/d/d5f1ece99269e01acd7a1be9bbaa751d\n/kaggle/input/gemma/flax/2b-it/2/2b-it/ocdbt.process_0/d/c21015490beaedb201ea7ac26b56cfdf\n/kaggle/input/gemma/flax/2b-it/2/2b-it/d/d501557e7c0f1656a4cbf1328f086d0e\n/kaggle/input/grade-school-math-8k-q-a/main_test.csv\n/kaggle/input/grade-school-math-8k-q-a/main_train.csv\n/kaggle/input/grade-school-math-8k-q-a/socratic_train.csv\n/kaggle/input/grade-school-math-8k-q-a/socratic_test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Suppress asyncio error messages (they are non-critical)\nimport logging\nlogging.getLogger('asyncio').setLevel(logging.CRITICAL)\nprint(\"‚úì Asyncio logging suppressed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:59:22.393205Z","iopub.execute_input":"2025-11-14T21:59:22.393516Z","iopub.status.idle":"2025-11-14T21:59:22.396923Z","shell.execute_reply.started":"2025-11-14T21:59:22.393498Z","shell.execute_reply":"2025-11-14T21:59:22.396181Z"}},"outputs":[{"name":"stdout","text":"‚úì Asyncio logging suppressed\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Tunix (Google‚Äôs new JAX-native library)\n\n\"Teach a LLM to reason using Tunix, Google‚Äôs new JAX-native library for LLM post-training.\"\n\n\"Most open-source or open-weight language models can give you an answer. But they typically **don‚Äôt ‚Äòshow their work‚Äô** - the steps they went through to arrive at that conclusion in a consistent manner. Here, you‚Äôll use Tunix, Google‚Äôs new JAX-native library for LLM post-training, to train a model to show its work by laying out a reasoning trace before landing on an answer.\"","metadata":{}},{"cell_type":"markdown","source":"## Competition Citation\n\n@misc{google-tunix-hackathon,\n    author = {Wei Wei and Mar√≠a Cruz},\n    title = {Google Tunix Hack - Train a model to show its work},\n    \n    year = {2025},\n    howpublished = {\\url{https://kaggle.com/competitions/google-tunix-hackathon}},\n    note = {Kaggle}\n}","metadata":{}},{"cell_type":"markdown","source":"## Tunix brought \"Don't show their work\" to a stratospheric level.\n\nNo outputs to the text file :) ","metadata":{}},{"cell_type":"code","source":"# http://www.gutenberg.org/cache/epub/730/pg730.txt\ntunix = open(\"../input/google-tunix-hackathon/Hackathon dataset.txt\", \"r\", encoding ='cp1251').read()\nprint (tunix[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:59:27.064358Z","iopub.execute_input":"2025-11-14T21:59:27.064601Z","iopub.status.idle":"2025-11-14T21:59:27.075093Z","shell.execute_reply.started":"2025-11-14T21:59:27.064585Z","shell.execute_reply":"2025-11-14T21:59:27.074332Z"}},"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Tunix on TPU\n\n\"In this hackathon, you‚Äôll start with Google‚Äôs open-weight Gemma model (Gemma2 2B or Gemma3 1B), fine-tune it with **Tunix on TPU**, and teach it how to reason through **complex questions**. You‚Äôll create a model that not only gets the right answer, but also explains **how it got there**.\"\n\n\"The solution should include a working training pipeline using **Tunix and Gemma**. Share your configs, reward function composition, and recipes so others can reproduce your results and build on them.\"\n\nhttps://www.kaggle.com/competitions/google-tunix-hackathon/overview","metadata":{}},{"cell_type":"markdown","source":"## Install Libraries","metadata":{}},{"cell_type":"code","source":"# Install packages in correct order to avoid conflicts\n!pip install -q wandb\n!pip install -q kagglehub\n!pip install -q ipywidgets\n!pip install -q tensorflow\n!pip install -q tensorflow_datasets\n!pip install -q tensorboardX\n!pip install -q transformers\n!pip install -q grain\n!pip install -q datasets\n\n# Uninstall existing flax/jax\n!pip uninstall -q -y flax jax jaxlib\n\n# Install google-tunix which will pull correct versions\n!pip install -q \"google-tunix[prod]==0.1.3\"\n\n# Verify versions\nimport jax\nimport flax\nprint(f\"JAX version: {jax.__version__}\")\nprint(f\"Flax version: {flax.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:01:52.231920Z","iopub.execute_input":"2025-11-14T22:01:52.232191Z","iopub.status.idle":"2025-11-14T22:02:10.287526Z","shell.execute_reply.started":"2025-11-14T22:01:52.232165Z","shell.execute_reply":"2025-11-14T22:02:10.286714Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nJAX version: 0.8.0\nFlax version: 0.12.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Attach Secret","metadata":{}},{"cell_type":"code","source":"import wandb, os\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    os.environ['WANDB_API_KEY'] = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n    print(\"‚úì WandB API key loaded\")\nexcept:\n    print(\"‚ö† WandB API key not available (skip if submission)\")\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:02:29.197715Z","iopub.execute_input":"2025-11-14T22:02:29.197974Z","iopub.status.idle":"2025-11-14T22:02:30.400599Z","shell.execute_reply.started":"2025-11-14T22:02:29.197956Z","shell.execute_reply":"2025-11-14T22:02:30.399643Z"}},"outputs":[{"name":"stdout","text":"‚úì WandB API key loaded\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Import humanize","metadata":{}},{"cell_type":"code","source":"from flax.nnx import Variable\n\n# Qwix/Flax 0.12.0 ‰∫íÊèõÊÄß„Éë„ÉÉ„ÉÅ\n_original_set_metadata = Variable.set_metadata\n\ndef _patched_set_metadata(self, *args, **kwargs):\n    \"\"\"\n    Qwix uses old Flax API: variable.set_metadata('key', value)\n    Flax 0.12.0 uses new API: variable.set_metadata(key=value)\n    This patch converts old format to new format.\n    \"\"\"\n    if len(args) == 2 and len(kwargs) == 0:\n        # Old API format detected - convert to new format\n        key, value = args\n        kwargs = {key: value}\n        args = ()\n    return _original_set_metadata(self, *args, **kwargs)\n\nVariable.set_metadata = _patched_set_metadata\nprint(\"‚úì Qwix/Flax 0.12.0 compatibility patch applied\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:02:56.556648Z","iopub.execute_input":"2025-11-14T22:02:56.557278Z","iopub.status.idle":"2025-11-14T22:02:56.562032Z","shell.execute_reply.started":"2025-11-14T22:02:56.557258Z","shell.execute_reply":"2025-11-14T22:02:56.561083Z"}},"outputs":[{"name":"stdout","text":"‚úì Qwix/Flax 0.12.0 compatibility patch applied\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import functools\nimport gc\nimport os\nfrom pprint import pprint\nimport re\nimport csv\nimport shutil\n\n# Import flax first, then configure\nimport flax\nfrom flax import nnx\n\n# REMOVE this line - it causes issues with TPU sharding\n# flax.config.update('flax_always_shard_variable', False)\n\nimport grain\nimport humanize\nimport jax\nimport jax.numpy as jnp\nimport kagglehub\nimport optax\nfrom orbax import checkpoint as ocp\nfrom pathlib import Path\nimport qwix\nimport tensorflow_datasets as tfds\nfrom tqdm.auto import tqdm\nfrom tunix.generate import sampler as sampler_lib\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\nfrom tunix.models.gemma import model as gemma_lib\nfrom tunix.models.gemma import params as params_lib\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.sft import metrics_logger\n\nprint(\"‚úì All imports successful\")\nprint(f\"JAX version: {jax.__version__}\")\nprint(f\"Flax version: {flax.__version__}\")\nprint(f\"JAX devices: {jax.devices()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:02:30.401433Z","iopub.execute_input":"2025-11-14T22:02:30.401871Z","iopub.status.idle":"2025-11-14T22:02:56.556088Z","shell.execute_reply.started":"2025-11-14T22:02:30.401852Z","shell.execute_reply":"2025-11-14T22:02:56.554780Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"‚úì All imports successful\nJAX version: 0.8.0\nFlax version: 0.12.0\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1763157768.191835      12 common_lib.cc:648] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:238\n","output_type":"stream"},{"name":"stdout","text":"JAX devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=4, process_index=0, coords=(0,2,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(1,2,0), core_on_chip=0), TpuDevice(id=6, process_index=0, coords=(0,3,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,3,0), core_on_chip=0)]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{}},{"cell_type":"code","source":"# ====== Data ======\nTRAIN_DATA_DIR = \"./data/train\"\nTEST_DATA_DIR = \"./data/test\"\nTRAIN_FRACTION = 1.0\n\n# ====== LoRA ======\nRANK = 64\nALPHA = 64.0\n\n# ====== Sharding ======\nMESH = [(1, 4), (\"fsdp\", \"tp\")]\n\n# ====== GRPO ======\n# === Generation during GRPO training ===\nMAX_PROMPT_LENGTH = 256\nTOTAL_GENERATION_STEPS = 256\n# Important to keep a high-ish temperature for varied, diverse responses during training.\nTEMPERATURE = 0.9\nTOP_P = 1.0\nTOP_K = 50\n# The number of times the policy generates multiple responses for a given prompt\n# within a single training step.\nNUM_GENERATIONS = 2\n\n# === other GRPO configs ===\n# The number of iterations per batch (ùúá in GRPO algo 1).\nNUM_ITERATIONS = 1\n# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\nBETA = 0.08\n# Epsilon value for clipping (ùúÄ in GRPO loss in paper).\nEPSILON = 0.2\n\n# ====== Training ======\nTRAIN_MICRO_BATCH_SIZE = 1\n# Reduce number of batches significantly to avoid OOM\nNUM_BATCHES = 100\nNUM_TEST_BATCHES = 50\nEVAL_EVERY_N_STEPS = 10\nNUM_EPOCHS = 1\n\n# Number of training steps.\nMAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n\n# === AdamW, warmup, cosine scheduler ===\nLEARNING_RATE = 3e-6\nB1 = 0.9\nB2 = 0.99\nWEIGHT_DECAY = 0.1\nWARMUP_STEPS = 0.1 * MAX_STEPS\nMAX_GRAD_NORM = 0.1\n\n# Checkpoint saving\nINTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\nCKPT_DIR = \"/tmp/content/ckpts/\"\nSAVE_INTERVAL_STEPS = 50\nMAX_TO_KEEP = 2\n\n# ====== Inference ======\nGENERATION_CONFIGS = {\n    \"greedy\": {\"temperature\": 0.3, \"top_k\": 10, \"top_p\": 0.9},\n    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n}\n\nprint(f\"Configuration set:\")\nprint(f\"  MAX_STEPS: {MAX_STEPS}\")\nprint(f\"  TRAIN_MICRO_BATCH_SIZE: {TRAIN_MICRO_BATCH_SIZE}\")\nprint(f\"  NUM_GENERATIONS: {NUM_GENERATIONS}\")\nprint(f\"  TOTAL_GENERATION_STEPS: {TOTAL_GENERATION_STEPS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:19:36.480374Z","iopub.execute_input":"2025-11-14T21:19:36.481080Z","iopub.status.idle":"2025-11-14T21:19:36.488709Z","shell.execute_reply.started":"2025-11-14T21:19:36.481055Z","shell.execute_reply":"2025-11-14T21:19:36.487701Z"}},"outputs":[{"name":"stdout","text":"Configuration set:\n  MAX_STEPS: 100\n  TRAIN_MICRO_BATCH_SIZE: 1\n  NUM_GENERATIONS: 2\n  TOTAL_GENERATION_STEPS: 256\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Memory usage","metadata":{}},{"cell_type":"code","source":"def show_hbm_usage():\n  \"\"\"Displays memory usage per device.\"\"\"\n  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n\n  for d in jax.local_devices():\n    stats = d.memory_stats()\n    used = stats[\"bytes_in_use\"]\n    limit = stats[\"bytes_limit\"]\n    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:19:36.489401Z","iopub.execute_input":"2025-11-14T21:19:36.489582Z","iopub.status.idle":"2025-11-14T21:19:36.502457Z","shell.execute_reply.started":"2025-11-14T21:19:36.489566Z","shell.execute_reply":"2025-11-14T21:19:36.501471Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"code","source":"reasoning_start = \"<reasoning>\"\nreasoning_end = \"</reasoning>\"\nsolution_start = \"<answer>\"\nsolution_end = \"</answer>\"\n\n\nSYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\nprovide your reasoning. Place it between {reasoning_start} and \\\n{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\nvalue) between {solution_start} and {solution_end}.\"\"\"\n\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\n\n{question}<end_of_turn>\n<start_of_turn>model\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:19:36.503024Z","iopub.execute_input":"2025-11-14T21:19:36.503205Z","iopub.status.idle":"2025-11-14T21:19:36.509735Z","shell.execute_reply.started":"2025-11-14T21:19:36.503190Z","shell.execute_reply":"2025-11-14T21:19:36.508837Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## The Devastator is here \n\nOriginal dataset was published by The Devastator (OpenAI's GSM8K dataset)\n\nthedevastator/grade-school-math-8k-q-a","metadata":{}},{"cell_type":"code","source":"def extract_hash_answer(text: str) -> str | None:\n  if \"####\" not in text:\n    return None\n  return text.split(\"####\")[1].strip()\n\n\ndef _load_from_tfds(data_dir: str, split: str):\n  import tensorflow_datasets.text.gsm8k\n  return tfds.data_source(\n      \"gsm8k\",\n      split=split,\n      data_dir=data_dir,\n      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n      download=True,\n  )\n\n\ndef download_kaggle_dataset(target_dir=\"./data/gsm8k\"):\n  os.makedirs(target_dir, exist_ok=True)\n  src = kagglehub.dataset_download(\"thedevastator/grade-school-math-8k-q-a\")\n  src = Path(src)\n  dst = Path(target_dir)\n\n  for csv_file in src.glob(\"*.csv\"):  # match all CSV files\n    shutil.copy2(csv_file, dst / csv_file.name)\n    print(f\"Copied {csv_file.name} ‚Üí {dst/csv_file.name}\")\n  return target_dir\n\n\ndef get_dataset(data_dir, split=\"train\", source=\"tfds\") -> grain.MapDataset:\n  # Download data\n  if not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n  if source == \"tfds\":\n    import tensorflow_datasets.text.gsm8k\n    data = tfds.data_source(\n        \"gsm8k\",\n        split=split,\n        data_dir=data_dir,\n        builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n        download=True,\n    )\n\n  elif source == \"kaggle\":\n    kaggle_dir = download_kaggle_dataset(data_dir)\n    file_name = \"main_\" + split + \".csv\"\n    csv_path = os.path.join(kaggle_dir, file_name)  # adjust filename if needed\n\n    data = []\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n      reader = csv.DictReader(csvfile)\n      for row in reader:\n        data.append({\n            \"question\": row[\"question\"],\n            \"answer\": row[\"answer\"],\n        })\n\n  else:\n    raise ValueError(f\"Unknown source: {source}\")\n\n  def _as_text(v):\n    return v if isinstance(v, str) else v.decode(\"utf-8\")\n\n  dataset = (\n      grain.MapDataset.source(data)\n      .shuffle(seed=42)\n      .map(\n          lambda x: {\n              # passed to model forward pass\n              \"prompts\": TEMPLATE.format(\n                  system_prompt=SYSTEM_PROMPT,\n                  question=_as_text(x[\"question\"]),\n              ),\n              # passed to reward functions\n              \"question\": _as_text(x[\"question\"]),\n              # passed to reward functions\n              \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n          }\n      )\n  )\n  return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:19:36.510401Z","iopub.execute_input":"2025-11-14T21:19:36.510574Z","iopub.status.idle":"2025-11-14T21:19:36.519693Z","shell.execute_reply.started":"2025-11-14T21:19:36.510560Z","shell.execute_reply":"2025-11-14T21:19:36.518839Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Only Adding some Dataset we can go forward.","metadata":{}},{"cell_type":"code","source":"# source = input(\"Choose data source [tfds/kaggle]: \").strip().lower()\nsource = 'kaggle'\n\nif source not in (\"tfds\", \"kaggle\"):\n  print(\"Invalid choice. Defaulting to 'tfds'.\")\n  source = \"tfds\"\n\nprint(f\"Using data source: {source}\")\n\ndataset = get_dataset(TRAIN_DATA_DIR, \"train\", source).batch(TRAIN_MICRO_BATCH_SIZE)[\n    :NUM_BATCHES\n]\n\nif TRAIN_FRACTION == 1.0:\n  train_dataset = dataset.repeat(NUM_EPOCHS)\n  val_dataset = None\nelse:\n  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n\n  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n\ntest_dataset = get_dataset(TEST_DATA_DIR, \"test\", source).batch(TRAIN_MICRO_BATCH_SIZE)[\n    :NUM_TEST_BATCHES\n]\n\ndataset_lengths = (\n    len(train_dataset),\n    len(val_dataset) if val_dataset is not None else 0,\n    len(test_dataset),\n)\nprint(f\"dataset contains {dataset_lengths} of batches\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:19:36.520430Z","iopub.execute_input":"2025-11-14T21:19:36.520608Z","iopub.status.idle":"2025-11-14T21:19:36.891840Z","shell.execute_reply.started":"2025-11-14T21:19:36.520593Z","shell.execute_reply":"2025-11-14T21:19:36.890569Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Using data source: kaggle\nCopied main_test.csv ‚Üí data/train/main_test.csv\nCopied main_train.csv ‚Üí data/train/main_train.csv\nCopied socratic_train.csv ‚Üí data/train/socratic_train.csv\nCopied socratic_test.csv ‚Üí data/train/socratic_test.csv\nCopied main_test.csv ‚Üí data/test/main_test.csv\nCopied main_train.csv ‚Üí data/test/main_train.csv\nCopied socratic_train.csv ‚Üí data/test/socratic_train.csv\nCopied socratic_test.csv ‚Üí data/test/socratic_test.csv\ndataset contains (100, 0, 50) of batches\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## One batch of the training dataset looks like.","metadata":{}},{"cell_type":"code","source":"for ele in train_dataset[:1]:\n  pprint(ele)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:19:36.892425Z","iopub.execute_input":"2025-11-14T21:19:36.892616Z","iopub.status.idle":"2025-11-14T21:19:36.898039Z","shell.execute_reply.started":"2025-11-14T21:19:36.892596Z","shell.execute_reply":"2025-11-14T21:19:36.897174Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"{'answer': array(['3'], dtype='<U1'),\n 'prompts': array(['<start_of_turn>user\\nYou are given a problem. Think about the problem and provide your reasoning. Place it between <reasoning> and </reasoning>. Then, provide the final answer (i.e., just one numerical value) between <answer> and </answer>.\\n\\nMaria has 4 dimes, 4 quarters, and 7 nickels in her piggy bank. Her mom gives her 5 quarters. How much money, in dollars, does Maria have now?<end_of_turn>\\n<start_of_turn>model'],\n      dtype='<U417'),\n 'question': array(['Maria has 4 dimes, 4 quarters, and 7 nickels in her piggy bank. Her mom gives her 5 quarters. How much money, in dollars, does Maria have now?'],\n      dtype='<U142')}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:19:36.898684Z","iopub.execute_input":"2025-11-14T21:19:36.898901Z","iopub.status.idle":"2025-11-14T21:19:36.905740Z","shell.execute_reply.started":"2025-11-14T21:19:36.898883Z","shell.execute_reply":"2025-11-14T21:19:36.904900Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Log in\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n  kagglehub.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:19:36.906433Z","iopub.execute_input":"2025-11-14T21:19:36.906603Z","iopub.status.idle":"2025-11-14T21:19:36.921533Z","shell.execute_reply.started":"2025-11-14T21:19:36.906587Z","shell.execute_reply":"2025-11-14T21:19:36.920776Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26879aff01dd44d38efd7a9605f111fa"}},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Gemma2\": \"google/gemma-2/flax","metadata":{}},{"cell_type":"code","source":"model_path = {\n    \"gemma2\": \"google/gemma-2/flax/\",\n}\nmodel_family = \"gemma2\"\nmodel_version = \"gemma2-2b-it\"\nprint(f\"{model_path[model_family]}{model_version}\")\n\nkaggle_ckpt_path = kagglehub.model_download(\n    f\"{model_path[model_family]}{model_version}\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:19:36.922119Z","iopub.execute_input":"2025-11-14T21:19:36.922286Z","iopub.status.idle":"2025-11-14T21:19:37.776288Z","shell.execute_reply.started":"2025-11-14T21:19:36.922271Z","shell.execute_reply":"2025-11-14T21:19:37.775375Z"}},"outputs":[{"name":"stdout","text":"google/gemma-2/flax/gemma2-2b-it\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## TPU is required here.","metadata":{}},{"cell_type":"code","source":"!rm /tmp/content/intermediate_ckpt/* -rf\n!rm /tmp/content/ckpts/* -rf\n\nif model_family == \"gemma2\":\n    print(\"Loading Gemma2-2B-IT model parameters...\")\n    params = params_lib.load_and_format_params(\n        os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\")\n    )\n    print(\"‚úì Parameters loaded\")\n    \n    print(\"Creating Transformer model...\")\n    gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n    print(\"‚úì Transformer created\")\n    \n    print(\"Saving intermediate checkpoint...\")\n    checkpointer = ocp.StandardCheckpointer()\n    _, state = nnx.split(gemma)\n    \n    # Save synchronously to avoid async errors\n    checkpointer.save(\n        os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), \n        state,\n        force=True  # Force synchronous save\n    )\n    \n    # Wait for completion (this may show warnings but is safe)\n    try:\n        checkpointer.wait_until_finished()\n        print(\"‚úì Checkpoint saved\")\n    except Exception as e:\n        print(f\"‚ö† Async warning (can be ignored): {e}\")\n        # Check if file was actually saved\n        if os.path.exists(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")):\n            print(\"‚úì Checkpoint file verified to exist\")\n        else:\n            raise RuntimeError(\"Checkpoint save failed!\")\n    \n    # Clean up memory\n    del params\n    del gemma\n    del state\n    gc.collect()\n    print(\"‚úì Memory cleaned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:19:37.777344Z","iopub.execute_input":"2025-11-14T21:19:37.777524Z","iopub.status.idle":"2025-11-14T21:20:28.898321Z","shell.execute_reply.started":"2025-11-14T21:19:37.777507Z","shell.execute_reply":"2025-11-14T21:20:28.896856Z"},"_kg_hide-output":true},"outputs":[{"name":"stderr","text":"WARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n","output_type":"stream"},{"name":"stdout","text":"Loading Gemma2-2B-IT model parameters...\n‚úì Parameters loaded\nCreating Transformer model...\n‚úì Transformer created\nSaving intermediate checkpoint...\n‚úì Checkpoint saved\n‚úì Memory cleaned\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"##  LoRA Application","metadata":{}},{"cell_type":"code","source":"def get_gemma_ref_model(ckpt_path, mesh): \n    model_config = gemma_lib.ModelConfig.gemma2_2b()\n    abs_gemma: nnx.Module = nnx.eval_shape(\n        lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n    )\n    abs_state = nnx.state(abs_gemma)\n    abs_state = jax.tree.map(\n        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n        abs_state,\n        nnx.get_named_sharding(abs_state, mesh),\n    )\n    checkpointer = ocp.StandardCheckpointer()\n    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n    \n    graph_def, _ = nnx.split(abs_gemma)\n    gemma = nnx.merge(graph_def, restored_params)\n    return gemma, model_config \n\n\ndef get_lora_model(base_model, mesh):\n    lora_provider = qwix.LoraProvider(\n        module_path=(\n            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n            \".*attn_vec_einsum\"\n        ),\n        rank=RANK,\n        alpha=ALPHA,\n    )\n    \n    model_input = base_model.get_model_input()\n    lora_model = qwix.apply_lora_to_model(\n        base_model, lora_provider, **model_input\n    )\n    \n    with mesh:\n        state = nnx.state(lora_model)\n        pspecs = nnx.get_partition_spec(state)\n        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n        nnx.update(lora_model, sharded_state)\n    \n    return lora_model","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create mesh FIRST (before loading model)\nprint(\"Creating TPU mesh...\")\nmesh = jax.make_mesh(*MESH)\nprint(f\"‚úì Mesh created: {mesh}\")\n\n# Reference model\nif model_family == \"gemma2\":\n    print(\"Loading reference model...\")\n    ref_model, model_config = get_gemma_ref_model(\n        ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"),\n        mesh=mesh  # Pass mesh as argument\n    )\n    print(\"‚úì Reference model loaded\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.501Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LoRA policy","metadata":{}},{"cell_type":"code","source":"# Policy model\nlora_policy = get_lora_model(ref_model, mesh=mesh)\nnnx.display(lora_policy)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.501Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"# Tokenizer\nimport sentencepiece as spm\ntokenizer = spm.SentencePieceProcessor()\ntokenizer.Load(os.path.join(kaggle_ckpt_path, \"tokenizer.model\"))\nprint(\"Loaded tokenizer using SentencePieceProcessor\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.501Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reward Function","metadata":{}},{"cell_type":"code","source":"import re\n\n# Define regex patterns\nmatch_format = re.compile(\n    rf\"^[\\s]{{0,}}\"\n    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n    rf\"{solution_start}(.+?){solution_end}\"\n    rf\"[\\s]{{0,}}$\",\n    flags=re.MULTILINE | re.DOTALL,\n)\n\nmatch_numbers = re.compile(\n    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n)\n\n# Reward function 1: Match format exactly (3 points)\ndef match_format_exactly(prompts, completions, **kwargs):\n    return [\n        0 if match_format.search(response) is None else 3.0\n        for response in completions\n    ]\n\n# Reward function 2: Match format approximately (partial points)\ndef match_format_approximately(prompts, completions, **kwargs):\n    scores = []\n    for completion in completions:\n        score = 0\n        score += 0.5 if completion.count(reasoning_start) == 1 else -0.5\n        score += 0.5 if completion.count(reasoning_end) == 1 else -0.5\n        score += 0.5 if completion.count(solution_start) == 1 else -0.5\n        score += 0.5 if completion.count(solution_end) == 1 else -0.5\n        scores.append(score)\n    return scores\n\n# Reward function 3: Check answer correctness\ndef check_answer(prompts, completions, answer, **kwargs):\n    extracted_responses = [\n        guess.group(1) if (guess := match_format.search(r)) is not None else None\n        for r in completions\n    ]\n    \n    scores = []\n    for guess, true_answer in zip(extracted_responses, answer):\n        score = 0\n        if guess is None:\n            scores.append(0)\n            continue\n        if guess == true_answer:\n            score += 3.0\n        elif guess.strip() == true_answer.strip():\n            score += 1.5\n        else:\n            try:\n                ratio = float(guess) / float(true_answer)\n                if ratio >= 0.9 and ratio <= 1.1:\n                    score += 0.5\n                elif ratio >= 0.8 and ratio <= 1.2:\n                    score += 0.25\n                else:\n                    score -= 1.0\n            except:\n                score -= 0.5\n        scores.append(score)\n    return scores\n\n# Reward function 4: Check extracted numbers\ndef check_numbers(prompts, completions, answer, **kwargs):\n    question = kwargs[\"question\"]\n    \n    extracted_responses = [\n        guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n        for r in completions\n    ]\n    \n    scores = []\n    if len(question) > 0 and len(answer) > 0 and len(completions) > 0:\n        print(\"START ============================\")\n        print(f\"Question: {question[0]}\")\n        print(f\"Answer: {answer[0]}\")\n        print(f\"Response: {completions[0][:100]}...\")\n        print(f\"Extracted: {extracted_responses[0]}\")\n        print(\"END ==============================\")\n    \n    for guess, true_answer in zip(extracted_responses, answer):\n        if guess is None:\n            scores.append(0)\n            continue\n        try:\n            true_answer_float = float(true_answer.strip())\n            guess_float = float(guess.strip())\n            scores.append(1.5 if guess_float == true_answer_float else 0.0)\n        except:\n            scores.append(0)\n    return scores\n\nprint(\"‚úì Reward functions defined (4 functions)\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.502Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optimizer","metadata":{}},{"cell_type":"code","source":"# Optimizer with warmup cosine decay and gradient clipping\noptimizer = optax.adamw(\n    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=LEARNING_RATE,\n        warmup_steps=WARMUP_STEPS,\n        decay_steps=MAX_STEPS,\n        end_value=0.0,\n    ),\n    b1=B1,\n    b2=B2,\n    weight_decay=WEIGHT_DECAY,\n)\nif MAX_GRAD_NORM is not None:\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n        optimizer,\n    )\nprint(\"‚úì Optimizer created\")\n\n# Checkpoint saving options\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS, \n    max_to_keep=MAX_TO_KEEP\n)\nprint(\"‚úì Checkpointing options created\")\n\n# Metrics logging options - DISABLED to avoid wandb issues\nmetrics_logging_options = None  # Disable WandB logging\nprint(\"‚úì Metrics logging options created (WandB disabled)\")\n\n# Cluster configuration\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine='vanilla',\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        metrics_logging_options=metrics_logging_options,  # None = no WandB\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=checkpointing_options,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n    ),\n)\nprint(\"‚úì ClusterConfig created\")\n\n# GRPO configuration\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=NUM_ITERATIONS,\n    beta=BETA,\n    epsilon=EPSILON,\n)\nprint(\"‚úì GRPOConfig created\")\n\n# RL cluster\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=ref_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\nprint(\"‚úì RLCluster created\")\n\n# GRPO Trainer\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[\n        match_format_exactly,\n        match_format_approximately,\n        check_answer,\n        check_numbers,\n    ],\n    grpo_config=grpo_config,\n)\n\nprint(\"\\n‚úÖ GRPO trainer created successfully!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.502Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"print(\"Starting GRPO training...\")\nprint(f\"Max steps: {MAX_STEPS}\")\nprint(f\"Save interval: {SAVE_INTERVAL_STEPS} steps\")\nprint(f\"Eval every: {EVAL_EVERY_N_STEPS} steps\")\nprint(f\"Dataset batches: {len(train_dataset)}\\n\")\n\n# Start training with mesh context\nwith mesh:\n    grpo_trainer.train(train_dataset)\n\nprint(\"\\n‚úÖ Training completed!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.503Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Checkpoint check","metadata":{}},{"cell_type":"code","source":"print(\"Loading trained checkpoint...\")\n\n# Auto-detect latest checkpoint\nckpt_base = os.path.join(CKPT_DIR, \"actor\")\nsaved_steps = sorted([int(d) for d in os.listdir(ckpt_base) if d.isdigit()])\nlatest_step = saved_steps[-1]\n\nprint(f\"Available checkpoints: {saved_steps}\")\nprint(f\"Loading from step: {latest_step}\")\n\ntrained_ckpt_path = os.path.join(ckpt_base, str(latest_step), \"model_params\")\n\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\ntrained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n\nnnx.update(\n    lora_policy,\n    jax.tree.map(lambda a, b: b, nnx.state(lora_policy, nnx.LoRAParam), trained_lora_params),\n)\n\nprint(f\"‚úì Trained checkpoint loaded from step {latest_step}\")\n\ncheckpoint_dir = os.path.join(ckpt_base, str(latest_step))\nprint(f\"\\nCheckpoint contents:\")\nfor item in os.listdir(checkpoint_dir):\n    item_path = os.path.join(checkpoint_dir, item)\n    if os.path.isfile(item_path):\n        size = os.path.getsize(item_path) / (1024**2)\n        print(f\"  {item}: {size:.2f} MB\")\n    else:\n        print(f\"  {item}/ (directory)\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Mock Evaluation","metadata":{}},{"cell_type":"code","source":"def generate(question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None):\n    \"\"\"Given prompt, generates text.\"\"\"\n    if isinstance(question, str):\n        input_batch = [\n            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question),\n        ]\n    else:\n        input_batch = [\n            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q)\n            for q in question\n        ]\n    \n    out_data = sampler(\n        input_strings=input_batch,\n        max_generation_steps=768,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        echo=False,\n        seed=seed,\n    )\n    \n    output = out_data.text\n    return output[0] if isinstance(question, str) else output\n\n\ndef evaluate(dataset, sampler, temperature=0.7, top_k=50, top_p=0.95, num_passes=1):\n    \"\"\"Computes accuracy and format matching percentage.\"\"\"\n    corr = 0\n    partially_corr = 0\n    corr_format = 0\n    total = 0\n    \n    for batch in tqdm(dataset):\n        answers = batch[\"answer\"]\n        questions = batch[\"question\"]\n        \n        for p in range(num_passes):\n            responses = generate(questions, sampler, temperature, top_k, top_p, seed=p)\n            \n            for response, answer in zip(responses, answers):\n                extracted = (\n                    guess.group(1)\n                    if (guess := match_numbers.search(response)) is not None\n                    else \"-1000000\"\n                )\n                try:\n                    if float(extracted.strip()) == float(answer.strip()):\n                        corr += 1\n                    ratio = float(extracted.strip()) / float(answer.strip())\n                    if 0.9 <= ratio <= 1.1:\n                        partially_corr += 1\n                except:\n                    pass\n                \n                if match_format.search(response) is not None:\n                    corr_format += 1\n                \n                total += 1\n                if total % 10 == 0:\n                    print(f\"Progress: {corr}/{total} correct ({corr/total*100:.1f}%)\")\n    \n    return (\n        corr, total, \n        corr / total * 100, \n        partially_corr / total * 100, \n        corr_format / total * 100\n    )\n\nprint(\"‚úì Evaluation functions defined\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create sampler with trained model\nsampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=1536,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)\n\n# Evaluate on test dataset\nprint(\"\\nEvaluating trained model on test set...\")\n(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n    test_dataset, sampler, **GENERATION_CONFIGS[\"greedy\"]\n)\nprint(f\"\\n{'='*50}\")\nprint(f\"FINAL RESULTS:\")\nprint(f\"Correct: {corr}/{total}\")\nprint(f\"Accuracy: {accuracy:.2f}%\")\nprint(f\"Partial Accuracy: {partial_accuracy:.2f}%\")\nprint(f\"Format Accuracy: {format_accuracy:.2f}%\")\nprint(f\"{'='*50}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create sampler with trained model\nsampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=1536,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)\n\n# Evaluate on test dataset\nprint(\"\\nEvaluating trained model on test set...\")\n(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n    test_dataset, sampler, **GENERATION_CONFIGS[\"standard\"]\n)\nprint(f\"\\n{'='*50}\")\nprint(f\"FINAL RESULTS:\")\nprint(f\"Correct: {corr}/{total}\")\nprint(f\"Accuracy: {accuracy:.2f}%\")\nprint(f\"Partial Accuracy: {partial_accuracy:.2f}%\")\nprint(f\"Format Accuracy: {format_accuracy:.2f}%\")\nprint(f\"{'='*50}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create sampler with trained model\nsampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=1536,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)\n\n# Evaluate on test dataset\nprint(\"\\nEvaluating trained model on test set...\")\n(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n    test_dataset, sampler, **GENERATION_CONFIGS[\"liberal\"]\n)\nprint(f\"\\n{'='*50}\")\nprint(f\"FINAL RESULTS:\")\nprint(f\"Correct: {corr}/{total}\")\nprint(f\"Accuracy: {accuracy:.2f}%\")\nprint(f\"Partial Accuracy: {partial_accuracy:.2f}%\")\nprint(f\"Format Accuracy: {format_accuracy:.2f}%\")\nprint(f\"{'='*50}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate sample outputs for demonstration\nsample_questions = [\n    \"What is 15 + 27?\",\n    \"If a train travels 60 miles per hour for 3 hours, how far does it travel?\",\n    \"What is 5 factorial (5!)?\",\n    \"A rectangle has length 8 cm and width 5 cm. What is its area?\",\n    \"If John has 3 apples and Mary gives him 4 more, how many apples does John have?\",\n]\n\nprint(\"=\"*80)\nprint(\"SAMPLE MODEL OUTPUTS - Demonstrating Reasoning Capability\")\nprint(\"=\"*80)\n\nfor i, q in enumerate(sample_questions, 1):\n    prompt = TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q)\n    out = sampler(\n        input_strings=[prompt],\n        max_generation_steps=512,\n        **GENERATION_CONFIGS[\"standard\"]\n    )\n    \n    response = out.text[0]\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"Example {i}:\")\n    print(f\"Question: {q}\")\n    print(f\"\\nModel Response:\")\n    print(response)\n    \n    # Check format\n    has_reasoning = '<reasoning>' in response and '</reasoning>' in response\n    has_answer = '<answer>' in response and '</answer>' in response\n    print(f\"\\n‚úì Format check: Reasoning={'‚úì' if has_reasoning else '‚úó'}, Answer={'‚úì' if has_answer else '‚úó'}\")\n    \nprint(f\"\\n{'='*80}\")\nprint(\"END OF SAMPLE OUTPUTS\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T21:54:10.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL: Test Model on Diverse Domains (Competition Evaluation Preview)\n# ============================================================================\n# Competition evaluates on diverse domains, not just math:\n# - Creative writing, Creative ideation, Summarization\n# - Math, Coding, Basic science, Other\n#\n# This cell tests whether GSM8K-only training is sufficient\n# ============================================================================\n\n# Diverse evaluation prompts\nDIVERSE_TEST_PROMPTS = [\n    # Math\n    {\n        \"domain\": \"Math\",\n        \"question\": \"If a rectangular garden is 12 meters long and 8 meters wide, what is its perimeter?\",\n    },\n    # Creative Writing\n    {\n        \"domain\": \"Creative Writing\",\n        \"question\": \"Write a short opening paragraph for a story about a detective who discovers that shadows are actually alive.\",\n    },\n    # Creative Ideation\n    {\n        \"domain\": \"Creative Ideation\",\n        \"question\": \"Brainstorm 3 innovative features for a smart water bottle designed for marathon runners.\",\n    },\n    # Summarization\n    {\n        \"domain\": \"Summarization\",\n        \"question\": \"Summarize this text in 2-3 sentences: 'Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods from carbon dioxide and water. Photosynthesis in plants generally involves the green pigment chlorophyll and generates oxygen as a byproduct. This process is fundamental to life on Earth as it provides the primary source of organic compounds and oxygen in the atmosphere.'\",\n    },\n    # Coding\n    {\n        \"domain\": \"Coding\",\n        \"question\": \"Write a Python function that checks if a given string is a palindrome (reads the same forwards and backwards). Explain your approach.\",\n    },\n    # Basic Science\n    {\n        \"domain\": \"Basic Science\",\n        \"question\": \"Explain why the sky appears blue during the day but turns red and orange during sunset.\",\n    },\n    # Logic/Reasoning\n    {\n        \"domain\": \"Logic\",\n        \"question\": \"If all roses are flowers, and some flowers fade quickly, can we conclude that some roses fade quickly?\",\n    },\n]\n\nprint(\"=\"*80)\nprint(\"DIVERSE DOMAIN EVALUATION - Preview of Competition Evaluation\")\nprint(\"=\"*80)\nprint(\"Testing model's ability to generalize beyond math problems\")\nprint(f\"Total test cases: {len(DIVERSE_TEST_PROMPTS)} domains\\n\")\n\nresults = []\nformat_pass_count = 0\n\nfor i, test_case in enumerate(DIVERSE_TEST_PROMPTS, 1):\n    domain = test_case[\"domain\"]\n    question = test_case[\"question\"]\n\n    # Generate response using sampler\n    prompt = TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question)\n    out = sampler(\n        input_strings=[prompt],\n        max_generation_steps=768,\n        echo=False,\n        seed=42,\n        **GENERATION_CONFIGS[\"standard\"]  # temp=0.7\n    )\n\n    response = out.text[0]\n\n    # Check format compliance\n    has_reasoning = '<reasoning>' in response and '</reasoning>' in response\n    has_answer = '<answer>' in response and '</answer>' in response\n    format_ok = has_reasoning and has_answer\n\n    if format_ok:\n        format_pass_count += 1\n\n    # Print output\n    print(f\"{'='*80}\")\n    print(f\"Test {i}/7 - Domain: {domain}\")\n    print(f\"{'='*80}\")\n    print(f\"Question: {question[:100]}{'...' if len(question) > 100 else ''}\")\n    print(f\"\\nModel Response:\")\n    print(response)\n    print(f\"\\n{'‚Äî'*40}\")\n    print(f\"Format: {'‚úì PASS' if format_ok else '‚úó FAIL'} (Reasoning: {'‚úì' if has_reasoning else '‚úó'}, Answer: {'‚úì' if has_answer else '‚úó'})\")\n    print()\n\n# Summary\nformat_pass_rate = format_pass_count / len(DIVERSE_TEST_PROMPTS) * 100\n\nprint(f\"{'='*80}\")\nprint(\"SUMMARY\")\nprint(f\"{'='*80}\")\nprint(f\"Format compliance: {format_pass_count}/{len(DIVERSE_TEST_PROMPTS)} ({format_pass_rate:.1f}%)\")\n\nif format_pass_rate >= 85:\n    print(\"\\n‚úì EXCELLENT: Model shows strong format compliance across domains\")\n    print(\"  ‚Üí GSM8K-only training may be sufficient for format learning\")\n    print(\"  ‚Üí Focus on improving reasoning quality\")\nelif format_pass_rate >= 60:\n    print(\"\\n‚ö† MODERATE: Model shows partial format compliance\")\n    print(\"  ‚Üí Consider adding diverse training examples\")\n    print(\"  ‚Üí Or increase GRPO training steps on GSM8K\")\nelse:\n    print(\"\\n‚úó WEAK: Model struggles with format in diverse domains\")\n    print(\"  ‚Üí Likely needs multi-domain training data\")\n    print(\"  ‚Üí GSM8K-only training insufficient\")\n\nprint(f\"{'='*80}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:38:08.683939Z","iopub.execute_input":"2025-11-14T21:38:08.684228Z","iopub.status.idle":"2025-11-14T21:38:08.825821Z","shell.execute_reply.started":"2025-11-14T21:38:08.684209Z","shell.execute_reply":"2025-11-14T21:38:08.824668Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nDIVERSE DOMAIN EVALUATION - Preview of Competition Evaluation\n================================================================================\nTesting model's ability to generalize beyond math problems\nTotal test cases: 7 domains\n\n","output_type":"stream"},{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Generate response using sampler\u001b[39;00m\n\u001b[32m     64\u001b[39m prompt = TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m out = \u001b[43msampler\u001b[49m(\n\u001b[32m     66\u001b[39m     input_strings=[prompt],\n\u001b[32m     67\u001b[39m     max_generation_steps=\u001b[32m768\u001b[39m,\n\u001b[32m     68\u001b[39m     echo=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     69\u001b[39m     seed=\u001b[32m42\u001b[39m,\n\u001b[32m     70\u001b[39m     **GENERATION_CONFIGS[\u001b[33m\"\u001b[39m\u001b[33mstandard\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# temp=0.7\u001b[39;00m\n\u001b[32m     71\u001b[39m )\n\u001b[32m     73\u001b[39m response = out.text[\u001b[32m0\u001b[39m]\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Check format compliance\u001b[39;00m\n","\u001b[31mNameError\u001b[39m: name 'sampler' is not defined"],"ename":"NameError","evalue":"name 'sampler' is not defined","output_type":"error"}],"execution_count":17},{"cell_type":"markdown","source":"## Kaggle model ID","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}