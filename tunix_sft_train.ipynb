{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec8e13e",
   "metadata": {},
   "source": [
    "# Tunix SFT: Teaching Reasoning Through Demonstration\n",
    "\n",
    "**Strategy**: Supervised Fine-Tuning on 180K high-quality reasoning traces from GlaiveAI.\n",
    "\n",
    "**Key Insight**: For 2B parameter models, learning from demonstrations is more effective than reinforcement learning. SFT provides dense supervision at every token, while RL provides sparse rewards only at sequence end.\n",
    "\n",
    "**Dataset**: GlaiveAI (glaiveai/reasoning-v1-20m)\n",
    "- Source: DeepSeek-R1-Distill-Llama-70B reasoning traces\n",
    "- Focus: Non-math/code domains (social science, creative writing, analytical reasoning)\n",
    "- License: Apache 2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ea938",
   "metadata": {},
   "source": [
    "\n",
    "## Overall training and evaluation strategy\n",
    "\n",
    "**Strategy: SFT on GlaiveAI Reasoning Traces**\n",
    "\n",
    "Competition FAQ explicitly states that verifiable tasks (math/coding) have \"much lower weights\". Our strategy prioritizes non-verifiable domains:\n",
    "\n",
    "1.  **Base Model**: We start with `Gemma-2-2b-it` for its instruction-following foundation.\n",
    "2.  **SFT Training**: We fine-tune on 180K reasoning traces from GlaiveAI (non-math/code focus).\n",
    "3.  **Format**: All data uses explicit `<reasoning>` and `<answer>` tags for structured outputs.\n",
    "\n",
    "## üó∫Ô∏è Workflow Diagram\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Gemma-2-2b-it] --> C[Tunix SFT Training]\n",
    "    B[GlaiveAI 180K Dataset] --> C\n",
    "    C --> D[Trained Model]\n",
    "    D --> E[Submission]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39687eca",
   "metadata": {},
   "source": [
    "\n",
    "## How your finetuning dataset is created\n",
    "\n",
    "We use a **single high-quality dataset** from GlaiveAI, chosen for alignment with competition evaluation:\n",
    "\n",
    "| Dataset | Source | Samples | Domain | License |\n",
    "|:---|:---|:---:|:---|:---|\n",
    "| GlaiveAI | [glaiveai/reasoning-v1-20m](https://huggingface.co/datasets/glaiveai/reasoning-v1-20m) | 180K | Non-math/code | Apache 2.0 |\n",
    "\n",
    "**Why GlaiveAI-only?**\n",
    "- 2025 model quality (DeepSeek-R1-Distill-70B)\n",
    "- Focus on creative, analytical, and social science domains\n",
    "- Competition deprioritizes math/code (FAQ: \"much lower weights\")\n",
    "\n",
    "**Adaptive Filtering:**\n",
    "To prevent dataset collapse from aggressive length filtering, we use an adaptive threshold strategy:\n",
    "1. Try 95th percentile (10.4k chars).\n",
    "2. If <80% data retained, relax to 12.5k chars.\n",
    "3. If still <80%, relax to 15k chars.\n",
    "4. As a last resort, disable filtering.\n",
    "This ensures we train on a representative >80% of the dataset logic.\n",
    "\n",
    "Dataset is split into two parquet files (90K each) to prevent OOM on Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3082f0d",
   "metadata": {},
   "source": [
    "## Tunix finetuning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220feb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training parameters\n",
    "TEMPERATURE=0.7\n",
    "TOP_K=50\n",
    "TOP_P=0.9\n",
    "MAX_GENERATION_STEPS=1024  # Aligned with EVAL_MAX_TOKENS\n",
    "\n",
    "# Output Tags\n",
    "REASONING_START = \"<reasoning>\"\n",
    "REASONING_END = \"</reasoning>\"\n",
    "SOLUTION_START = \"<answer>\"\n",
    "SOLUTION_END = \"</answer>\"\n",
    "\n",
    "# Inference Params\n",
    "INF_TEMPERATURE=0\n",
    "INF_TOP_K=1\n",
    "INF_TOP_P=None\n",
    "SEED=42\n",
    "\n",
    "# System prompt and template\n",
    "SYSTEM_PROMPT = \"You are a deep thinking AI. Think step by step about the problem and provide your reasoning between <reasoning> and </reasoning> tags. Then, provide the final answer between <answer> and </answer> tags.\"\n",
    "TEMPLATE = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{{question}}<end_of_turn>\\n<start_of_turn>model\"\n",
    "\n",
    "print(\"Template variables defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62940261",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup & Install ---\n",
    "!pip install -q wandb==0.22.0\n",
    "!pip install -q kagglehub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q transformers\n",
    "!pip install -q grain\n",
    "\n",
    "# Tunix/Qwix Installation\n",
    "# Check if we are offline (no internet), if so, assume wheels are attached\n",
    "import socket\n",
    "import os\n",
    "\n",
    "def is_connected():\n",
    "    try:\n",
    "        # Check simple connectivity\n",
    "        socket.create_connection((\"1.1.1.1\", 53))\n",
    "        return True\n",
    "    except OSError:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "if is_connected():\n",
    "    !pip install -q -U google-tunix[prod]==0.1.5 chex>=0.1.90 distrax==0.1.7 optax==0.2.6 listsafe\n",
    "    !pip install -q -U wandb\n",
    "    !pip install git+https://github.com/google/qwix\n",
    "else:\n",
    "    print(\"Offline mode detected. Assuming dependencies are installed or wheels provided.\")\n",
    "    # Fallback: Try installing from local wheels if available\n",
    "    if os.path.exists(\"/kaggle/input/tunix-wheels\"):\n",
    "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels google-tunix\n",
    "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels qwix\n",
    "\n",
    "\n",
    "# Fix Flax Version to 0.12.0 as required\n",
    "!pip uninstall -q -y flax\n",
    "!pip install flax==0.12.0\n",
    "\n",
    "!pip install -q datasets==3.2.0 optax==0.2.4 chex==0.1.88\n",
    "\n",
    "# --- Imports ---\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "from pprint import pprint\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "import tensorflow_datasets as tfds\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Tunix Imports\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma import model as gemma_lib\n",
    "from tunix.models.gemma import params as params_lib\n",
    "from tunix.sft import metrics_logger\n",
    "from tunix.sft import peft_trainer\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Stability Configs ---\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.95'\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "\n",
    "print(f\"JAX Devices: {jax.devices()}\")\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "DATASET_PATH = \"/kaggle/input/tunix-sft-data\"\n",
    "SFT_OUTPUT_DIR = \"/kaggle/working/sft_checkpoint\"\n",
    "\n",
    "# Tuning Hyperparams - Adjust these for HP tuning\n",
    "# SFT_STEPS is now calculated dynamically based on dataset size\n",
    "# SFT_STEPS = 22500 (Removed)\n",
    "TRAIN_BATCH_SIZE = 8 # Per-step batch size across all 8 TPU chips (1 sample/chip)\n",
    "GRADIENT_ACCUMULATION = 4  # Effective batch = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION\n",
    "EFFECTIVE_BATCH = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION  # 32\n",
    "\n",
    "# Learning Rate - Key HP for tuning\n",
    "LEARNING_RATE = 2e-5  # Try: 5e-5, 2e-5, 1e-5\n",
    "WARMUP_STEPS = 200  # Warmup before reaching peak LR\n",
    "\n",
    "# LoRA Hyperparams\n",
    "RANK = 64\n",
    "ALPHA = 64.0\n",
    "\n",
    "# Sequence Length\n",
    "MAX_SEQ_LEN = 2048  # Critical: increased from 1024 to avoid truncating reasoning\n",
    "\n",
    "# Inference Hyperparams (shared across all evaluations)\n",
    "INFERENCE_TEMPERATURE = 0.7\n",
    "INFERENCE_TOP_K = 50\n",
    "INFERENCE_TOP_P = 0.95\n",
    "EVAL_MAX_TOKENS = 1024  # Max tokens for eval generation (less than MAX_SEQ_LEN to save memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Utilities ---\n",
    "MESH = [(8, 1), (\"fsdp\", \"tp\")]\n",
    "\n",
    "def get_gemma_model(ckpt_path):\n",
    "    mesh = jax.make_mesh(*MESH)\n",
    "    model_config = gemma_lib.ModelConfig.gemma2_2b()\n",
    "    abs_gemma: nnx.Module = nnx.eval_shape(\n",
    "        lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
    "    )\n",
    "    abs_state = nnx.state(abs_gemma)\n",
    "    abs_state = jax.tree.map(\n",
    "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "        abs_state,\n",
    "        nnx.get_named_sharding(abs_state, mesh),\n",
    "    )\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "\n",
    "    graph_def, _ = nnx.split(abs_gemma)\n",
    "    gemma = nnx.merge(graph_def, restored_params)\n",
    "    return gemma, mesh, model_config\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    # LoRA config uses RANK and ALPHA from constants\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "            \".*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "\n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, rngs=nnx.Rngs(params=0), **model_input\n",
    "    )\n",
    "\n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "\n",
    "    return lora_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- WandB Logging with Metrics Backend ---\n",
    "WANDB_ENABLED = False\n",
    "\n",
    "# Define WandB Backend for MetricsLogger\n",
    "class WandbBackend:\n",
    "    '''Custom backend to stream metrics to WandB during training'''\n",
    "    def log_scalar(self, event: str, value, **kwargs):\n",
    "        if WANDB_ENABLED:\n",
    "            step = kwargs.get(\"step\", 0)\n",
    "            wandb.log({event: float(value)}, step=step)\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "    if secret_value:\n",
    "        wandb.login(key=secret_value)\n",
    "        # Log hyperparameters to WandB config\n",
    "        wandb.init(\n",
    "            project=\"tunix-sft-diverse\",\n",
    "            name=\"sft-run-v2\",\n",
    "            anonymous=\"allow\",\n",
    "            config={\n",
    "                \"sft_steps\": SFT_STEPS,\n",
    "                \"learning_rate\": LEARNING_RATE,\n",
    "                \"warmup_steps\": WARMUP_STEPS,\n",
    "                \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
    "                \"gradient_accumulation\": GRADIENT_ACCUMULATION,\n",
    "                \"effective_batch\": EFFECTIVE_BATCH,\n",
    "                \"max_seq_len\": MAX_SEQ_LEN,\n",
    "                \"lora_rank\": RANK,\n",
    "                \"lora_alpha\": ALPHA,\n",
    "                \"model_id\": MODEL_ID,\n",
    "            }\n",
    "        )\n",
    "        WANDB_ENABLED = True\n",
    "        print(\"WandB Logging Enabled with hyperparameter tracking.\")\n",
    "    else:\n",
    "        raise ValueError(\"Empty WANDB_API_KEY\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"WandB not enabled: {e}\")\n",
    "    os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "    print(\"Proceeding without cloud logging (WANDB_MODE='disabled').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Data Preprocessing (GlaiveAI-Only) ---\n",
    "# Strategy: Single high-quality dataset aligned with competition goals\n",
    "\n",
    "print(\"Loading GlaiveAI dataset...\")\n",
    "\n",
    "def standardize_glaive_format(prompt, response):\n",
    "    '''Convert GlaiveAI <think> format to <reasoning>/<answer> tags'''\n",
    "    \n",
    "    # GlaiveAI uses <think>...</think> for reasoning\n",
    "    text = response\n",
    "    \n",
    "    # Replace think tags with reasoning tags\n",
    "    text = re.sub(r\"<think>\", \"<reasoning>\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"</think>\", \"</reasoning>\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Extract reasoning and answer parts\n",
    "    reasoning_match = re.search(r\"<reasoning>(.*?)</reasoning>\", text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if reasoning_match:\n",
    "        reasoning = reasoning_match.group(1).strip()\n",
    "        # Get content after </reasoning> as answer\n",
    "        remaining = re.sub(r\"<reasoning>.*?</reasoning>\", \"\", text, flags=re.DOTALL | re.IGNORECASE).strip()\n",
    "        \n",
    "        if remaining:\n",
    "            answer = remaining\n",
    "        else:\n",
    "            # No content after reasoning - use summary\n",
    "            sentences = reasoning.split(\".\")\n",
    "            answer = sentences[-1].strip() if sentences else reasoning[:200]\n",
    "    else:\n",
    "        # No think tags - use whole response\n",
    "        reasoning = text[:500] if len(text) > 500 else text\n",
    "        answer = text\n",
    "    \n",
    "    # Format for Gemma\n",
    "    formatted = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n<reasoning>{reasoning}</reasoning>\\n<answer>{answer}</answer>\"\n",
    "    return formatted\n",
    "\n",
    "# Load from Kaggle Dataset (pre-downloaded parquet)\n",
    "# Uses streaming JSONL logic, no list initialization\n",
    "\n",
    "\n",
    "try:\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        import glob\n",
    "        parquet_files = glob.glob(f\"{DATASET_PATH}/*.parquet\")\n",
    "        \n",
    "        if parquet_files:\n",
    "            # 1. Pass 1: Counting (No Memory Load)\n",
    "            print(\"Pass 1: Scanning dataset for adaptive filtering...\")\n",
    "            total_samples = 0\n",
    "            threshold_counts = {t: 0 for t in [10400, 12500, 15000]}\n",
    "            threshold_counts[None] = 0 # No filter\n",
    "\n",
    "            for parquet_file in parquet_files:\n",
    "                ds = datasets.load_dataset(\"parquet\", data_files=parquet_file, split=\"train\")\n",
    "                print(f\"Scanning {os.path.basename(parquet_file)} ({len(ds)} samples)...\")\n",
    "                \n",
    "                for sample in ds:\n",
    "                    response_len = len(sample.get(\"response\", \"\"))\n",
    "                    total_samples += 1 # Count ALL samples for correct ratio\n",
    "                    \n",
    "                    if response_len < 50: continue # Always skip short/empty\n",
    "                    \n",
    "                    for t in threshold_counts:\n",
    "                        if t is None or response_len <= t:\n",
    "                            threshold_counts[t] += 1\n",
    "            \n",
    "            print(f\"Total samples scanned: {total_samples}\")\n",
    "\n",
    "            # 2. Select Threshold\n",
    "            selected_threshold = None\n",
    "            thresholds_ordered = [10400, 12500, 15000, None]\n",
    "            \n",
    "            for t in thresholds_ordered:\n",
    "                count = threshold_counts[t]\n",
    "                ratio = count / total_samples if total_samples > 0 else 0\n",
    "                print(f\"Threshold: {t} -> Kept: {count}/{total_samples} ({ratio:.2%})\")\n",
    "                \n",
    "                if ratio >= 0.8:\n",
    "                    selected_threshold = t\n",
    "                    print(f\"Selected Threshold: {selected_threshold}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"All thresholds yielded < 80% data. Disabling length filter.\")\n",
    "                selected_threshold = None\n",
    "\n",
    "            # 3. Pass 2: Loading & Formatting (Memory Safe)\n",
    "            print(\"Pass 2: Loading and formatting selected samples...\")\n",
    "            \n",
    "            # Stream directly to JSONL file to avoid holding all strings in RAM\n",
    "            with open(\"sft_data.jsonl\", \"w\") as f:\n",
    "                import json\n",
    "                for parquet_file in parquet_files:\n",
    "                    ds = datasets.load_dataset(\"parquet\", data_files=parquet_file, split=\"train\")\n",
    "                    for sample in ds:\n",
    "                        prompt = sample.get(\"prompt\", \"\")\n",
    "                        response = sample.get(\"response\", \"\")\n",
    "                        \n",
    "                        # Apply selected filter\n",
    "                        if len(response) < 50: continue\n",
    "                        if selected_threshold is not None and len(response) > selected_threshold:\n",
    "                            continue\n",
    "                            \n",
    "                        formatted = standardize_glaive_format(prompt, response)\n",
    "                        # Write line immediately\n",
    "                        f.write(json.dumps({\"text\": formatted}) + \"\\n\")\n",
    "            \n",
    "            # Load using memory-mapped Arrow dataset\n",
    "            print(\"Loading dataset from JSONL (Memory Safe)...\")\n",
    "            sft_dataset = datasets.load_dataset(\"json\", data_files=\"sft_data.jsonl\", split=\"train\")\n",
    "            dataset_size = len(sft_dataset)\n",
    "            print(f\"Final Dataset Size: {dataset_size}\")\n",
    "            \n",
    "            # Reduce disk pressure\n",
    "            if os.path.exists(\"sft_data.jsonl\"):\n",
    "                os.remove(\"sft_data.jsonl\")\n",
    "                print(\"Removed temporary sft_data.jsonl\")\n",
    "                \n",
    "        else:\n",
    "            raise FileNotFoundError(\"No parquet files found\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Dataset path {DATASET_PATH} not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Kaggle dataset not found ({e}), downloading from HuggingFace...\")\n",
    "    print(f\"Warning: This may be slow. Pre-download recommended.\")\n",
    "    \n",
    "    # Fallback: Stream from HuggingFace\n",
    "    ds = datasets.load_dataset(\"glaiveai/reasoning-v1-20m\", split=\"train\", streaming=True)\n",
    "    \n",
    "    count = 0\n",
    "    limit = 180000 \n",
    "    FALLBACK_THRESHOLD = 15000\n",
    "    print(f\"Adaptive filtering skipped in fallback mode. Using fixed safe threshold: {FALLBACK_THRESHOLD} chars.\")\n",
    "    \n",
    "    # Stream fallback to JSONL as well\n",
    "    with open(\"sft_data.jsonl\", \"w\") as f:\n",
    "        import json\n",
    "        for sample in ds:\n",
    "            prompt = sample.get(\"prompt\", \"\")\n",
    "            response = sample.get(\"response\", \"\")\n",
    "            \n",
    "            if len(response) > FALLBACK_THRESHOLD or len(response) < 50:\n",
    "                continue\n",
    "            \n",
    "            formatted = standardize_glaive_format(prompt, response)\n",
    "            f.write(json.dumps({\"text\": formatted}) + \"\\n\")\n",
    "            \n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(f\"  Downloaded {count} samples...\")\n",
    "\n",
    "            if count >= limit:\n",
    "                break\n",
    "    \n",
    "    sft_dataset = datasets.load_dataset(\"json\", data_files=\"sft_data.jsonl\", split=\"train\")\n",
    "    dataset_size = len(sft_dataset)\n",
    "    \n",
    "    if os.path.exists(\"sft_data.jsonl\"):\n",
    "        os.remove(\"sft_data.jsonl\")\n",
    "        print(\"Removed temporary sft_data.jsonl\")\n",
    "\n",
    "# --- Dynamic SFT Steps Calculation ---\n",
    "# Target: ~4 epochs\n",
    "# SAFETY: Use math.ceil and max(1, ...) to prevent 0 steps or rounding down\n",
    "import math\n",
    "TARGET_EPOCHS = 4\n",
    "SFT_STEPS = max(1, math.ceil((dataset_size * TARGET_EPOCHS) / EFFECTIVE_BATCH))\n",
    "print(f\"Dynamic SFT Steps: {SFT_STEPS} (based on {dataset_size} samples, 4 epochs, effective batch {EFFECTIVE_BATCH})\")\n",
    "\n",
    "print(f\"Total samples after preprocessing: {dataset_size}\")\n",
    "\n",
    "# Shuffle\n",
    "sft_dataset = sft_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Final SFT dataset: {len(sft_dataset)} samples\")\n",
    "print(f\"Sample: {sft_dataset[0]['text'][:500]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3649860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Main Training Logic ---\n",
    "\n",
    "# 1. Download/setup Base Model\n",
    "if \"KAGGLE_USERNAME\" not in os.environ:\n",
    "    kagglehub.login()\n",
    "\n",
    "# Download Gemma 2 (Flax)\n",
    "model_path = { \"gemma2\": \"google/gemma-2/flax/\" }\n",
    "model_family = \"gemma2\"\n",
    "model_version = \"gemma2-2b-it\" \n",
    "kaggle_ckpt_path = kagglehub.model_download(f\"{model_path[model_family]}{model_version}\")\n",
    "\n",
    "# Convert checkpoint format for Tunix/NNX\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
    "!rm -rf {INTERMEDIATE_CKPT_DIR} {CKPT_DIR}\n",
    "\n",
    "params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\"))\n",
    "gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "_, state = nnx.split(gemma)\n",
    "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "checkpointer.wait_until_finished()\n",
    "del params, gemma, state\n",
    "gc.collect()\n",
    "\n",
    "# 2. Load Models\n",
    "base_model, mesh, model_config = get_gemma_model(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"))\n",
    "lora_model = get_lora_model(base_model, mesh=mesh)\n",
    "\n",
    "# 3. Setup Tokenizer\n",
    "tokenizer = tokenizer_lib.Tokenizer(\n",
    "    tokenizer_path=os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
    ")\n",
    "\n",
    "# 4. Baseline Evaluation (Same prompts as post-training for comparison)\n",
    "print(\"Running Baseline Evaluation...\")\n",
    "EVAL_PROMPTS = [\n",
    "    # Creative writing\n",
    "    \"Write a short story about a robot learning to paint.\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    # Creative ideation\n",
    "    \"Propose three innovative uses for AI in education.\",\n",
    "    # Summarization\n",
    "    \"Summarize the key benefits and risks of renewable energy in 3 paragraphs.\",\n",
    "    # Math (verifiable)\n",
    "    \"Solve step-by-step: If 2x + 5 = 15, what is x?\",\n",
    "    # Coding (verifiable)\n",
    "    \"Write a Python function to check if a string is a palindrome.\",\n",
    "    # Basic science\n",
    "    \"Explain why the sky is blue to a 5-year-old.\",\n",
    "    \"Explain the process of photosynthesis step by step.\",\n",
    "    # Ethics/Reasoning\n",
    "    \"What are the ethical implications of AI in healthcare?\",\n",
    "    \"Should AI systems have rights? Argue both sides.\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    baseline_sampler = sampler_lib.Sampler(\n",
    "        transformer=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        cache_config=sampler_lib.CacheConfig(\n",
    "            cache_size=MAX_SEQ_LEN + 512,\n",
    "            num_layers=model_config.num_layers,\n",
    "            num_kv_heads=model_config.num_kv_heads,\n",
    "            head_dim=model_config.head_dim,\n",
    "        ),\n",
    "    )\n",
    "    formatted = [TEMPLATE.format(question=p) for p in EVAL_PROMPTS]\n",
    "    baseline_out = baseline_sampler(\n",
    "        input_strings=formatted,\n",
    "        max_generation_steps=EVAL_MAX_TOKENS,\n",
    "        temperature=INFERENCE_TEMPERATURE,\n",
    "        top_k=INFERENCE_TOP_K,\n",
    "        top_p=INFERENCE_TOP_P,\n",
    "        echo=False\n",
    "    )\n",
    "    print(\"--- Baseline Outputs (Before Training) ---\")\n",
    "    baseline_results = []\n",
    "    for p, o in zip(EVAL_PROMPTS, baseline_out.text):\n",
    "        print(f\"Q: {p}\")\n",
    "        print(f\"A: {o}\")  # Full output\n",
    "        has_reasoning = bool(re.search(r\"<reasoning>.*?</reasoning>\", o, re.DOTALL))\n",
    "        has_answer = bool(re.search(r\"<answer>.*?</answer>\", o, re.DOTALL))\n",
    "        baseline_results.append({\"prompt\": p, \"output\": o, \"has_reasoning\": has_reasoning, \"has_answer\": has_answer})\n",
    "        print(\"-\"*40)\n",
    "except Exception as e:\n",
    "    print(f\"Baseline eval skipped: {e}\")\n",
    "    baseline_results = []\n",
    "print(\"Baseline Done.\")\n",
    "\n",
    "# 5. SFT Training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting SFT Training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Optimizer - Uses LEARNING_RATE from constants for HP tuning\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    decay_steps=SFT_STEPS,\n",
    "    end_value=LEARNING_RATE / 20  # End at 5% of peak\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(learning_rate=schedule, weight_decay=0.01)\n",
    ")\n",
    "\n",
    "# Checkpointing\n",
    "# Using Orbax options via TrainingConfig\n",
    "checkpoint_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=500, max_to_keep=2\n",
    ")\n",
    "\n",
    "# Data Iterator\n",
    "from tunix.sft import utils as sft_utils\n",
    "\n",
    "def create_data_iterator(dataset, batch_size, tokenizer):\n",
    "    '''Create batches with tokenization and masking'''\n",
    "    indices = np.random.permutation(len(dataset))\n",
    "    \n",
    "    # Infinite iterator matching steps\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            if len(batch_indices) < batch_size:\n",
    "                continue # Skip incomplete batches\n",
    "                \n",
    "            texts = [dataset[int(idx)]['text'] for idx in batch_indices]\n",
    "            \n",
    "            # Tokenize\n",
    "            # Tunix tokenizer returns list of ids\n",
    "            batch_input_tokens = []\n",
    "            batch_input_mask = []\n",
    "            \n",
    "            for text in texts:\n",
    "                # Use Tunix Tokenizer.tokenize which handles BOS/EOS\n",
    "                # tokenize returns np.array, convert to list for padding\n",
    "                tokens = tokenizer.tokenize(text, add_eos=True).tolist()\n",
    "                \n",
    "                # Truncate / Pad\n",
    "                if len(tokens) > MAX_SEQ_LEN:\n",
    "                    tokens = tokens[:MAX_SEQ_LEN]\n",
    "                    mask = [True] * MAX_SEQ_LEN\n",
    "                else:\n",
    "                    pad_len = MAX_SEQ_LEN - len(tokens)\n",
    "                    mask = [True] * len(tokens) + [False] * pad_len\n",
    "                    # Use pad_id if available, else 0\n",
    "                    pad_id = getattr(tokenizer, 'pad_id', lambda: 0)()\n",
    "                    tokens = tokens + [pad_id] * pad_len # 0 is usually pad, verify if needed\n",
    "                \n",
    "                batch_input_tokens.append(tokens)\n",
    "                batch_input_mask.append(mask)\n",
    "            \n",
    "            # Convert to JAX arrays\n",
    "            input_tokens = jnp.array(batch_input_tokens, dtype=jnp.int32)\n",
    "            input_mask = jnp.array(batch_input_mask, dtype=jnp.bool_)\n",
    "            \n",
    "            # Create PEFT required inputs\n",
    "            positions = sft_utils.build_positions_from_mask(input_mask)\n",
    "            attention_mask = sft_utils.make_causal_attn_mask(input_mask)\n",
    "            \n",
    "            yield {\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"input_mask\": input_mask,\n",
    "                \"positions\": positions,\n",
    "                \"attention_mask\": attention_mask\n",
    "            }\n",
    "\n",
    "# Training Configuration with WandB Metrics Backend\n",
    "from tunix.sft import metrics_logger as sft_metrics_logger\n",
    "\n",
    "metrics_logging_options = sft_metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/kaggle/working/logs\",\n",
    "    backend_factories=[WandbBackend] if WANDB_ENABLED else []\n",
    ")\n",
    "\n",
    "training_config = peft_trainer.TrainingConfig(\n",
    "    max_steps=SFT_STEPS,\n",
    "    checkpoint_root_directory=SFT_OUTPUT_DIR,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    checkpointing_options=checkpoint_options,\n",
    "    pbar_description=\"SFT Training\",\n",
    "    metrics_prefix=\"sft\",\n",
    "    metrics_logging_options=metrics_logging_options,\n",
    "    eval_every_n_steps=10000, # Disable freq eval for speed or set high\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "# Note: we pass the optimizer, model, and config.\n",
    "# Metrics logger defaults are fine.\n",
    "trainer = peft_trainer.PeftTrainer(\n",
    "    model=lora_model,\n",
    "    optimizer=optimizer,\n",
    "    training_config=training_config\n",
    ")\n",
    "\n",
    "# Create Iterator\n",
    "train_iter = create_data_iterator(sft_dataset, TRAIN_BATCH_SIZE, tokenizer)\n",
    "\n",
    "print(f\"Starting Training for {SFT_STEPS} steps...\")\n",
    "with mesh:\n",
    "    trainer.train(train_ds=train_iter, skip_jit=False)\n",
    "\n",
    "print(\"SFT Training Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save Final Model ---\n",
    "FINAL_SAVE_DIR = \"/kaggle/working/final_sft_model\"\n",
    "os.makedirs(FINAL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save the trained LoRA model checkpoint\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "checkpointer.save(os.path.join(FINAL_SAVE_DIR, \"checkpoint\"), nnx.state(lora_model, nnx.LoRAParam))\n",
    "checkpointer.wait_until_finished()\n",
    "\n",
    "print(f\"‚úÖ Model saved to '{FINAL_SAVE_DIR}/'\")\n",
    "print(\"To submit for Unrestricted Mode:\")\n",
    "print(\"   1. Download the output folder after this notebook finishes.\")\n",
    "print(\"   2. Go to Kaggle -> Models -> New Model -> Upload the checkpoint files.\")\n",
    "print(\"   3. Set the Model ID below to match your upload.\")\n",
    "\n",
    "# Your Kaggle Model ID for Unrestricted Mode:\n",
    "unrestricted_kaggle_model = \"yuyamukai/tunix-gemma2-sft\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Visual Sanity Check & Validation ---\n",
    "print(\"Running Post-Training Evaluation...\")\n",
    "\n",
    "try:\n",
    "    inference_sampler = sampler_lib.Sampler(\n",
    "        transformer=lora_model,\n",
    "        tokenizer=tokenizer,\n",
    "        cache_config=sampler_lib.CacheConfig(\n",
    "            cache_size=MAX_SEQ_LEN + 512,\n",
    "            num_layers=model_config.num_layers,\n",
    "            num_kv_heads=model_config.num_kv_heads,\n",
    "            head_dim=model_config.head_dim,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    test_prompts = [\n",
    "        # Creative writing\n",
    "        \"Write a short story about a robot learning to paint.\",\n",
    "        \"Write a haiku about artificial intelligence.\",\n",
    "        # Creative ideation\n",
    "        \"Propose three innovative uses for AI in education.\",\n",
    "        # Summarization\n",
    "        \"Summarize the key benefits and risks of renewable energy in 3 paragraphs.\",\n",
    "        # Math (verifiable)\n",
    "        \"Solve step-by-step: If 2x + 5 = 15, what is x?\",\n",
    "        # Coding (verifiable)\n",
    "        \"Write a Python function to check if a string is a palindrome.\",\n",
    "        # Basic science\n",
    "        \"Explain why the sky is blue to a 5-year-old.\",\n",
    "        \"Explain the process of photosynthesis step by step.\",\n",
    "        # Ethics/Reasoning\n",
    "        \"What are the ethical implications of AI in healthcare?\",\n",
    "        \"Should AI systems have rights? Argue both sides.\",\n",
    "    ]\n",
    "    \n",
    "    # Use same prompts as baseline for fair comparison\n",
    "    test_prompts = EVAL_PROMPTS\n",
    "    formatted_prompts = [TEMPLATE.format(question=p) for p in test_prompts]\n",
    "    \n",
    "    out_data = inference_sampler(\n",
    "        input_strings=formatted_prompts,\n",
    "        max_generation_steps=EVAL_MAX_TOKENS,\n",
    "        temperature=INFERENCE_TEMPERATURE,\n",
    "        top_k=INFERENCE_TOP_K,\n",
    "        top_p=INFERENCE_TOP_P,\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Validation Logic\n",
    "    print(\"--- Post-Training Outputs ---\")\n",
    "    valid_format_count = 0\n",
    "    results_for_wandb = []\n",
    "    \n",
    "    for p, o in zip(test_prompts, out_data.text):\n",
    "        print(f\"Prompt: {p}\")\n",
    "        print(f\"Output: {o}\")  # Full output, no truncation\n",
    "        \n",
    "        # Robust Regex Check\n",
    "        has_reasoning = bool(re.search(r\"<reasoning>.*?</reasoning>\", o, re.DOTALL))\n",
    "        has_answer = bool(re.search(r\"<answer>.*?</answer>\", o, re.DOTALL))\n",
    "        \n",
    "        is_valid = has_reasoning and has_answer\n",
    "        if is_valid:\n",
    "            valid_format_count += 1\n",
    "            print(\"‚úÖ Format Check: Passed\")\n",
    "        else:\n",
    "            print(f\"‚ùå Format Check: Failed (Reasoning: {has_reasoning}, Answer: {has_answer})\")\n",
    "            \n",
    "        results_for_wandb.append([p, o, is_valid])\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"Format Validation: {valid_format_count}/{len(test_prompts)} passed.\")\n",
    "    \n",
    "    # Extended WandB Evaluation (25 prompts for statistical confidence)\n",
    "    WANDB_EVAL_PROMPTS = [\n",
    "        # Original 10 prompts\n",
    "        *test_prompts,\n",
    "        # Additional 15 prompts for diversity\n",
    "        \"Explain quantum entanglement to a high school student.\",\n",
    "        \"Write a poem about the passage of time.\",\n",
    "        \"What are the pros and cons of remote work?\",\n",
    "        \"Describe how a compiler works step by step.\",\n",
    "        \"Compare democracy and authoritarianism objectively.\",\n",
    "        \"Write a short dialogue between a human and an AI about consciousness.\",\n",
    "        \"Explain the greenhouse effect and its consequences.\",\n",
    "        \"How would you teach a child about money management?\",\n",
    "        \"What lessons can we learn from the fall of ancient Rome?\",\n",
    "        \"Design a simple mobile app for tracking habits.\",\n",
    "        \"Explain the difference between correlation and causation.\",\n",
    "        \"Write a persuasive argument for learning a second language.\",\n",
    "        \"How do vaccines work to protect against diseases?\",\n",
    "        \"What ethical considerations arise with genetic engineering?\",\n",
    "        \"Explain the concept of supply and demand with examples.\",\n",
    "    ]\n",
    "    \n",
    "    # Run extended evaluation for WandB\n",
    "    try:\n",
    "        if wandb.run is not None and WANDB_ENABLED:\n",
    "            print(\"\\nRunning Extended WandB Evaluation (25 prompts in batches)...\")\n",
    "            extended_results = []\n",
    "            extended_valid = 0\n",
    "            BATCH_SIZE = 5  # Process in smaller batches to avoid OOM\n",
    "            \n",
    "            for batch_start in range(0, len(WANDB_EVAL_PROMPTS), BATCH_SIZE):\n",
    "                batch_prompts = WANDB_EVAL_PROMPTS[batch_start:batch_start + BATCH_SIZE]\n",
    "                batch_formatted = [TEMPLATE.format(question=p) for p in batch_prompts]\n",
    "                batch_out = inference_sampler(\n",
    "                    input_strings=batch_formatted,\n",
    "                    max_generation_steps=EVAL_MAX_TOKENS,\n",
    "                    temperature=INFERENCE_TEMPERATURE,\n",
    "                    top_k=INFERENCE_TOP_K,\n",
    "                    top_p=INFERENCE_TOP_P,\n",
    "                    echo=False\n",
    "                )\n",
    "                \n",
    "                for p, o in zip(batch_prompts, batch_out.text):\n",
    "                    has_r = bool(re.search(r\"<reasoning>.*?</reasoning>\", o, re.DOTALL))\n",
    "                    has_a = bool(re.search(r\"<answer>.*?</answer>\", o, re.DOTALL))\n",
    "                    is_valid = has_r and has_a\n",
    "                    if is_valid:\n",
    "                        extended_valid += 1\n",
    "                    extended_results.append([p, o[:1000], is_valid])  # Truncate for table\n",
    "                print(f\"  Batch {batch_start//BATCH_SIZE + 1}/{(len(WANDB_EVAL_PROMPTS) + BATCH_SIZE - 1)//BATCH_SIZE} complete.\")\n",
    "            \n",
    "            # Log table\n",
    "            tbl = wandb.Table(columns=[\"Prompt\", \"Output\", \"IsValid\"], data=extended_results)\n",
    "            wandb.log({\"eval_results\": tbl})\n",
    "            \n",
    "            # Log summary metrics\n",
    "            format_compliance = extended_valid / len(WANDB_EVAL_PROMPTS) * 100\n",
    "            wandb.log({\n",
    "                \"eval/format_compliance_pct\": format_compliance,\n",
    "                \"eval/total_prompts\": len(WANDB_EVAL_PROMPTS),\n",
    "                \"eval/valid_count\": extended_valid,\n",
    "            })\n",
    "            print(f\"Extended Evaluation: {extended_valid}/{len(WANDB_EVAL_PROMPTS)} ({format_compliance:.1f}%) passed.\")\n",
    "            print(\"Logged to WandB: eval_results table + summary metrics.\")\n",
    "            \n",
    "            # Force sync before exit/crash\n",
    "            wandb.finish()\n",
    "            import time\n",
    "            time.sleep(5)  # Give the background process a moment to finalize\n",
    "            \n",
    "    except Exception as w_err:\n",
    "        print(f\"Extended WandB eval skipped: {w_err}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb9ca62",
   "metadata": {},
   "source": [
    "## [Optional 15pts] unrestricted mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For Unrestricted Mode, upload the saved checkpoint as a Kaggle Model.\n",
    "# Then update this variable with your Model ID:\n",
    "unrestricted_kaggle_model = \"yuyamukai/tunix-gemma2-sft\"\n",
    "\n",
    "print(f\"Unrestricted Mode Model ID: {unrestricted_kaggle_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f33df",
   "metadata": {},
   "source": [
    "\n",
    "## Other things I want the judges to know\n",
    "\n",
    "### 1. Our Approach\n",
    "Instead of reinforcement learning (GRPO) on verifiable tasks, we teach reasoning through **demonstration**. SFT on high-quality reasoning traces provides:\n",
    "- **Dense supervision**: Every token gets feedback, not just final answers\n",
    "- **Diverse domains**: Creative, analytical, philosophical reasoning\n",
    "- **Competition-aligned**: Focuses on what judges actually evaluate\n",
    "\n",
    "### 2. Why This Works\n",
    "The FAQ states \"verifiable tasks (math/coding) will have **much lower weights**.\" By training on 180K reasoning traces from domains the competition values, we maximize evaluation performance.\n",
    "\n",
    "### 3. Technical Details\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Dataset | GlaiveAI (DeepSeek-R1-Distill-70B traces) |\n",
    "| Samples | 180K √ó 4 epochs |\n",
    "| Runtime | ~7 hours |\n",
    "| Method | LoRA (rank=64) |\n",
    "\n",
    "### 4. Learnings\n",
    "- **Quality > Quantity**: One curated 2025 dataset outperforms multiple older datasets\n",
    "- **Alignment Matters**: Train on what the competition measures\n",
    "- **SFT Scales**: 180K samples in 7 hours vs ~1,500 GRPO steps\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
