{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1fafa7",
   "metadata": {},
   "source": [
    "# Tunix SFT: Teaching Reasoning Through Demonstration\n",
    "\n",
    "**Strategy**: Supervised Fine-Tuning on high-quality reasoning traces across diverse domains.\n",
    "\n",
    "**Key Insight**: For 2B parameter models, learning from demonstrations is more effective than reinforcement learning. SFT provides dense supervision at every token, while RL provides sparse rewards only at sequence end.\n",
    "\n",
    "**Datasets**: \n",
    "- Raiden-DeepSeek-R1 (Creative/Analytical)\n",
    "- OpenO1-SFT (General Reasoning)\n",
    "- CoT-Collection (Commonsense/Ethics)\n",
    "- GlaiveAI-Reasoning (Math/Code/General)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798bca2",
   "metadata": {},
   "source": [
    "\n",
    "## Overall training and evaluation strategy\n",
    "\n",
    "**Strategy: SFT on Diverse Domain Reasoning Traces**\n",
    "\n",
    "Competition FAQ explicitly states that verifiable tasks (math/coding) have \"much lower weights\". Our strategy prioritizes non-verifiable domains:\n",
    "\n",
    "1.  **Base Model**: We start with `Gemma-2-2b-it` for its instruction-following foundation.\n",
    "2.  **SFT Training**: We fine-tune on ~100K reasoning traces from diverse domains (creative, analytical, philosophical, commonsense).\n",
    "3.  **Format**: All data uses explicit `<reasoning>` and `<answer>` tags for structured outputs.\n",
    "\n",
    "## üó∫Ô∏è Workflow Diagram\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Gemma-2B-IT] --> B{SFT Training}\n",
    "    B -->|Creative| C[Raiden-DeepSeek-R1]\n",
    "    B -->|Reasoning| D[OpenO1-SFT]\n",
    "    B -->|Ethics| E[CoT-Collection]\n",
    "    B -->|General| F[GlaiveAI]\n",
    "    C & D & E & F --> G[Trained Model]\n",
    "    G --> H[Submission]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a6a93",
   "metadata": {},
   "source": [
    "\n",
    "## How your finetuning dataset is created\n",
    "\n",
    "We employ a **Diverse Domain Strategy** using publicly available datasets with reasoning traces:\n",
    "\n",
    "| Dataset | Source | Samples | Domain | License |\n",
    "|:---|:---|:---:|:---|:---|\n",
    "| Raiden-DeepSeek-R1 | HuggingFace | 62.9K | Creative/Analytical | Apache 2.0 |\n",
    "| OpenO1-SFT | HuggingFace | 20K | General Reasoning | Apache 2.0 |\n",
    "| CoT-Collection | HuggingFace | 10K | Commonsense/Ethics | CC-BY-4.0 |\n",
    "| GlaiveAI-Reasoning | HuggingFace | 30K | Math/Code/General | Apache 2.0 |\n",
    "\n",
    "All datasets are downloaded and processed in-notebook to demonstrate public data usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b2af8",
   "metadata": {},
   "source": [
    "## Tunix finetuning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training parameters\n",
    "TEMPERATURE=0.7\n",
    "TOP_K=50\n",
    "TOP_P=0.9\n",
    "MAX_GENERATION_STEPS=768\n",
    "\n",
    "# Output Tags\n",
    "REASONING_START = \"<reasoning>\"\n",
    "REASONING_END = \"</reasoning>\"\n",
    "SOLUTION_START = \"<answer>\"\n",
    "SOLUTION_END = \"</answer>\"\n",
    "\n",
    "# Inference Params\n",
    "INF_TEMPERATURE=0\n",
    "INF_TOP_K=1\n",
    "INF_TOP_P=None\n",
    "SEED=42\n",
    "\n",
    "# System prompt and template\n",
    "SYSTEM_PROMPT = \"You are a deep thinking AI. Think step by step about the problem and provide your reasoning between <reasoning> and </reasoning> tags. Then, provide the final answer between <answer> and </answer> tags.\"\n",
    "TEMPLATE = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{{question}}<end_of_turn>\\n<start_of_turn>model\"\n",
    "\n",
    "print(\"Template variables defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup & Install ---\n",
    "!pip install -q wandb==0.22.0\n",
    "!pip install -q kagglehub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q transformers\n",
    "!pip install -q grain\n",
    "\n",
    "# Tunix/Qwix Installation\n",
    "# Check if we are offline (no internet), if so, assume wheels are attached\n",
    "import socket\n",
    "import os\n",
    "\n",
    "def is_connected():\n",
    "    try:\n",
    "        # Check simple connectivity\n",
    "        socket.create_connection((\"1.1.1.1\", 53))\n",
    "        return True\n",
    "    except OSError:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "if is_connected():\n",
    "    !pip install \"google-tunix[prod]==0.1.5\"\n",
    "    !pip install git+https://github.com/google/qwix\n",
    "else:\n",
    "    print(\"Offline mode detected. Assuming dependencies are installed or wheels provided.\")\n",
    "    # Fallback: Try installing from local wheels if available\n",
    "    if os.path.exists(\"/kaggle/input/tunix-wheels\"):\n",
    "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels google-tunix\n",
    "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels qwix\n",
    "\n",
    "\n",
    "# Fix Flax Version to 0.12.0 as required\n",
    "!pip uninstall -q -y flax\n",
    "!pip install flax==0.12.0\n",
    "\n",
    "!pip install -q datasets==3.2.0 optax==0.2.4 chex==0.1.88\n",
    "\n",
    "# --- Imports ---\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "from pprint import pprint\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "import tensorflow_datasets as tfds\n",
    "import datasets\n",
    "import tensorflow_datasets as tfds\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Tunix Imports\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma import model as gemma_lib\n",
    "from tunix.models.gemma import params as params_lib\n",
    "from tunix.sft import metrics_logger\n",
    "from tunix.sft import peft_trainer\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Stability Configs ---\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.95'\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "\n",
    "print(f\"JAX Devices: {jax.devices()}\")\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "DATASET_PATH = \"/kaggle/input/tunix-sft-data\"\n",
    "SFT_OUTPUT_DIR = \"/kaggle/working/sft_checkpoint\"\n",
    "\n",
    "# Tuning Hyperparams\n",
    "SFT_STEPS = 3000  # ~100K samples with batch_size ~32/epoch\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION = 16\n",
    "EFFECTIVE_BATCH = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION  # 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Utilities ---\n",
    "MESH = [(8, 1), (\"fsdp\", \"tp\")]\n",
    "\n",
    "def get_gemma_model(ckpt_path):\n",
    "    mesh = jax.make_mesh(*MESH)\n",
    "    model_config = gemma_lib.ModelConfig.gemma2_2b()\n",
    "    abs_gemma: nnx.Module = nnx.eval_shape(\n",
    "        lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
    "    )\n",
    "    abs_state = nnx.state(abs_gemma)\n",
    "    abs_state = jax.tree.map(\n",
    "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "        abs_state,\n",
    "        nnx.get_named_sharding(abs_state, mesh),\n",
    "    )\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "\n",
    "    graph_def, _ = nnx.split(abs_gemma)\n",
    "    gemma = nnx.merge(graph_def, restored_params)\n",
    "    return gemma, mesh, model_config\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    # Tunix LoRA Config\n",
    "    RANK = 64\n",
    "    ALPHA = 64.0\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "            \".*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "\n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, rngs=nnx.Rngs(params=0), **model_input\n",
    "    )\n",
    "\n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "\n",
    "    return lora_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34454a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Optional: WandB Logging ---\n",
    "try:\n",
    "    import wandb\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "    if secret_value:\n",
    "        wandb.login(key=secret_value)\n",
    "        wandb.init(project=\"tunix-sft-diverse\", name=\"sft-run-v1\", anonymous=\"allow\")\n",
    "        print(\"WandB Logging Enabled.\")\n",
    "    else:\n",
    "        raise ValueError(\"Empty WANDB_API_KEY\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"WandB not enabled: {e}\")\n",
    "    os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "    if 'wandb' in locals():\n",
    "        wandb.init = lambda *args, **kwargs: None\n",
    "    print(\"Proceeding without cloud logging (WANDB_MODE='disabled').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Data Preprocessing ---\n",
    "# Download and process diverse domain datasets\n",
    "\n",
    "print(\"Loading datasets from Kaggle/HuggingFace...\")\n",
    "\n",
    "def standardize_to_gemma_format(text, question=None):\n",
    "    '''Convert various formats to Gemma chat template with <reasoning>/<answer> tags'''\n",
    "    \n",
    "    # Handle already formatted text\n",
    "    if \"<start_of_turn>\" in text:\n",
    "        # Just ensure we have our tags (case insensitive replacement)\n",
    "        text = re.sub(r\"<think>\", \"<reasoning>\", text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r\"</think>\", \"</reasoning>\", text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r\"<thought>\", \"<reasoning>\", text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r\"</thought>\", \"</reasoning>\", text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Enforce <answer> tags if missing (sometimes models output just the answer after start_of_turn)\n",
    "        if \"<answer>\" not in text and \"<start_of_turn>model\" in text:\n",
    "            # Heuristic: Wrap the last part of the model turn in answer tags if not present\n",
    "            match = re.search(r\"<start_of_turn>model\\n(.*)$\", text, re.DOTALL)\n",
    "            if match:\n",
    "                 content = match.group(1).strip()\n",
    "                 # If no reasoning tag either, wrap whole thing\n",
    "                 if \"<reasoning>\" not in content:\n",
    "                     text = text.replace(content, f\"<answer>{content}</answer>\")\n",
    "                 else:\n",
    "                     # Reasoning exists, so wrap whatever is after reasoning? \n",
    "                     # Risk of breaking format. Best safe fallback is just appending expected structure if completely malformed\n",
    "                     pass \n",
    "        return text\n",
    "    \n",
    "    # For raw question/response pairs\n",
    "    if question:\n",
    "        # Extract reasoning and answer from response\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "        \n",
    "        # Try to extract think/reasoning\n",
    "        think_match = re.search(r\"<think>(.*?)</think>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        thought_match = re.search(r\"<Thought>(.*?)</Thought>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        reasoning_tag_match = re.search(r\"<reasoning>(.*?)</reasoning>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if think_match:\n",
    "            reasoning = think_match.group(1).strip()\n",
    "        elif thought_match:\n",
    "            reasoning = thought_match.group(1).strip()\n",
    "        elif reasoning_tag_match:\n",
    "            reasoning = reasoning_tag_match.group(1).strip()\n",
    "        else:\n",
    "            # Use the whole text as reasoning if no specific tags found\n",
    "            reasoning = text.strip()\n",
    "        \n",
    "        # Try to extract answer\n",
    "        ans_match = re.search(r\"<Output>(.*?)</Output>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        if ans_match:\n",
    "            answer = ans_match.group(1).strip()\n",
    "        else:\n",
    "            answer_match = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL | re.IGNORECASE)\n",
    "            if answer_match:\n",
    "                answer = answer_match.group(1).strip()\n",
    "            else:\n",
    "                # If no explicit answer tag, assume the last paragraph is the answer\n",
    "                # or the whole text if reasoning was extracted from specific tags\n",
    "                if reasoning_tag_match or think_match or thought_match:\n",
    "                    # If reasoning was explicitly tagged, the rest is likely the answer\n",
    "                    # This is a heuristic and might need refinement for specific datasets\n",
    "                    remaining_text = re.sub(r\"<reasoning>.*?</reasoning>\", \"\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "                    remaining_text = re.sub(r\"<think>.*?</think>\", \"\", remaining_text, flags=re.DOTALL | re.IGNORECASE)\n",
    "                    remaining_text = re.sub(r\"<Thought>.*?</Thought>\", \"\", remaining_text, flags=re.DOTALL | re.IGNORECASE)\n",
    "                    answer = remaining_text.strip()\n",
    "                    if not answer and reasoning: # If no answer found, and reasoning was found, use reasoning as answer\n",
    "                        answer = reasoning\n",
    "                else:\n",
    "                    # If no specific tags for reasoning, and no answer tag,\n",
    "                    # try to split by paragraphs and take the last one as answer\n",
    "                    paragraphs = text.strip().split(\"\\n\\n\")\n",
    "                    answer = paragraphs[-1] if paragraphs else text[:200] # Fallback to first 200 chars\n",
    "        \n",
    "        # Ensure reasoning and answer are not empty\n",
    "        if not reasoning and answer:\n",
    "            reasoning = answer # If only answer, use it as reasoning\n",
    "        elif not answer and reasoning:\n",
    "            answer = reasoning # If only reasoning, use it as answer\n",
    "        elif not reasoning and not answer:\n",
    "            reasoning = text.strip()\n",
    "            answer = text.strip()\n",
    "\n",
    "        formatted = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n<start_of_turn>model\\n<reasoning>{reasoning}</reasoning>\\n<answer>{answer}</answer>\"\n",
    "        return formatted\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Load from Kaggle Dataset (pre-downloaded for efficiency)\n",
    "try:\n",
    "    # Primary: Load pre-processed data from Kaggle Dataset\n",
    "    all_texts = []\n",
    "    \n",
    "    # Try loading from attached dataset\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        import glob\n",
    "        # Load Parquet files (Preferred)\n",
    "        for parquet_file in glob.glob(f\"{DATASET_PATH}/*.parquet\"):\n",
    "            ds = datasets.load_dataset(\"parquet\", data_files=parquet_file, split=\"train\")\n",
    "            print(f\"Loaded {len(ds)} samples from {parquet_file}\")\n",
    "            \n",
    "            # Identify dataset type based on filename\n",
    "            fname = os.path.basename(parquet_file).lower()\n",
    "            \n",
    "            # 1. CoT-Collection\n",
    "            if \"cot_collection\" in fname:\n",
    "                # CoT Collection: source (q), rationale (r), target (a)\n",
    "                # Strategy: Probabilistic sampling to get uniform distribution from ~1.8M samples\n",
    "                # Target: 10,000 samples. Total ~1.84M. Rate ~ 0.54%.\n",
    "                # We use 0.6% to be safe, then truncate.\n",
    "                import random \n",
    "                cot_current_count = 0\n",
    "                cot_target = 10000\n",
    "                sampling_rate = 0.006 \n",
    "                \n",
    "                for sample in ds:\n",
    "                    if cot_current_count >= cot_target:\n",
    "                        break\n",
    "                    \n",
    "                    # Random selection\n",
    "                    if random.random() < sampling_rate:\n",
    "                        q = sample.get(\"source\", \"\")\n",
    "                        r = sample.get(\"rationale\", \"\")\n",
    "                        a = sample.get(\"target\", \"\")\n",
    "                        formatted = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{q}<end_of_turn>\\n<start_of_turn>model\\n<reasoning>{r}</reasoning>\\n<answer>{a}</answer>\"\n",
    "                        all_texts.append({\"text\": formatted})\n",
    "                        cot_current_count += 1\n",
    "\n",
    "            # 2. GlaiveAI-Reasoning\n",
    "            elif \"glaive\" in fname:\n",
    "                # Glaive: prompt, response (contains <think>...</think> then answer)\n",
    "                for sample in ds:\n",
    "                    q = sample.get(\"prompt\", sample.get(\"question\", sample.get(\"instruction\", \"\")))\n",
    "                    a = sample.get(\"response\", sample.get(\"answer\", sample.get(\"output\", \"\")))\n",
    "                    \n",
    "                    # Special handling for Glaive's <think> only format\n",
    "                    # If we just force standardized format from raw text, it might miss the answer part if it expects explicit tags\n",
    "                    # But standardize_to_gemma_format logic for raw text is:\n",
    "                    # extract reasoning from tags (think/Thought/reasoning)\n",
    "                    # output rest as answer\n",
    "                    # Let's verify standardizer handles this.\n",
    "                    # It does: `remaining_text` logic handles extraction.\n",
    "                    \n",
    "                    formatted = standardize_to_gemma_format(a, question=q)\n",
    "                    all_texts.append({\"text\": formatted})\n",
    "\n",
    "            # 3. Raiden / OpenO1 (Standard formatted or text column)\n",
    "            else: \n",
    "                for sample in ds:\n",
    "                     # Check if pre-formatted 'text' field exists\n",
    "                     if \"text\" in sample:\n",
    "                         formatted = standardize_to_gemma_format(sample[\"text\"])\n",
    "                         all_texts.append({\"text\": formatted})\n",
    "                     # Fallback to instruction/response pair\n",
    "                     elif (\"prompt\" in sample and \"response\" in sample):\n",
    "                         formatted = standardize_to_gemma_format(sample[\"response\"], question=sample[\"prompt\"])\n",
    "                         all_texts.append({\"text\": formatted})\n",
    "                     elif (\"instruction\" in sample and \"output\" in sample):\n",
    "                         formatted = standardize_to_gemma_format(sample[\"output\"], question=sample[\"instruction\"])\n",
    "                         all_texts.append({\"text\": formatted})\n",
    "\n",
    "    else:\n",
    "        print(f\"Dataset path {DATASET_PATH} not found. Downloading from HuggingFace...\")\n",
    "        \n",
    "        # Fallback: Download from HuggingFace\n",
    "        # 1. Raiden-DeepSeek-R1 (main dataset)\n",
    "        raiden = datasets.load_dataset(\"sequelbox/Raiden-DeepSeek-R1\", split=\"train[:20000]\")\n",
    "        print(f\"Downloaded Raiden: {len(raiden)} samples\")\n",
    "        for sample in raiden:\n",
    "            prompt = sample.get(\"prompt\", \"\")\n",
    "            response = sample.get(\"response\", sample.get(\"completion\", \"\"))\n",
    "            if prompt and response:\n",
    "                formatted = standardize_to_gemma_format(response, question=prompt)\n",
    "                all_texts.append({\"text\": formatted})\n",
    "        \n",
    "            # 2. OpenO1-SFT\n",
    "        try:\n",
    "            openo1 = datasets.load_dataset(\"O1-OPEN/OpenO1-SFT\", split=\"train[:10000]\")\n",
    "            print(f\"Downloaded OpenO1: {len(openo1)} samples\")\n",
    "            for sample in openo1:\n",
    "                instruction = sample.get(\"instruction\", \"\")\n",
    "                output = sample.get(\"output\", \"\")\n",
    "                \n",
    "                # Filter Chinese characters to prevent language leakage\n",
    "                if any(u'‰∏Ä' <= c <= u'Èøø' for c in instruction + output):\n",
    "                    continue\n",
    "                    \n",
    "                if instruction and output:\n",
    "                    formatted = standardize_to_gemma_format(output, question=instruction)\n",
    "                    all_texts.append({\"text\": formatted})\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping OpenO1: {e}\")\n",
    "\n",
    "        # 3. GlaiveAI-Reasoning\n",
    "        try:\n",
    "            glaive = datasets.load_dataset(\"glaiveai/reasoning-v1-20m\", split=\"train[:10000]\")\n",
    "            print(f\"Downloaded GlaiveAI: {len(glaive)} samples\")\n",
    "            for sample in glaive:\n",
    "                instruction = sample.get(\"instruction\", \"\")\n",
    "                output = sample.get(\"output\", \"\")\n",
    "                if instruction and output:\n",
    "                    formatted = standardize_to_gemma_format(output, question=instruction)\n",
    "                    all_texts.append({\"text\": formatted})\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping GlaiveAI: {e}\")\n",
    "\n",
    "    print(f\"Total samples after preprocessing: {len(all_texts)}\")\n",
    "    \n",
    "    # Create HuggingFace dataset\n",
    "    sft_dataset = datasets.Dataset.from_list(all_texts)\n",
    "    sft_dataset = sft_dataset.shuffle(seed=42)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Failed to load datasets: {e}\")\n",
    "    raise RuntimeError(f\"Dataset loading failed: {e}\")\n",
    "\n",
    "print(f\"Final SFT dataset: {len(sft_dataset)} samples\")\n",
    "print(f\"Sample: {sft_dataset[0]['text'][:500]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Main Training Logic ---\n",
    "\n",
    "# 1. Download/setup Base Model\n",
    "if \"KAGGLE_USERNAME\" not in os.environ:\n",
    "    kagglehub.login()\n",
    "\n",
    "# Download Gemma 2 (Flax)\n",
    "model_path = { \"gemma2\": \"google/gemma-2/flax/\" }\n",
    "model_family = \"gemma2\"\n",
    "model_version = \"gemma2-2b-it\" \n",
    "kaggle_ckpt_path = kagglehub.model_download(f\"{model_path[model_family]}{model_version}\")\n",
    "\n",
    "# Convert checkpoint format for Tunix/NNX\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
    "!rm -rf {INTERMEDIATE_CKPT_DIR} {CKPT_DIR}\n",
    "\n",
    "params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\"))\n",
    "gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "_, state = nnx.split(gemma)\n",
    "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "checkpointer.wait_until_finished()\n",
    "del params, gemma, state\n",
    "gc.collect()\n",
    "\n",
    "# 2. Load Models\n",
    "base_model, mesh, model_config = get_gemma_model(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"))\n",
    "lora_model = get_lora_model(base_model, mesh=mesh)\n",
    "\n",
    "# 3. Setup Tokenizer\n",
    "tokenizer = tokenizer_lib.Tokenizer(\n",
    "    tokenizer_path=os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
    ")\n",
    "\n",
    "# 4. Baseline Evaluation\n",
    "print(\"Running Baseline Evaluation...\")\n",
    "try:\n",
    "    baseline_sampler = sampler_lib.Sampler(\n",
    "        transformer=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        cache_config=sampler_lib.CacheConfig(\n",
    "            cache_size=512,\n",
    "            num_layers=model_config.num_layers,\n",
    "            num_kv_heads=model_config.num_kv_heads,\n",
    "            head_dim=model_config.head_dim,\n",
    "        ),\n",
    "    )\n",
    "    test_prompts = [\n",
    "        \"What are the ethical implications of AI in healthcare?\",\n",
    "        \"Explain the concept of opportunity cost in simple terms.\"\n",
    "    ]\n",
    "    formatted = [TEMPLATE.format(question=p) for p in test_prompts]\n",
    "    baseline_out = baseline_sampler(\n",
    "        input_strings=formatted,\n",
    "        max_generation_steps=150,\n",
    "        temperature=0.7,\n",
    "        echo=False\n",
    "    )\n",
    "    print(\"--- Baseline Outputs (Before Training) ---\")\n",
    "    for p, o in zip(test_prompts, baseline_out.text):\n",
    "        print(f\"Q: {p}\\nA: {o[:300]}...\\n{'-'*40}\")\n",
    "except Exception as e:\n",
    "    print(f\"Baseline eval skipped: {e}\")\n",
    "print(\"Baseline Done.\")\n",
    "\n",
    "# 5. SFT Training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting SFT Training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Optimizer\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=2e-5,\n",
    "    warmup_steps=100,\n",
    "    decay_steps=SFT_STEPS,\n",
    "    end_value=1e-6\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(learning_rate=schedule, weight_decay=0.01)\n",
    ")\n",
    "\n",
    "# Checkpointing\n",
    "# Using Orbax options via TrainingConfig\n",
    "checkpoint_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=500, max_to_keep=2\n",
    ")\n",
    "\n",
    "# Data Iterator\n",
    "from tunix.sft import utils as sft_utils\n",
    "\n",
    "MAX_SEQ_LEN = 1024\n",
    "\n",
    "def create_data_iterator(dataset, batch_size, tokenizer):\n",
    "    '''Create batches with tokenization and masking'''\n",
    "    indices = np.random.permutation(len(dataset))\n",
    "    \n",
    "    # Infinite iterator matching steps\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            if len(batch_indices) < batch_size:\n",
    "                continue # Skip incomplete batches\n",
    "                \n",
    "            texts = [dataset[int(idx)]['text'] for idx in batch_indices]\n",
    "            \n",
    "            # Tokenize\n",
    "            # Tunix tokenizer returns list of ids\n",
    "            batch_input_tokens = []\n",
    "            batch_input_mask = []\n",
    "            \n",
    "            for text in texts:\n",
    "                # Use Tunix Tokenizer.tokenize which handles BOS/EOS\n",
    "                # tokenize returns np.array, convert to list for padding\n",
    "                tokens = tokenizer.tokenize(text, add_eos=True).tolist()\n",
    "                \n",
    "                # Truncate / Pad\n",
    "                if len(tokens) > MAX_SEQ_LEN:\n",
    "                    tokens = tokens[:MAX_SEQ_LEN]\n",
    "                    mask = [True] * MAX_SEQ_LEN\n",
    "                else:\n",
    "                    pad_len = MAX_SEQ_LEN - len(tokens)\n",
    "                    mask = [True] * len(tokens) + [False] * pad_len\n",
    "                    # Use pad_id if available, else 0\n",
    "                    pad_id = getattr(tokenizer, 'pad_id', lambda: 0)()\n",
    "                    tokens = tokens + [pad_id] * pad_len # 0 is usually pad, verify if needed\n",
    "                \n",
    "                batch_input_tokens.append(tokens)\n",
    "                batch_input_mask.append(mask)\n",
    "            \n",
    "            # Convert to JAX arrays\n",
    "            input_tokens = jnp.array(batch_input_tokens, dtype=jnp.int32)\n",
    "            input_mask = jnp.array(batch_input_mask, dtype=jnp.bool_)\n",
    "            \n",
    "            # Create PEFT required inputs\n",
    "            positions = sft_utils.build_positions_from_mask(input_mask)\n",
    "            attention_mask = sft_utils.make_causal_attn_mask(input_mask)\n",
    "            \n",
    "            yield {\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"input_mask\": input_mask,\n",
    "                \"positions\": positions,\n",
    "                \"attention_mask\": attention_mask\n",
    "            }\n",
    "\n",
    "# Training Configuration\n",
    "training_config = peft_trainer.TrainingConfig(\n",
    "    max_steps=SFT_STEPS,\n",
    "    checkpoint_root_directory=SFT_OUTPUT_DIR,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    checkpointing_options=checkpoint_options,\n",
    "    pbar_description=\"SFT Training\",\n",
    "    metrics_prefix=\"sft\",\n",
    "    eval_every_n_steps=10000, # Disable freq eval for speed or set high\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "# Note: we pass the optimizer, model, and config.\n",
    "# Metrics logger defaults are fine.\n",
    "trainer = peft_trainer.PeftTrainer(\n",
    "    model=lora_model,\n",
    "    optimizer=optimizer,\n",
    "    training_config=training_config\n",
    ")\n",
    "\n",
    "# Create Iterator\n",
    "train_iter = create_data_iterator(sft_dataset, TRAIN_BATCH_SIZE, tokenizer)\n",
    "\n",
    "print(f\"Starting Training for {SFT_STEPS} steps...\")\n",
    "with mesh:\n",
    "    trainer.train(train_ds=train_iter, skip_jit=False)\n",
    "\n",
    "print(\"SFT Training Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1393ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save Final Model ---\n",
    "FINAL_SAVE_DIR = \"/kaggle/working/final_sft_model\"\n",
    "os.makedirs(FINAL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save the trained LoRA model checkpoint\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "checkpointer.save(os.path.join(FINAL_SAVE_DIR, \"checkpoint\"), nnx.state(lora_model, nnx.LoRAParam))\n",
    "checkpointer.wait_until_finished()\n",
    "\n",
    "print(f\"‚úÖ Model saved to '{FINAL_SAVE_DIR}/'\")\n",
    "print(\"To submit for Unrestricted Mode:\")\n",
    "print(\"   1. Download the output folder after this notebook finishes.\")\n",
    "print(\"   2. Go to Kaggle -> Models -> New Model -> Upload the checkpoint files.\")\n",
    "print(\"   3. Set the Model ID below to match your upload.\")\n",
    "\n",
    "# Your Kaggle Model ID for Unrestricted Mode:\n",
    "unrestricted_kaggle_model = \"yuyamukai/tunix-gemma2-sft\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c1926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Visual Sanity Check ---\n",
    "print(\"Running Post-Training Evaluation...\")\n",
    "\n",
    "try:\n",
    "    inference_sampler = sampler_lib.Sampler(\n",
    "        transformer=lora_model,\n",
    "        tokenizer=tokenizer,\n",
    "        cache_config=sampler_lib.CacheConfig(\n",
    "            cache_size=MAX_GENERATION_STEPS + 512,\n",
    "            num_layers=model_config.num_layers,\n",
    "            num_kv_heads=model_config.num_kv_heads,\n",
    "            head_dim=model_config.head_dim,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    test_prompts = [\n",
    "        \"What are the ethical implications of AI in healthcare?\",\n",
    "        \"Write a short story about a robot learning to paint.\",\n",
    "        \"Explain why the sky is blue to a 5-year-old.\"\n",
    "    ]\n",
    "    \n",
    "    formatted_prompts = [TEMPLATE.format(question=p) for p in test_prompts]\n",
    "    \n",
    "    out_data = inference_sampler(\n",
    "        input_strings=formatted_prompts,\n",
    "        max_generation_steps=300,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    print(\"--- Post-Training Outputs ---\")\n",
    "    valid_format_count = 0\n",
    "    for p, o in zip(test_prompts, out_data.text):\n",
    "        print(f\"Prompt: {p}\")\n",
    "        print(f\"Output: {o[:500]}\")\n",
    "        \n",
    "        # Simple quantitative format check\n",
    "        if \"<reasoning>\" in o and \"</reasoning>\" in o and \"<answer>\" in o and \"</answer>\" in o:\n",
    "            valid_format_count += 1\n",
    "            print(\"‚úÖ Format Check: Passed\")\n",
    "        else:\n",
    "            print(\"‚ùå Format Check: Failed\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"Format Validation: {valid_format_count}/{len(test_prompts)} passed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b65a15",
   "metadata": {},
   "source": [
    "## [Optional 15pts] unrestricted mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For Unrestricted Mode, upload the saved checkpoint as a Kaggle Model.\n",
    "# Then update this variable with your Model ID:\n",
    "unrestricted_kaggle_model = \"yuyamukai/tunix-gemma2-sft\"\n",
    "\n",
    "print(f\"Unrestricted Mode Model ID: {unrestricted_kaggle_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7439db4",
   "metadata": {},
   "source": [
    "\n",
    "## Other things I want the judges to know\n",
    "\n",
    "### 1. Learnings\n",
    "*   **Domain Matters More Than Method**: Competition FAQ explicitly states verifiable tasks (math/code) have \"much lower weights\". We prioritized diverse domains (creative, analytical, philosophical) over math/code.\n",
    "*   **SFT Efficiency**: We processed ~100K samples vs ~1,500 GRPO steps in the same 9-hour window. SFT provides dense supervision at every token.\n",
    "*   **Reasoning Trace Quality**: Datasets like Raiden-DeepSeek-R1 are rare finds - most reasoning datasets focus on math/code where verification is easier.\n",
    "\n",
    "### 2. Data Sources (All Public, Apache 2.0/MIT/CC-BY)\n",
    "*   sequelbox/Raiden-DeepSeek-R1 - Creative & analytical reasoning\n",
    "*   O1-OPEN/OpenO1-SFT - General reasoning with explicit <Thought>/<Output> tags\n",
    "*   pharaouk/CoT-Collection - Commonsense & ethics tasks\n",
    "*   glaiveai/reasoning-v1-20m - Math/Code/General tasks\n",
    "\n",
    "### 3. Key Design Decisions\n",
    "*   **Format Standardization**: All datasets converted to consistent `<reasoning>`/`<answer>` tags\n",
    "*   **LoRA Training**: Efficient parameter updates for 9-hour constraint\n",
    "*   **Domain Priority**: Creative > Analytical > Philosophical > General > (Math/Code deprioritized)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
