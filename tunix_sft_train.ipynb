{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9de59a3",
   "metadata": {},
   "source": [
    "# Tunix SFT: Teaching Reasoning Through Demonstration\n",
    "\n",
    "**Strategy**: Supervised Fine-Tuning on high-quality reasoning traces across diverse domains.\n",
    "\n",
    "**Key Insight**: For 2B parameter models, learning from demonstrations is more effective than reinforcement learning. SFT provides dense supervision at every token, while RL provides sparse rewards only at sequence end.\n",
    "\n",
    "**Datasets**: \n",
    "- Raiden-DeepSeek-R1 (Creative/Analytical)\n",
    "- OpenO1-SFT (General Reasoning)\n",
    "- CoT-Collection (Commonsense/Ethics)\n",
    "- GlaiveAI-Reasoning (Math/Code/General)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89b7e55",
   "metadata": {},
   "source": [
    "\n",
    "## Overall training and evaluation strategy\n",
    "\n",
    "**Strategy: SFT on Diverse Domain Reasoning Traces**\n",
    "\n",
    "Competition FAQ explicitly states that verifiable tasks (math/coding) have \"much lower weights\". Our strategy prioritizes non-verifiable domains:\n",
    "\n",
    "1.  **Base Model**: We start with `Gemma-2-2b-it` for its instruction-following foundation.\n",
    "2.  **SFT Training**: We fine-tune on ~100K reasoning traces from diverse domains (creative, analytical, philosophical, commonsense).\n",
    "3.  **Format**: All data uses explicit `<reasoning>` and `<answer>` tags for structured outputs.\n",
    "\n",
    "## üó∫Ô∏è Workflow Diagram\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Gemma-2B-IT] --> B{SFT Training}\n",
    "    B -->|Creative| C[Raiden-DeepSeek-R1]\n",
    "    B -->|Reasoning| D[OpenO1-SFT]\n",
    "    B -->|Ethics| E[CoT-Collection]\n",
    "    B -->|General| F[GlaiveAI]\n",
    "    C & D & E & F --> G[Trained Model]\n",
    "    G --> H[Submission]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f07db9",
   "metadata": {},
   "source": [
    "\n",
    "## How your finetuning dataset is created\n",
    "\n",
    "We employ a **Diverse Domain Strategy** using publicly available datasets with reasoning traces:\n",
    "\n",
    "| Dataset | Source | Samples | Domain | License |\n",
    "|:---|:---|:---:|:---|:---|\n",
    "| Raiden-DeepSeek-R1 | HuggingFace | 62.9K | Creative/Analytical | Apache 2.0 |\n",
    "| OpenO1-SFT | HuggingFace | 20K (English-only) | General Reasoning | Apache 2.0 |\n",
    "| CoT-Collection | HuggingFace | 10K (pre-sampled) | Commonsense/Ethics | CC-BY-4.0 |\n",
    "| GlaiveAI-Reasoning | HuggingFace | 30K | Non-math/code | Apache 2.0 |\n",
    "\n",
    "All datasets are pre-processed and attached as parquet files for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a54d4",
   "metadata": {},
   "source": [
    "## Tunix finetuning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training parameters\n",
    "TEMPERATURE=0.7\n",
    "TOP_K=50\n",
    "TOP_P=0.9\n",
    "MAX_GENERATION_STEPS=768\n",
    "\n",
    "# Output Tags\n",
    "REASONING_START = \"<reasoning>\"\n",
    "REASONING_END = \"</reasoning>\"\n",
    "SOLUTION_START = \"<answer>\"\n",
    "SOLUTION_END = \"</answer>\"\n",
    "\n",
    "# Inference Params\n",
    "INF_TEMPERATURE=0\n",
    "INF_TOP_K=1\n",
    "INF_TOP_P=None\n",
    "SEED=42\n",
    "\n",
    "# System prompt and template\n",
    "SYSTEM_PROMPT = \"You are a deep thinking AI. Think step by step about the problem and provide your reasoning between <reasoning> and </reasoning> tags. Then, provide the final answer between <answer> and </answer> tags.\"\n",
    "TEMPLATE = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{{question}}<end_of_turn>\\n<start_of_turn>model\"\n",
    "\n",
    "print(\"Template variables defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup & Install ---\n",
    "!pip install -q wandb==0.22.0\n",
    "!pip install -q kagglehub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q transformers\n",
    "!pip install -q grain\n",
    "\n",
    "# Tunix/Qwix Installation\n",
    "# Check if we are offline (no internet), if so, assume wheels are attached\n",
    "import socket\n",
    "import os\n",
    "\n",
    "def is_connected():\n",
    "    try:\n",
    "        # Check simple connectivity\n",
    "        socket.create_connection((\"1.1.1.1\", 53))\n",
    "        return True\n",
    "    except OSError:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "if is_connected():\n",
    "    !pip install \"google-tunix[prod]==0.1.5\"\n",
    "    !pip install git+https://github.com/google/qwix\n",
    "else:\n",
    "    print(\"Offline mode detected. Assuming dependencies are installed or wheels provided.\")\n",
    "    # Fallback: Try installing from local wheels if available\n",
    "    if os.path.exists(\"/kaggle/input/tunix-wheels\"):\n",
    "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels google-tunix\n",
    "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels qwix\n",
    "\n",
    "\n",
    "# Fix Flax Version to 0.12.0 as required\n",
    "!pip uninstall -q -y flax\n",
    "!pip install flax==0.12.0\n",
    "\n",
    "!pip install -q datasets==3.2.0 optax==0.2.4 chex==0.1.88\n",
    "\n",
    "# --- Imports ---\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "from pprint import pprint\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "import tensorflow_datasets as tfds\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Tunix Imports\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma import model as gemma_lib\n",
    "from tunix.models.gemma import params as params_lib\n",
    "from tunix.sft import metrics_logger\n",
    "from tunix.sft import peft_trainer\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Stability Configs ---\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.95'\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "\n",
    "print(f\"JAX Devices: {jax.devices()}\")\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "DATASET_PATH = \"/kaggle/input/tunix-sft-data\"\n",
    "SFT_OUTPUT_DIR = \"/kaggle/working/sft_checkpoint\"\n",
    "\n",
    "# Tuning Hyperparams - Adjust these for HP tuning\n",
    "SFT_STEPS = 8000  # ~2 epochs with 122k samples, effective batch 32\n",
    "TRAIN_BATCH_SIZE = 8 # Per-step batch size across all 8 TPU chips (1 sample/chip)\n",
    "GRADIENT_ACCUMULATION = 4  # Effective batch = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION\n",
    "EFFECTIVE_BATCH = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION  # 32\n",
    "\n",
    "# Learning Rate - Key HP for tuning\n",
    "LEARNING_RATE = 2e-5  # Try: 5e-5, 2e-5, 1e-5\n",
    "WARMUP_STEPS = 200  # Warmup before reaching peak LR\n",
    "\n",
    "# LoRA Hyperparams\n",
    "RANK = 64\n",
    "ALPHA = 64.0\n",
    "\n",
    "# Sequence Length\n",
    "MAX_SEQ_LEN = 2048  # Critical: increased from 1024 to avoid truncating reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Utilities ---\n",
    "MESH = [(8, 1), (\"fsdp\", \"tp\")]\n",
    "\n",
    "def get_gemma_model(ckpt_path):\n",
    "    mesh = jax.make_mesh(*MESH)\n",
    "    model_config = gemma_lib.ModelConfig.gemma2_2b()\n",
    "    abs_gemma: nnx.Module = nnx.eval_shape(\n",
    "        lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
    "    )\n",
    "    abs_state = nnx.state(abs_gemma)\n",
    "    abs_state = jax.tree.map(\n",
    "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "        abs_state,\n",
    "        nnx.get_named_sharding(abs_state, mesh),\n",
    "    )\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "\n",
    "    graph_def, _ = nnx.split(abs_gemma)\n",
    "    gemma = nnx.merge(graph_def, restored_params)\n",
    "    return gemma, mesh, model_config\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    # LoRA config uses RANK and ALPHA from constants\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "            \".*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "\n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, rngs=nnx.Rngs(params=0), **model_input\n",
    "    )\n",
    "\n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "\n",
    "    return lora_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c593ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- WandB Logging with Metrics Backend ---\n",
    "WANDB_ENABLED = False\n",
    "\n",
    "# Define WandB Backend for MetricsLogger\n",
    "class WandbBackend:\n",
    "    '''Custom backend to stream metrics to WandB during training'''\n",
    "    def log_scalar(self, event: str, value, **kwargs):\n",
    "        if WANDB_ENABLED:\n",
    "            step = kwargs.get(\"step\", 0)\n",
    "            wandb.log({event: float(value)}, step=step)\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "    if secret_value:\n",
    "        wandb.login(key=secret_value)\n",
    "        # Log hyperparameters to WandB config\n",
    "        wandb.init(\n",
    "            project=\"tunix-sft-diverse\",\n",
    "            name=\"sft-run-v2\",\n",
    "            anonymous=\"allow\",\n",
    "            config={\n",
    "                \"sft_steps\": SFT_STEPS,\n",
    "                \"learning_rate\": LEARNING_RATE,\n",
    "                \"warmup_steps\": WARMUP_STEPS,\n",
    "                \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
    "                \"gradient_accumulation\": GRADIENT_ACCUMULATION,\n",
    "                \"effective_batch\": EFFECTIVE_BATCH,\n",
    "                \"max_seq_len\": MAX_SEQ_LEN,\n",
    "                \"lora_rank\": RANK,\n",
    "                \"lora_alpha\": ALPHA,\n",
    "                \"model_id\": MODEL_ID,\n",
    "            }\n",
    "        )\n",
    "        WANDB_ENABLED = True\n",
    "        print(\"WandB Logging Enabled with hyperparameter tracking.\")\n",
    "    else:\n",
    "        raise ValueError(\"Empty WANDB_API_KEY\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"WandB not enabled: {e}\")\n",
    "    os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "    print(\"Proceeding without cloud logging (WANDB_MODE='disabled').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ff10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Data Preprocessing ---\n",
    "# Download and process diverse domain datasets\n",
    "\n",
    "print(\"Loading datasets from Kaggle/HuggingFace...\")\n",
    "\n",
    "def standardize_to_gemma_format(text, question=None):\n",
    "    '''Convert various formats to Gemma chat template with <reasoning>/<answer> tags'''\n",
    "    \n",
    "    # Handle already formatted text\n",
    "    if \"<start_of_turn>\" in text:\n",
    "        # Just ensure we have our tags (case insensitive replacement)\n",
    "        text = re.sub(r\"<think>\", \"<reasoning>\", text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r\"</think>\", \"</reasoning>\", text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r\"<thought>\", \"<reasoning>\", text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r\"</thought>\", \"</reasoning>\", text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Case 1: Has <answer> but no <reasoning> - wrap content before <answer> as reasoning\n",
    "        if \"<answer>\" in text and \"<reasoning>\" not in text and \"<start_of_turn>model\" in text:\n",
    "            match = re.search(r\"<start_of_turn>model\\n(.*)(<answer>.*</answer>)\", text, re.DOTALL)\n",
    "            if match:\n",
    "                pre_answer = match.group(1).strip()\n",
    "                answer_tag = match.group(2)\n",
    "                if pre_answer:\n",
    "                    new_content = f\"<reasoning>{pre_answer}</reasoning>\\n{answer_tag}\"\n",
    "                    text = re.sub(r\"<start_of_turn>model\\n.*(<answer>.*</answer>)\", \n",
    "                                  f\"<start_of_turn>model\\n{new_content}\", text, flags=re.DOTALL)\n",
    "                else:\n",
    "                    # No content before answer - extract answer content as reasoning too\n",
    "                    answer_content = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL)\n",
    "                    if answer_content:\n",
    "                        text = re.sub(r\"<start_of_turn>model\\n\", \n",
    "                                      f\"<start_of_turn>model\\n<reasoning>{answer_content.group(1).strip()}</reasoning>\\n\", text)\n",
    "        \n",
    "        # Case 2: Enforce <answer> tags if missing (sometimes models output just the answer after start_of_turn)\n",
    "        elif \"<answer>\" not in text and \"<start_of_turn>model\" in text:\n",
    "            # Heuristic: Wrap the last part of the model turn in answer tags if not present\n",
    "            match = re.search(r\"<start_of_turn>model\\n(.*)$\", text, re.DOTALL)\n",
    "            if match:\n",
    "                 content = match.group(1).strip()\n",
    "                 # If no reasoning tag either, wrap whole thing\n",
    "                 if \"<reasoning>\" not in content:\n",
    "                     text = text.replace(content, f\"<answer>{content}</answer>\")\n",
    "                 else:\n",
    "                     # Reasoning exists but no answer - extract answer from after </reasoning>\n",
    "                     parts = content.split(\"</reasoning>\")\n",
    "                     if len(parts) > 1 and parts[1].strip():\n",
    "                         answer_part = parts[1].strip()\n",
    "                         text = text.replace(content, f\"{parts[0]}</reasoning>\\n<answer>{answer_part}</answer>\")\n",
    "                     else:\n",
    "                         # No content after reasoning, use reasoning summary as answer\n",
    "                         reasoning_match = re.search(r\"<reasoning>(.*?)</reasoning>\", content, re.DOTALL)\n",
    "                         if reasoning_match:\n",
    "                             # Use last sentence of reasoning as answer\n",
    "                             reasoning_text = reasoning_match.group(1).strip()\n",
    "                             sentences = reasoning_text.split(\".\")\n",
    "                             answer_fallback = sentences[-1].strip() if sentences else reasoning_text[:200]\n",
    "                             text = text + f\"\\n<answer>{answer_fallback}</answer>\"\n",
    "        return text\n",
    "    \n",
    "    # For raw question/response pairs\n",
    "    if question:\n",
    "        # Extract reasoning and answer from response\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "        \n",
    "        # Try to extract think/reasoning\n",
    "        think_match = re.search(r\"<think>(.*?)</think>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        thought_match = re.search(r\"<Thought>(.*?)</Thought>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        reasoning_tag_match = re.search(r\"<reasoning>(.*?)</reasoning>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if think_match:\n",
    "            reasoning = think_match.group(1).strip()\n",
    "        elif thought_match:\n",
    "            reasoning = thought_match.group(1).strip()\n",
    "        elif reasoning_tag_match:\n",
    "            reasoning = reasoning_tag_match.group(1).strip()\n",
    "        else:\n",
    "            # Use the whole text as reasoning if no specific tags found\n",
    "            reasoning = text.strip()\n",
    "        \n",
    "        # Try to extract answer\n",
    "        ans_match = re.search(r\"<Output>(.*?)</Output>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        if ans_match:\n",
    "            answer = ans_match.group(1).strip()\n",
    "        else:\n",
    "            answer_match = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL | re.IGNORECASE)\n",
    "            if answer_match:\n",
    "                answer = answer_match.group(1).strip()\n",
    "            else:\n",
    "                # If no explicit answer tag, assume the last paragraph is the answer\n",
    "                # or the whole text if reasoning was extracted from specific tags\n",
    "                if reasoning_tag_match or think_match or thought_match:\n",
    "                    # If reasoning was explicitly tagged, the rest is likely the answer\n",
    "                    # This is a heuristic and might need refinement for specific datasets\n",
    "                    remaining_text = re.sub(r\"<reasoning>.*?</reasoning>\", \"\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "                    remaining_text = re.sub(r\"<think>.*?</think>\", \"\", remaining_text, flags=re.DOTALL | re.IGNORECASE)\n",
    "                    remaining_text = re.sub(r\"<Thought>.*?</Thought>\", \"\", remaining_text, flags=re.DOTALL | re.IGNORECASE)\n",
    "                    answer = remaining_text.strip()\n",
    "                    if not answer and reasoning: # If no answer found, and reasoning was found, use reasoning as answer\n",
    "                        answer = reasoning\n",
    "                else:\n",
    "                    # If no specific tags for reasoning, and no answer tag,\n",
    "                    # try to split by paragraphs and take the last one as answer\n",
    "                    paragraphs = text.strip().split(\"\\n\\n\")\n",
    "                    answer = paragraphs[-1] if paragraphs else text[:200] # Fallback to first 200 chars\n",
    "        \n",
    "        # Ensure reasoning and answer are not empty\n",
    "        if not reasoning and answer:\n",
    "            reasoning = answer # If only answer, use it as reasoning\n",
    "        elif not answer and reasoning:\n",
    "            answer = reasoning # If only reasoning, use it as answer\n",
    "        elif not reasoning and not answer:\n",
    "            reasoning = text.strip()\n",
    "            answer = text.strip()\n",
    "\n",
    "        formatted = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n<start_of_turn>model\\n<reasoning>{reasoning}</reasoning>\\n<answer>{answer}</answer>\"\n",
    "        return formatted\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Load from Kaggle Dataset (pre-downloaded for efficiency)\n",
    "try:\n",
    "    # Primary: Load pre-processed data from Kaggle Dataset\n",
    "    all_texts = []\n",
    "    \n",
    "    # Try loading from attached dataset\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        import glob\n",
    "        # Load Parquet files (Preferred)\n",
    "        for parquet_file in glob.glob(f\"{DATASET_PATH}/*.parquet\"):\n",
    "            ds = datasets.load_dataset(\"parquet\", data_files=parquet_file, split=\"train\")\n",
    "            print(f\"Loaded {len(ds)} samples from {parquet_file}\")\n",
    "            \n",
    "            # Identify dataset type based on filename\n",
    "            fname = os.path.basename(parquet_file).lower()\n",
    "            \n",
    "            # 1. CoT-Collection (pre-sampled 10K)\n",
    "            if \"cot_collection\" in fname:\n",
    "                # CoT Collection: source (q), rationale (r), target (a)\n",
    "                # Data is pre-sampled to 10K, just load all rows\n",
    "                for sample in ds:\n",
    "                    q = sample.get(\"source\", \"\")\n",
    "                    r = sample.get(\"rationale\", \"\")\n",
    "                    a = sample.get(\"target\", \"\")\n",
    "                    formatted = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{q}<end_of_turn>\\n<start_of_turn>model\\n<reasoning>{r}</reasoning>\\n<answer>{a}</answer>\"\n",
    "                    all_texts.append({\"text\": formatted})\n",
    "\n",
    "            # 2. GlaiveAI-Reasoning\n",
    "            elif \"glaive\" in fname:\n",
    "                # Glaive: prompt, response (contains <think>...</think> then answer)\n",
    "                for sample in ds:\n",
    "                    q = sample.get(\"prompt\", sample.get(\"question\", sample.get(\"instruction\", \"\")))\n",
    "                    a = sample.get(\"response\", sample.get(\"answer\", sample.get(\"output\", \"\")))\n",
    "                    formatted = standardize_to_gemma_format(a, question=q)\n",
    "                    all_texts.append({\"text\": formatted})\n",
    "\n",
    "            # 3. OpenO1-SFT (pre-filtered English-only, instruction/output columns)\n",
    "            elif \"openo1\" in fname:\n",
    "                for sample in ds:\n",
    "                    q = sample.get(\"instruction\", \"\")\n",
    "                    a = sample.get(\"output\", \"\")\n",
    "                    if q and a:\n",
    "                        formatted = standardize_to_gemma_format(a, question=q)\n",
    "                        all_texts.append({\"text\": formatted})\n",
    "\n",
    "            # 4. Raiden (text column with pre-formatted conversation)\n",
    "            else: \n",
    "                for sample in ds:\n",
    "                     # Check if pre-formatted 'text' field exists\n",
    "                     if \"text\" in sample:\n",
    "                         formatted = standardize_to_gemma_format(sample[\"text\"])\n",
    "                         all_texts.append({\"text\": formatted})\n",
    "                     # Fallback to instruction/response pair\n",
    "                     elif (\"prompt\" in sample and \"response\" in sample):\n",
    "                         formatted = standardize_to_gemma_format(sample[\"response\"], question=sample[\"prompt\"])\n",
    "                         all_texts.append({\"text\": formatted})\n",
    "                     elif (\"instruction\" in sample and \"output\" in sample):\n",
    "                         formatted = standardize_to_gemma_format(sample[\"output\"], question=sample[\"instruction\"])\n",
    "                         all_texts.append({\"text\": formatted})\n",
    "\n",
    "    else:\n",
    "        print(f\"Dataset path {DATASET_PATH} not found. Downloading from HuggingFace...\")\n",
    "        \n",
    "        # Fallback: Download from HuggingFace\n",
    "        # 1. Raiden-DeepSeek-R1 (main dataset) - Safety limit to prevent timeout\n",
    "        raiden = datasets.load_dataset(\"sequelbox/Raiden-DeepSeek-R1\", split=\"train[:50000]\") \n",
    "        print(f\"Downloaded Raiden: {len(raiden)} samples\")\n",
    "        for sample in raiden:\n",
    "            prompt = sample.get(\"prompt\", \"\")\n",
    "            response = sample.get(\"response\", sample.get(\"completion\", \"\"))\n",
    "            \n",
    "            # Filter: Skip long outputs (>4000 chars ~1K tokens) or empty/short samples\n",
    "            if len(response) > 4000 or len(response) < 50:\n",
    "                continue\n",
    "                \n",
    "            if prompt and response:\n",
    "                formatted = standardize_to_gemma_format(response, question=prompt)\n",
    "                all_texts.append({\"text\": formatted})\n",
    "        \n",
    "        # 2. OpenO1-SFT - Safety limit\n",
    "        try:\n",
    "            # Limited to 50K to prevent timeout\n",
    "            openo1 = datasets.load_dataset(\"O1-OPEN/OpenO1-SFT\", split=\"train[:50000]\")\n",
    "            print(f\"Downloaded OpenO1: {len(openo1)} samples\")\n",
    "            for sample in openo1:\n",
    "                instruction = sample.get(\"instruction\", \"\")\n",
    "                output = sample.get(\"output\", \"\")\n",
    "                \n",
    "                # Filter Chinese characters to prevent language leakage\n",
    "                if any(u'‰∏Ä' <= c <= u'Èøø' for c in instruction + output):\n",
    "                    continue\n",
    "                    \n",
    "                if instruction and output:\n",
    "                    formatted = standardize_to_gemma_format(output, question=instruction)\n",
    "                    all_texts.append({\"text\": formatted})\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping OpenO1: {e}\")\n",
    "\n",
    "        # 3. GlaiveAI-Reasoning\n",
    "        try:\n",
    "            # Keep limit for Glaive as it's huge, but increase slightly to 30k\n",
    "            glaive = datasets.load_dataset(\"glaiveai/reasoning-v1-20m\", split=\"train[:30000]\")\n",
    "            print(f\"Downloaded GlaiveAI: {len(glaive)} samples\")\n",
    "            for sample in glaive:\n",
    "                instruction = sample.get(\"instruction\", \"\")\n",
    "                output = sample.get(\"output\", \"\")\n",
    "                if instruction and output:\n",
    "                    formatted = standardize_to_gemma_format(output, question=instruction)\n",
    "                    all_texts.append({\"text\": formatted})\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping GlaiveAI: {e}\")\n",
    "\n",
    "    print(f\"Total samples after preprocessing: {len(all_texts)}\")\n",
    "    \n",
    "    # Create HuggingFace dataset\n",
    "    sft_dataset = datasets.Dataset.from_list(all_texts)\n",
    "    sft_dataset = sft_dataset.shuffle(seed=42)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Failed to load datasets: {e}\")\n",
    "    raise RuntimeError(f\"Dataset loading failed: {e}\")\n",
    "\n",
    "print(f\"Final SFT dataset: {len(sft_dataset)} samples\")\n",
    "print(f\"Sample: {sft_dataset[0]['text'][:500]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cbb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Main Training Logic ---\n",
    "\n",
    "# 1. Download/setup Base Model\n",
    "if \"KAGGLE_USERNAME\" not in os.environ:\n",
    "    kagglehub.login()\n",
    "\n",
    "# Download Gemma 2 (Flax)\n",
    "model_path = { \"gemma2\": \"google/gemma-2/flax/\" }\n",
    "model_family = \"gemma2\"\n",
    "model_version = \"gemma2-2b-it\" \n",
    "kaggle_ckpt_path = kagglehub.model_download(f\"{model_path[model_family]}{model_version}\")\n",
    "\n",
    "# Convert checkpoint format for Tunix/NNX\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
    "!rm -rf {INTERMEDIATE_CKPT_DIR} {CKPT_DIR}\n",
    "\n",
    "params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\"))\n",
    "gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "_, state = nnx.split(gemma)\n",
    "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "checkpointer.wait_until_finished()\n",
    "del params, gemma, state\n",
    "gc.collect()\n",
    "\n",
    "# 2. Load Models\n",
    "base_model, mesh, model_config = get_gemma_model(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"))\n",
    "lora_model = get_lora_model(base_model, mesh=mesh)\n",
    "\n",
    "# 3. Setup Tokenizer\n",
    "tokenizer = tokenizer_lib.Tokenizer(\n",
    "    tokenizer_path=os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
    ")\n",
    "\n",
    "# 4. Baseline Evaluation (Same prompts as post-training for comparison)\n",
    "print(\"Running Baseline Evaluation...\")\n",
    "EVAL_PROMPTS = [\n",
    "    # Creative writing\n",
    "    \"Write a short story about a robot learning to paint.\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    # Creative ideation\n",
    "    \"Propose three innovative uses for AI in education.\",\n",
    "    # Summarization\n",
    "    \"Summarize the key benefits and risks of renewable energy in 3 paragraphs.\",\n",
    "    # Math (verifiable)\n",
    "    \"Solve step-by-step: If 2x + 5 = 15, what is x?\",\n",
    "    # Coding (verifiable)\n",
    "    \"Write a Python function to check if a string is a palindrome.\",\n",
    "    # Basic science\n",
    "    \"Explain why the sky is blue to a 5-year-old.\",\n",
    "    \"Explain the process of photosynthesis step by step.\",\n",
    "    # Ethics/Reasoning\n",
    "    \"What are the ethical implications of AI in healthcare?\",\n",
    "    \"Should AI systems have rights? Argue both sides.\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    baseline_sampler = sampler_lib.Sampler(\n",
    "        transformer=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        cache_config=sampler_lib.CacheConfig(\n",
    "            cache_size=MAX_SEQ_LEN + 512,\n",
    "            num_layers=model_config.num_layers,\n",
    "            num_kv_heads=model_config.num_kv_heads,\n",
    "            head_dim=model_config.head_dim,\n",
    "        ),\n",
    "    )\n",
    "    formatted = [TEMPLATE.format(question=p) for p in EVAL_PROMPTS]\n",
    "    baseline_out = baseline_sampler(\n",
    "        input_strings=formatted,\n",
    "        max_generation_steps=MAX_SEQ_LEN,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        echo=False\n",
    "    )\n",
    "    print(\"--- Baseline Outputs (Before Training) ---\")\n",
    "    baseline_results = []\n",
    "    for p, o in zip(EVAL_PROMPTS, baseline_out.text):\n",
    "        print(f\"Q: {p}\")\n",
    "        print(f\"A: {o}\")  # Full output\n",
    "        has_reasoning = bool(re.search(r\"<reasoning>.*?</reasoning>\", o, re.DOTALL))\n",
    "        has_answer = bool(re.search(r\"<answer>.*?</answer>\", o, re.DOTALL))\n",
    "        baseline_results.append({\"prompt\": p, \"output\": o, \"has_reasoning\": has_reasoning, \"has_answer\": has_answer})\n",
    "        print(\"-\"*40)\n",
    "except Exception as e:\n",
    "    print(f\"Baseline eval skipped: {e}\")\n",
    "    baseline_results = []\n",
    "print(\"Baseline Done.\")\n",
    "\n",
    "# 5. SFT Training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting SFT Training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Optimizer - Uses LEARNING_RATE from constants for HP tuning\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    decay_steps=SFT_STEPS,\n",
    "    end_value=LEARNING_RATE / 20  # End at 5% of peak\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(learning_rate=schedule, weight_decay=0.01)\n",
    ")\n",
    "\n",
    "# Checkpointing\n",
    "# Using Orbax options via TrainingConfig\n",
    "checkpoint_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=500, max_to_keep=2\n",
    ")\n",
    "\n",
    "# Data Iterator\n",
    "from tunix.sft import utils as sft_utils\n",
    "\n",
    "def create_data_iterator(dataset, batch_size, tokenizer):\n",
    "    '''Create batches with tokenization and masking'''\n",
    "    indices = np.random.permutation(len(dataset))\n",
    "    \n",
    "    # Infinite iterator matching steps\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            if len(batch_indices) < batch_size:\n",
    "                continue # Skip incomplete batches\n",
    "                \n",
    "            texts = [dataset[int(idx)]['text'] for idx in batch_indices]\n",
    "            \n",
    "            # Tokenize\n",
    "            # Tunix tokenizer returns list of ids\n",
    "            batch_input_tokens = []\n",
    "            batch_input_mask = []\n",
    "            \n",
    "            for text in texts:\n",
    "                # Use Tunix Tokenizer.tokenize which handles BOS/EOS\n",
    "                # tokenize returns np.array, convert to list for padding\n",
    "                tokens = tokenizer.tokenize(text, add_eos=True).tolist()\n",
    "                \n",
    "                # Truncate / Pad\n",
    "                if len(tokens) > MAX_SEQ_LEN:\n",
    "                    tokens = tokens[:MAX_SEQ_LEN]\n",
    "                    mask = [True] * MAX_SEQ_LEN\n",
    "                else:\n",
    "                    pad_len = MAX_SEQ_LEN - len(tokens)\n",
    "                    mask = [True] * len(tokens) + [False] * pad_len\n",
    "                    # Use pad_id if available, else 0\n",
    "                    pad_id = getattr(tokenizer, 'pad_id', lambda: 0)()\n",
    "                    tokens = tokens + [pad_id] * pad_len # 0 is usually pad, verify if needed\n",
    "                \n",
    "                batch_input_tokens.append(tokens)\n",
    "                batch_input_mask.append(mask)\n",
    "            \n",
    "            # Convert to JAX arrays\n",
    "            input_tokens = jnp.array(batch_input_tokens, dtype=jnp.int32)\n",
    "            input_mask = jnp.array(batch_input_mask, dtype=jnp.bool_)\n",
    "            \n",
    "            # Create PEFT required inputs\n",
    "            positions = sft_utils.build_positions_from_mask(input_mask)\n",
    "            attention_mask = sft_utils.make_causal_attn_mask(input_mask)\n",
    "            \n",
    "            yield {\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"input_mask\": input_mask,\n",
    "                \"positions\": positions,\n",
    "                \"attention_mask\": attention_mask\n",
    "            }\n",
    "\n",
    "# Training Configuration with WandB Metrics Backend\n",
    "from tunix.sft import metrics_logger as sft_metrics_logger\n",
    "\n",
    "metrics_logging_options = sft_metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/kaggle/working/logs\",\n",
    "    backend_factories=[WandbBackend] if WANDB_ENABLED else []\n",
    ")\n",
    "\n",
    "training_config = peft_trainer.TrainingConfig(\n",
    "    max_steps=SFT_STEPS,\n",
    "    checkpoint_root_directory=SFT_OUTPUT_DIR,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    checkpointing_options=checkpoint_options,\n",
    "    pbar_description=\"SFT Training\",\n",
    "    metrics_prefix=\"sft\",\n",
    "    metrics_logging_options=metrics_logging_options,\n",
    "    eval_every_n_steps=10000, # Disable freq eval for speed or set high\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "# Note: we pass the optimizer, model, and config.\n",
    "# Metrics logger defaults are fine.\n",
    "trainer = peft_trainer.PeftTrainer(\n",
    "    model=lora_model,\n",
    "    optimizer=optimizer,\n",
    "    training_config=training_config\n",
    ")\n",
    "\n",
    "# Create Iterator\n",
    "train_iter = create_data_iterator(sft_dataset, TRAIN_BATCH_SIZE, tokenizer)\n",
    "\n",
    "print(f\"Starting Training for {SFT_STEPS} steps...\")\n",
    "with mesh:\n",
    "    trainer.train(train_ds=train_iter, skip_jit=False)\n",
    "\n",
    "print(\"SFT Training Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save Final Model ---\n",
    "FINAL_SAVE_DIR = \"/kaggle/working/final_sft_model\"\n",
    "os.makedirs(FINAL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save the trained LoRA model checkpoint\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "checkpointer.save(os.path.join(FINAL_SAVE_DIR, \"checkpoint\"), nnx.state(lora_model, nnx.LoRAParam))\n",
    "checkpointer.wait_until_finished()\n",
    "\n",
    "print(f\"‚úÖ Model saved to '{FINAL_SAVE_DIR}/'\")\n",
    "print(\"To submit for Unrestricted Mode:\")\n",
    "print(\"   1. Download the output folder after this notebook finishes.\")\n",
    "print(\"   2. Go to Kaggle -> Models -> New Model -> Upload the checkpoint files.\")\n",
    "print(\"   3. Set the Model ID below to match your upload.\")\n",
    "\n",
    "# Your Kaggle Model ID for Unrestricted Mode:\n",
    "unrestricted_kaggle_model = \"yuyamukai/tunix-gemma2-sft\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ccccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Visual Sanity Check & Validation ---\n",
    "print(\"Running Post-Training Evaluation...\")\n",
    "\n",
    "try:\n",
    "    inference_sampler = sampler_lib.Sampler(\n",
    "        transformer=lora_model,\n",
    "        tokenizer=tokenizer,\n",
    "        cache_config=sampler_lib.CacheConfig(\n",
    "            cache_size=MAX_SEQ_LEN + 512,\n",
    "            num_layers=model_config.num_layers,\n",
    "            num_kv_heads=model_config.num_kv_heads,\n",
    "            head_dim=model_config.head_dim,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    test_prompts = [\n",
    "        # Creative writing\n",
    "        \"Write a short story about a robot learning to paint.\",\n",
    "        \"Write a haiku about artificial intelligence.\",\n",
    "        # Creative ideation\n",
    "        \"Propose three innovative uses for AI in education.\",\n",
    "        # Summarization\n",
    "        \"Summarize the key benefits and risks of renewable energy in 3 paragraphs.\",\n",
    "        # Math (verifiable)\n",
    "        \"Solve step-by-step: If 2x + 5 = 15, what is x?\",\n",
    "        # Coding (verifiable)\n",
    "        \"Write a Python function to check if a string is a palindrome.\",\n",
    "        # Basic science\n",
    "        \"Explain why the sky is blue to a 5-year-old.\",\n",
    "        \"Explain the process of photosynthesis step by step.\",\n",
    "        # Ethics/Reasoning\n",
    "        \"What are the ethical implications of AI in healthcare?\",\n",
    "        \"Should AI systems have rights? Argue both sides.\",\n",
    "    ]\n",
    "    \n",
    "    # Use same prompts as baseline for fair comparison\n",
    "    test_prompts = EVAL_PROMPTS\n",
    "    formatted_prompts = [TEMPLATE.format(question=p) for p in test_prompts]\n",
    "    \n",
    "    out_data = inference_sampler(\n",
    "        input_strings=formatted_prompts,\n",
    "        max_generation_steps=MAX_SEQ_LEN,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Validation Logic\n",
    "    print(\"--- Post-Training Outputs ---\")\n",
    "    valid_format_count = 0\n",
    "    results_for_wandb = []\n",
    "    \n",
    "    for p, o in zip(test_prompts, out_data.text):\n",
    "        print(f\"Prompt: {p}\")\n",
    "        print(f\"Output: {o}\")  # Full output, no truncation\n",
    "        \n",
    "        # Robust Regex Check\n",
    "        has_reasoning = bool(re.search(r\"<reasoning>.*?</reasoning>\", o, re.DOTALL))\n",
    "        has_answer = bool(re.search(r\"<answer>.*?</answer>\", o, re.DOTALL))\n",
    "        \n",
    "        is_valid = has_reasoning and has_answer\n",
    "        if is_valid:\n",
    "            valid_format_count += 1\n",
    "            print(\"‚úÖ Format Check: Passed\")\n",
    "        else:\n",
    "            print(f\"‚ùå Format Check: Failed (Reasoning: {has_reasoning}, Answer: {has_answer})\")\n",
    "            \n",
    "        results_for_wandb.append([p, o, is_valid])\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"Format Validation: {valid_format_count}/{len(test_prompts)} passed.\")\n",
    "    \n",
    "    # Extended WandB Evaluation (25 prompts for statistical confidence)\n",
    "    WANDB_EVAL_PROMPTS = [\n",
    "        # Original 10 prompts\n",
    "        *test_prompts,\n",
    "        # Additional 15 prompts for diversity\n",
    "        \"Explain quantum entanglement to a high school student.\",\n",
    "        \"Write a poem about the passage of time.\",\n",
    "        \"What are the pros and cons of remote work?\",\n",
    "        \"Describe how a compiler works step by step.\",\n",
    "        \"Compare democracy and authoritarianism objectively.\",\n",
    "        \"Write a short dialogue between a human and an AI about consciousness.\",\n",
    "        \"Explain the greenhouse effect and its consequences.\",\n",
    "        \"How would you teach a child about money management?\",\n",
    "        \"What lessons can we learn from the fall of ancient Rome?\",\n",
    "        \"Design a simple mobile app for tracking habits.\",\n",
    "        \"Explain the difference between correlation and causation.\",\n",
    "        \"Write a persuasive argument for learning a second language.\",\n",
    "        \"How do vaccines work to protect against diseases?\",\n",
    "        \"What ethical considerations arise with genetic engineering?\",\n",
    "        \"Explain the concept of supply and demand with examples.\",\n",
    "    ]\n",
    "    \n",
    "    # Run extended evaluation for WandB\n",
    "    try:\n",
    "        if wandb.run is not None and WANDB_ENABLED:\n",
    "            print(\"\\nRunning Extended WandB Evaluation (25 prompts)...\")\n",
    "            extended_formatted = [TEMPLATE.format(question=p) for p in WANDB_EVAL_PROMPTS]\n",
    "            extended_out = inference_sampler(\n",
    "                input_strings=extended_formatted,\n",
    "                max_generation_steps=MAX_SEQ_LEN,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                echo=False\n",
    "            )\n",
    "            \n",
    "            extended_results = []\n",
    "            extended_valid = 0\n",
    "            for p, o in zip(WANDB_EVAL_PROMPTS, extended_out.text):\n",
    "                has_r = bool(re.search(r\"<reasoning>.*?</reasoning>\", o, re.DOTALL))\n",
    "                has_a = bool(re.search(r\"<answer>.*?</answer>\", o, re.DOTALL))\n",
    "                is_valid = has_r and has_a\n",
    "                if is_valid:\n",
    "                    extended_valid += 1\n",
    "                extended_results.append([p, o[:1000], is_valid])  # Truncate for table\n",
    "            \n",
    "            # Log table\n",
    "            tbl = wandb.Table(columns=[\"Prompt\", \"Output\", \"IsValid\"], data=extended_results)\n",
    "            wandb.log({\"eval_results\": tbl})\n",
    "            \n",
    "            # Log summary metrics\n",
    "            format_compliance = extended_valid / len(WANDB_EVAL_PROMPTS) * 100\n",
    "            wandb.log({\n",
    "                \"eval/format_compliance_pct\": format_compliance,\n",
    "                \"eval/total_prompts\": len(WANDB_EVAL_PROMPTS),\n",
    "                \"eval/valid_count\": extended_valid,\n",
    "            })\n",
    "            print(f\"Extended Evaluation: {extended_valid}/{len(WANDB_EVAL_PROMPTS)} ({format_compliance:.1f}%) passed.\")\n",
    "            print(\"Logged to WandB: eval_results table + summary metrics.\")\n",
    "    except Exception as w_err:\n",
    "        print(f\"Extended WandB eval skipped: {w_err}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61b072d",
   "metadata": {},
   "source": [
    "## [Optional 15pts] unrestricted mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e7f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For Unrestricted Mode, upload the saved checkpoint as a Kaggle Model.\n",
    "# Then update this variable with your Model ID:\n",
    "unrestricted_kaggle_model = \"yuyamukai/tunix-gemma2-sft\"\n",
    "\n",
    "print(f\"Unrestricted Mode Model ID: {unrestricted_kaggle_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ce690",
   "metadata": {},
   "source": [
    "\n",
    "## Other things I want the judges to know\n",
    "\n",
    "### 1. Learnings\n",
    "*   **Domain Matters More Than Method**: Competition FAQ explicitly states verifiable tasks (math/code) have \"much lower weights\". We prioritized diverse domains (creative, analytical, philosophical) over math/code.\n",
    "*   **SFT Efficiency**: We processed ~123K samples vs ~1,500 GRPO steps in the same 9-hour window. SFT provides dense supervision at every token.\n",
    "*   **Reasoning Trace Quality**: Datasets like Raiden-DeepSeek-R1 are rare finds - most reasoning datasets focus on math/code where verification is easier.\n",
    "\n",
    "### 2. Data Sources (All Public, Apache 2.0/MIT/CC-BY)\n",
    "*   sequelbox/Raiden-DeepSeek-R1 - Creative & analytical reasoning\n",
    "*   O1-OPEN/OpenO1-SFT - General reasoning with explicit <Thought>/<Output> tags\n",
    "*   pharaouk/CoT-Collection - Commonsense & ethics tasks\n",
    "*   glaiveai/reasoning-v1-20m - Non-math/code: social science, creative writing\n",
    "\n",
    "### 3. Key Design Decisions\n",
    "*   **Format Standardization**: All datasets converted to consistent `<reasoning>`/`<answer>` tags\n",
    "*   **LoRA Training**: Efficient parameter updates for 9-hour constraint\n",
    "*   **Domain Priority**: Creative > Analytical > Philosophical > General > (Math/Code deprioritized)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
