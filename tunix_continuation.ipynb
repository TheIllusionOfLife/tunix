{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc450f4",
   "metadata": {},
   "source": [
    "# Tunix Unrestricted Mode: Continuation Training\n",
    "\n",
    "This notebook is designed for **Session 2+** of the Unrestricted Mode.\n",
    "\n",
    "**Prerequisites**:\n",
    "1. You have run `tunix_zero_cost_train.ipynb` (Session 1) and saved the checkpoint.\n",
    "2. You have uploaded the checkpoint as a Kaggle Dataset.\n",
    "3. You have uploaded `private_hard_reasoning.jsonl` as a separate private dataset.\n",
    "\n",
    "**What this notebook does**:\n",
    "1. Loads the checkpoint from your previous session.\n",
    "2. Continues training on harder data (your private dataset).\n",
    "3. Saves the final model for upload as a Kaggle Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# SESSION CONFIGURATION - UPDATE THESE FOR EACH RUN\n",
    "# ==============================================================================\n",
    "\n",
    "# Path to your previous session's checkpoint (uploaded as a Kaggle Dataset)\n",
    "PREV_CHECKPOINT_DATASET = \"/kaggle/input/tunix-session1-checkpoint/checkpoint\"\n",
    "# For Session 3, change to:\n",
    "# PREV_CHECKPOINT_DATASET = \"/kaggle/input/tunix-session2-checkpoint/checkpoint\"\n",
    "\n",
    "# Path to your training data (private hard reasoning dataset)\n",
    "DATA_DATASET = \"/kaggle/input/tunix-private-hard-reasoning\"\n",
    "\n",
    "# Training parameters\n",
    "GRPO_STEPS = 800  # Increase for longer training\n",
    "TRAIN_MICRO_BATCH_SIZE = 1\n",
    "\n",
    "# Output directory for the final model\n",
    "FINAL_SAVE_DIR = \"final_continuation_model\"\n",
    "\n",
    "# Your Kaggle Model ID (set this after uploading)\n",
    "unrestricted_kaggle_model = \"yuyamukai/tunix-gemma2-2b-unrestricted\"\n",
    "\n",
    "print(\"Configuration loaded. Ready to continue training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup & Install ---\n",
    "!pip install -q wandb==0.22.0\n",
    "!pip install -q kagglehub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q 'google-tunix[prod]==0.1.5' --no-deps\n",
    "!pip install -q orbax-checkpoint==0.11.4 grain==0.2.2 optax==0.2.4\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import ast\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "import kagglehub\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Tunix Imports\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma import model as gemma_lib\n",
    "from tunix.models.gemma import params as params_lib\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.rl.common import metrics_logger\n",
    "from tunix.models.gemma import qwix\n",
    "\n",
    "# JAX Config\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "print(f\"JAX Devices: {jax.devices()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Utilities ---\n",
    "MESH = [(8, 1), (\"fsdp\", \"tp\")]\n",
    "\n",
    "def get_gemma_ref_model(ckpt_path):\n",
    "  mesh = jax.make_mesh(*MESH)\n",
    "  model_config = gemma_lib.ModelConfig.gemma2_2b()\n",
    "  abs_gemma: nnx.Module = nnx.eval_shape(\n",
    "      lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
    "  )\n",
    "  abs_state = nnx.state(abs_gemma)\n",
    "  abs_state = jax.tree.map(\n",
    "      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "      abs_state,\n",
    "      nnx.get_named_sharding(abs_state, mesh),\n",
    "  )\n",
    "  checkpointer = ocp.StandardCheckpointer()\n",
    "  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "\n",
    "  graph_def, _ = nnx.split(abs_gemma)\n",
    "  gemma = nnx.merge(graph_def, restored_params)\n",
    "  return gemma, mesh, model_config\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "  RANK = 64\n",
    "  ALPHA = 64.0\n",
    "  lora_provider = qwix.LoraProvider(\n",
    "      module_path=(\n",
    "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "          \".*attn_vec_einsum\"\n",
    "      ),\n",
    "      rank=RANK,\n",
    "      alpha=ALPHA,\n",
    "  )\n",
    "\n",
    "  model_input = base_model.get_model_input()\n",
    "  lora_model = qwix.apply_lora_to_model(\n",
    "      base_model, lora_provider, rngs=nnx.Rngs(params=0), **model_input\n",
    "  )\n",
    "\n",
    "  with mesh:\n",
    "    state = nnx.state(lora_model)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    nnx.update(lora_model, sharded_state)\n",
    "\n",
    "  return lora_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ceb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load Checkpoint from Previous Session ---\n",
    "print(f\"Loading checkpoint from: {PREV_CHECKPOINT_DATASET}\")\n",
    "\n",
    "# First, get the base Gemma model structure (for tokenizer)\n",
    "if \"KAGGLE_USERNAME\" not in os.environ:\n",
    "    kagglehub.login()\n",
    "\n",
    "model_path = {\"gemma2\": \"google/gemma-2/flax/\"}\n",
    "model_family = \"gemma2\"\n",
    "model_version = \"gemma2-2b-it\"\n",
    "kaggle_ckpt_path = kagglehub.model_download(f\"{model_path[model_family]}{model_version}\")\n",
    "\n",
    "# Create intermediate directory for base model\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "!rm -rf {INTERMEDIATE_CKPT_DIR}\n",
    "\n",
    "params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\"))\n",
    "gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "_, state = nnx.split(gemma)\n",
    "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "checkpointer.wait_until_finished()\n",
    "del params, gemma, state\n",
    "gc.collect()\n",
    "\n",
    "# Load models\n",
    "ref_model, mesh, model_config = get_gemma_ref_model(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"))\n",
    "lora_policy = get_lora_model(ref_model, mesh=mesh)\n",
    "\n",
    "# Load previous session's LoRA weights\n",
    "print(\"Loading LoRA weights from previous session...\")\n",
    "abs_lora_params = jax.tree.map(\n",
    "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    nnx.state(lora_policy, nnx.LoRAParam),\n",
    ")\n",
    "prev_checkpointer = ocp.StandardCheckpointer()\n",
    "prev_lora_params = prev_checkpointer.restore(PREV_CHECKPOINT_DATASET, target=abs_lora_params)\n",
    "\n",
    "# Update the policy with loaded weights\n",
    "nnx.update(\n",
    "    lora_policy,\n",
    "    jax.tree.map(\n",
    "        lambda a, b: b,\n",
    "        nnx.state(lora_policy, nnx.LoRAParam),\n",
    "        prev_lora_params,\n",
    "    ),\n",
    ")\n",
    "print(\"✅ Previous checkpoint loaded successfully!\")\n",
    "\n",
    "# Setup Tokenizer\n",
    "tokenizer = tokenizer_lib.Tokenizer(\n",
    "    tokenizer_path=os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e23e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Continue GRPO Training on Hard Data ---\n",
    "print(\"Loading private/hard training data...\")\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a deep thinking AI. Think step by step and provide reasoning between <reasoning> and </reasoning> tags. Then provide the final answer between <answer> and </answer> tags.\"\n",
    "TEMPLATE = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{{question}}<end_of_turn>\\n<start_of_turn>model\"\n",
    "\n",
    "# --- Reward Functions (same as Session 1) ---\n",
    "def soft_structure_reward(prompts, completions, **kwargs):\n",
    "    rewards = []\n",
    "    for c in completions:\n",
    "        score = 0.0\n",
    "        if \"<reasoning>\" in c: score += 0.1\n",
    "        if \"</reasoning>\" in c: score += 0.1\n",
    "        if \"<answer>\" in c: score += 0.1\n",
    "        if \"</answer>\" in c: score += 0.1\n",
    "        if re.search(r\"<reasoning>.*?</reasoning>\", c, re.DOTALL): score += 0.3\n",
    "        if re.search(r\"<answer>.*?</answer>\", c, re.DOTALL): score += 0.3\n",
    "        rewards.append(min(1.0, score))\n",
    "    return rewards\n",
    "\n",
    "def structure_reward(prompts, completions, **kwargs):\n",
    "    rewards = []\n",
    "    for c in completions:\n",
    "        has_reasoning = \"<reasoning>\" in c and \"</reasoning>\" in c\n",
    "        has_answer = \"<answer>\" in c and \"</answer>\" in c\n",
    "        score = 0.5 * has_reasoning + 0.5 * has_answer\n",
    "        rewards.append(score)\n",
    "    return rewards\n",
    "\n",
    "def math_correctness_reward(prompts, completions, answer, **kwargs):\n",
    "    rewards = []\n",
    "    for c, gt in zip(completions, answer):\n",
    "        try:\n",
    "            match = re.search(r\"<answer>(.*?)</answer>\", c, re.DOTALL)\n",
    "            if match:\n",
    "                extracted = match.group(1).strip()\n",
    "                if float(extracted) == float(gt):\n",
    "                    rewards.append(1.0)\n",
    "                else:\n",
    "                    rewards.append(0.0)\n",
    "            else:\n",
    "                rewards.append(0.0)\n",
    "        except:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "def code_correctness_reward(prompts, completions, answer, **kwargs):\n",
    "    rewards = []\n",
    "    for c, gt in zip(completions, answer):\n",
    "        try:\n",
    "            # First try to extract from <answer> tags\n",
    "            ans_match = re.search(r\"<answer>(.*?)</answer>\", c, re.DOTALL)\n",
    "            if ans_match:\n",
    "                code_str = ans_match.group(1).strip()\n",
    "            else:\n",
    "                code_str = c.strip()\n",
    "            \n",
    "            # Check syntax validity\n",
    "            ast.parse(code_str)\n",
    "            # Check if ground truth is contained\n",
    "            if gt.strip() in code_str or code_str in gt.strip():\n",
    "                rewards.append(1.0)\n",
    "            else:\n",
    "                rewards.append(0.5)  # Valid syntax but different from GT\n",
    "        except:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "# Load Hard Dataset\n",
    "try:\n",
    "    hard_data_file = f\"{DATA_DATASET}/private_hard_reasoning.jsonl\"\n",
    "    grpo_dataset = datasets.load_dataset(\"json\", data_files=hard_data_file, split=\"train\")\n",
    "    print(f\"Loaded {len(grpo_dataset)} hard reasoning samples.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Failed to load dataset: {e}\")\n",
    "    print(f\"Please ensure '{DATA_DATASET}' is attached with required files.\")\n",
    "    raise RuntimeError(f\"Dataset loading failed. Cannot proceed without data. Error: {e}\")\n",
    "\n",
    "# Optimizer\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=3e-6,  # Lower LR for continuation\n",
    "    warmup_steps=50,\n",
    "    decay_steps=GRPO_STEPS,\n",
    "    end_value=1e-7\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(learning_rate=schedule, weight_decay=0.1)\n",
    ")\n",
    "\n",
    "# Configs\n",
    "GRPO_OUTPUT_DIR = \"grpo_continuation_checkpoint\"\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=500, max_to_keep=2\n",
    ")\n",
    "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/tmp/grpo_logs\", flush_every_n_steps=20\n",
    ")\n",
    "\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
    "    },\n",
    "    rollout_engine='vanilla',\n",
    "    offload_to_cpu=False,\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        eval_every_n_steps=10,\n",
    "        max_steps=GRPO_STEPS,\n",
    "        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        metrics_logging_options=metrics_logging_options,\n",
    "        checkpoint_root_directory=GRPO_OUTPUT_DIR,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "         max_tokens_to_generate=400,\n",
    "         max_prompt_length=256,\n",
    "         kv_cache_size=1024,\n",
    "         temperature=0.9, top_p=1.0, top_k=50\n",
    "    ),\n",
    ")\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=4,\n",
    "    num_iterations=1,\n",
    "    beta=0.08,\n",
    "    epsilon=0.2,\n",
    ")\n",
    "\n",
    "# Create Cluster\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    actor=lora_policy,\n",
    "    reference=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    cluster_config=cluster_config,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=[soft_structure_reward, structure_reward, math_correctness_reward, code_correctness_reward],\n",
    "    algo_config=grpo_config,  # v0.1.5 API uses 'algo_config'\n",
    ")\n",
    "\n",
    "# Data Formatting & Training\n",
    "with mesh:\n",
    "    def format_fn(x):\n",
    "        return {\n",
    "            \"prompts\": x[\"prompt\"],  # Already formatted in dataset\n",
    "            \"answer\": x.get(\"answer\", \"\")\n",
    "        }\n",
    "    \n",
    "    train_ds = grpo_dataset.map(format_fn)\n",
    "    \n",
    "    import itertools\n",
    "    import numpy as np\n",
    "\n",
    "    def batched(iterable, n):\n",
    "        it = iter(iterable)\n",
    "        while True:\n",
    "            chunk = list(itertools.islice(it, n))\n",
    "            if not chunk: return\n",
    "            batch = {k: np.array([d[k] for d in chunk]) for k in chunk[0]}\n",
    "            yield batch\n",
    "\n",
    "    def infinite_batch_generator(ds):\n",
    "        while True:\n",
    "            for batch in batched(ds.shuffle(seed=int(time.time())), TRAIN_MICRO_BATCH_SIZE):\n",
    "                yield batch\n",
    "\n",
    "    # Start Training\n",
    "    print(\"Starting continuation GRPO training...\")\n",
    "    trainer.train(infinite_batch_generator(train_ds))\n",
    "\n",
    "print(\"✅ Continuation GRPO Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403cc339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save Final Model for Kaggle Upload ---\n",
    "os.makedirs(FINAL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "checkpointer.save(os.path.join(FINAL_SAVE_DIR, \"checkpoint\"), nnx.state(lora_policy, nnx.LoRAParam))\n",
    "checkpointer.wait_until_finished()\n",
    "\n",
    "print(f\"✅ Final model saved to '{FINAL_SAVE_DIR}/'\")\n",
    "print(\"\")\n",
    "print(\"=== NEXT STEPS ===\")\n",
    "print(\"1. Download the output folder after this notebook finishes.\")\n",
    "print(\"2. Go to Kaggle -> Models -> New Model -> Upload the checkpoint files.\")\n",
    "print(f\"3. Set the Model ID to: {unrestricted_kaggle_model}\")\n",
    "print(\"\")\n",
    "print(\"Or, to continue for another session:\")\n",
    "print(\"1. Upload the output as a Dataset (e.g., 'tunix-session2-checkpoint')\")\n",
    "print(\"2. Update PREV_CHECKPOINT_DATASET in the config cell\")\n",
    "print(\"3. Run this notebook again\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Unrestricted Mode Submission ---\n",
    "# This is the Kaggle Model ID for the 15 bonus points.\n",
    "# Make sure you have uploaded the model files BEFORE submission.\n",
    "# Note: unrestricted_kaggle_model is defined in the config cell above.\n",
    "\n",
    "print(f\"Unrestricted Mode Model ID: {unrestricted_kaggle_model}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
