{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0beca7",
   "metadata": {},
   "source": [
    "# Tunix Inference & Evaluation\n",
    "\n",
    "This notebook loads a trained Tunix SFT checkpoint (Gemma 2 2B + LoRA) and runs inference on evaluation prompts.\n",
    "Use this to verify model performance without re-running training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup & Install ---\n",
    "!pip install -q kagglehub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q transformers\n",
    "!pip install -q grain\n",
    "\n",
    "import socket\n",
    "import os\n",
    "\n",
    "def is_connected():\n",
    "    try:\n",
    "        socket.create_connection((\"1.1.1.1\", 53))\n",
    "        return True\n",
    "    except OSError:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "if is_connected():\n",
    "    !pip install -q -U chex==0.1.90\n",
    "    !pip install -q -U google-tunix[prod]==0.1.5 distrax==0.1.7 optax==0.2.6\n",
    "    !pip install git+https://github.com/google/qwix\n",
    "else:\n",
    "    print(\"Offline mode detected. Assuming dependencies are installed or wheels provided.\")\n",
    "    # Fallback: Try installing from local wheels if available\n",
    "    if os.path.exists(\"/kaggle/input/tunix-wheels\"):\n",
    "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels google-tunix\n",
    "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels qwix\n",
    "\n",
    "# Fix Flax Version\n",
    "!pip uninstall -q -y flax\n",
    "!pip install flax==0.12.0\n",
    "\n",
    "!pip install -q datasets==3.2.0 optax==0.2.4 chex==0.1.88\n",
    "\n",
    "# --- Imports ---\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "\n",
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "from orbax import checkpoint as ocp\n",
    "import qwix\n",
    "import numpy as np\n",
    "\n",
    "# Tunix Imports\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma import model as gemma_lib\n",
    "from tunix.models.gemma import params as params_lib\n",
    "\n",
    "# --- Config ---\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.95'\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "\n",
    "print(f\"JAX Devices: {jax.devices()}\")\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_DIR = \"/kaggle/input/tunix-sft-checkpoint-v5/sft_checkpoint\"  # Updated for manual dataset upload\n",
    "# CHECKPOINT_DIR = \"/kaggle/working/sft_checkpoint\"  # Default for training\n",
    "\n",
    "RANK = 64\n",
    "ALPHA = 64.0\n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "# Inference Params\n",
    "INFERENCE_TEMPERATURE = 0.0 # Greedy decoding as per competition check\n",
    "INFERENCE_TOP_K = 1\n",
    "INFERENCE_TOP_P = None\n",
    "MAX_GENERATION_STEPS = 2048\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Loading Utilities ---\n",
    "MESH = [(8, 1), (\"fsdp\", \"tp\")]\n",
    "\n",
    "def get_gemma_model(ckpt_path):\n",
    "    mesh = jax.make_mesh(*MESH)\n",
    "    model_config = gemma_lib.ModelConfig.gemma2_2b()\n",
    "    abs_gemma: nnx.Module = nnx.eval_shape(\n",
    "        lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
    "    )\n",
    "    abs_state = nnx.state(abs_gemma)\n",
    "    abs_state = jax.tree.map(\n",
    "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "        abs_state,\n",
    "        nnx.get_named_sharding(abs_state, mesh),\n",
    "    )\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    # Restore base model params\n",
    "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "\n",
    "    graph_def, _ = nnx.split(abs_gemma)\n",
    "    gemma = nnx.merge(graph_def, restored_params)\n",
    "    return gemma, mesh, model_config\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "            \".*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "\n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, rngs=nnx.Rngs(params=0), **model_input\n",
    "    )\n",
    "\n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "\n",
    "    return lora_model\n",
    "\n",
    "def restore_lora_checkpoint(lora_model, checkpoint_path):\n",
    "    '''Restores LoRA adapter weights from Orbax checkpoint'''\n",
    "    print(f\"Restoring LoRA weights from {checkpoint_path}...\")\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    \n",
    "    # We only need to restore the params structure\n",
    "    abstract_state = nnx.state(lora_model, nnx.LoRAParam)\n",
    "    restored_state = checkpointer.restore(checkpoint_path, target=abstract_state)\n",
    "    \n",
    "    # Update model with restored LoRA params\n",
    "    nnx.update(lora_model, restored_state)\n",
    "    print(\"LoRA weights restored.\")\n",
    "    return lora_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab54ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Load Base Model ---\n",
    "if \"KAGGLE_USERNAME\" not in os.environ:\n",
    "    kagglehub.login()\n",
    "\n",
    "# Download Base Gemma 2\n",
    "model_path = { \"gemma2\": \"google/gemma-2/flax/\" }\n",
    "model_version = \"gemma2-2b-it\" \n",
    "kaggle_ckpt_path = kagglehub.model_download(f\"{model_path['gemma2']}{model_version}\")\n",
    "\n",
    "# Convert/Prepare Base Checkpoint\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "if not os.path.exists(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")):\n",
    "    print(\"Converting base checkpoint...\")\n",
    "    params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\"))\n",
    "    gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    _, state = nnx.split(gemma)\n",
    "    checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "    checkpointer.wait_until_finished()\n",
    "    del params, gemma, state\n",
    "    gc.collect()\n",
    "\n",
    "# Load Base Model\n",
    "print(\"Loading Base Model...\")\n",
    "base_model, mesh, model_config = get_gemma_model(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"))\n",
    "lora_model = get_lora_model(base_model, mesh=mesh)\n",
    "\n",
    "# Setup Tokenizer\n",
    "tokenizer = tokenizer_lib.Tokenizer(\n",
    "    tokenizer_path=os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Load Trained Adapters ---\n",
    "# Find latest checkpoint\n",
    "import glob\n",
    "try:\n",
    "    # Orbax checkpoints are typically directories named by step number, e.g., 20500\n",
    "    # We need to find the step directories inside CHECKPOINT_DIR\n",
    "    \n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        raise ValueError(f\"Checkpoint directory {CHECKPOINT_DIR} does not exist.\")\n",
    "        \n",
    "    # List subdirectories that are integers (steps)\n",
    "    subdirs = [d for d in os.listdir(CHECKPOINT_DIR) if os.path.isdir(os.path.join(CHECKPOINT_DIR, d)) and d.isdigit()]\n",
    "    if not subdirs:\n",
    "        raise ValueError(f\"No step checkpoints found in {CHECKPOINT_DIR}\")\n",
    "    \n",
    "    latest_step = max([int(d) for d in subdirs])\n",
    "    checkpoint_step_dir = os.path.join(CHECKPOINT_DIR, str(latest_step))\n",
    "    \n",
    "    print(f\"Found latest checkpoint step: {latest_step}\")\n",
    "    print(f\"Directory: {checkpoint_step_dir}\")\n",
    "    \n",
    "    # --- Debug: List Directory Contents ---\n",
    "    print(\"--- Directory Structure ---\")\n",
    "    for root, dirs, files in os.walk(checkpoint_step_dir):\n",
    "        level = root.replace(checkpoint_step_dir, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        for f in files:\n",
    "            print('{}{}'.format(indent + '    ', f))\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "    # Smart Path Detection\n",
    "    # Orbax/Tunix might save under 'default', 'params', 'state', 'model_params', or directly in the step dir\n",
    "    potential_subdirs = [\"default\", \"params\", \"state\", \"model_params\", \".\"]\n",
    "    \n",
    "    checkpoint_path = None\n",
    "    \n",
    "    for sub in potential_subdirs:\n",
    "        path = os.path.join(checkpoint_step_dir, sub) if sub != \".\" else checkpoint_step_dir\n",
    "        # Check if it looks like a checkpoint (contains msgpack or similar)\n",
    "        # Or just try to restore from it\n",
    "        if os.path.exists(path):\n",
    "            # Basic check: does it contain files?\n",
    "             if len(os.listdir(path)) > 0:\n",
    "                 print(f\"Attempting to restore from CANDIDATE path: {path}\")\n",
    "                 try:\n",
    "                     restore_lora_checkpoint(lora_model, path)\n",
    "                     checkpoint_path = path\n",
    "                     print(\"✅ Restore successful!\")\n",
    "                     break\n",
    "                 except Exception as restore_err:\n",
    "                     print(f\"       ⚠️ Failed to restore from {path}: {restore_err}\")\n",
    "    \n",
    "    if checkpoint_path is None:\n",
    "         raise RuntimeError(\"Could not find a valid checkpoint structure in any standard subdirectory.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Failed to load checkpoint: {e}\")\n",
    "    # Raise error to STOP execution. Do not continue to inference with random weights.\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756be927",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Run Inference ---\n",
    "print(\"Running Evaluation (Strict Template Mode)...\")\n",
    "\n",
    "# --- Set Random Seeds ---\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "print(f\"Random seed set to {SEED}\")\n",
    "\n",
    "prompts = [\n",
    "    \"Write a short story about a robot learning to paint.\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"Propose three innovative uses for AI in education.\",\n",
    "    \"Summarize the key benefits and risks of renewable energy in 3 paragraphs.\",\n",
    "    \"Solve step-by-step: If 2x + 5 = 15, what is x?\",\n",
    "    \"Write a Python function to check if a string is a palindrome.\",\n",
    "    \"Explain why the sky is blue to a 5-year-old.\",\n",
    "    \"Explain the process of photosynthesis step by step.\",\n",
    "    \"What are the ethical implications of AI in healthcare?\",\n",
    "    \"Should AI systems have rights? Argue both sides.\",\n",
    "]\n",
    "\n",
    "# --- Competition-Compliant Prompt Template ---\n",
    "PROMPT_TEMPLATE = \"<start_of_turn>user\\nYou are a deep thinking AI. Think step by step about the problem and provide your reasoning between <reasoning> and </reasoning> tags. Then, provide the final answer between <answer> and </answer> tags.\\n\\n{question}<end_of_turn>\\n<start_of_turn>model\"\n",
    "\n",
    "# Calculate Dynamic MAX_PROMPT_LENGTH\n",
    "print(\"Calculating prompt lengths for cache sizing...\")\n",
    "formatted_prompts_for_sizing = [PROMPT_TEMPLATE.format(question=p) for p in prompts]\n",
    "prompt_lengths = [len(tokenizer.encode(p)) for p in formatted_prompts_for_sizing]\n",
    "MAX_PROMPT_LENGTH = max(prompt_lengths)\n",
    "print(f\"Detected Max Prompt Length: {MAX_PROMPT_LENGTH} tokens\")\n",
    "\n",
    "print(f\"Initializing Sampler with MAX_GENERATION_STEPS={MAX_GENERATION_STEPS}...\")\n",
    "inference_sampler = sampler_lib.Sampler(\n",
    "    transformer=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        cache_size=MAX_PROMPT_LENGTH + MAX_GENERATION_STEPS + 256, # Formula from template\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# --- Judge Class (Mimicking Template Structure) ---\n",
    "class TunixHackathonJudge:\n",
    "    def __init__(self, temperature, top_k, top_p, max_steps, seed):\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.max_steps = max_steps\n",
    "        self.seed = seed\n",
    "\n",
    "    def evaluate(self, sampler, prompt_template, questions):\n",
    "        valid_count = 0\n",
    "        print(\"--- Results & Validation (Greedy Decoding) ---\")\n",
    "        \n",
    "        for i, question in enumerate(questions):\n",
    "            print(f\"\\nProcessing Prompt {i+1}/{len(questions)}...\")\n",
    "            formatted_prompt = prompt_template.format(question=question)\n",
    "            \n",
    "            try:\n",
    "                # Run Inference\n",
    "                out_data = sampler(\n",
    "                    input_strings=[formatted_prompt],\n",
    "                    max_generation_steps=self.max_steps,\n",
    "                    temperature=self.temperature,\n",
    "                    top_k=self.top_k,\n",
    "                    top_p=self.top_p,\n",
    "                    echo=False\n",
    "                )\n",
    "                output_text = out_data.text[0]\n",
    "                \n",
    "                print(f\"Prompt: {question}\")\n",
    "                print(f\"Output: {output_text}\")\n",
    "                \n",
    "                # --- Format Validation ---\n",
    "                has_reasoning = bool(re.search(r\"<reasoning>.*?</reasoning>\", output_text, re.DOTALL))\n",
    "                has_answer = bool(re.search(r\"<answer>.*?</answer>\", output_text, re.DOTALL))\n",
    "                \n",
    "                if has_reasoning and has_answer:\n",
    "                    valid_count += 1\n",
    "                    print(\"✅ Format Check: Passed\")\n",
    "                else:\n",
    "                    print(f\"❌ Format Check: Failed (Reasoning: {has_reasoning}, Answer: {has_answer})\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "            except Exception as e:\n",
    "                 print(f\"❌ Error generating response for prompt {i+1}: {e}\")\n",
    "            \n",
    "            # Memory Cleanup\n",
    "            gc.collect()\n",
    "            \n",
    "        return valid_count\n",
    "\n",
    "# --- Run the Judge ---\n",
    "judge = TunixHackathonJudge(\n",
    "    temperature=INFERENCE_TEMPERATURE,\n",
    "    top_k=INFERENCE_TOP_K,\n",
    "    top_p=INFERENCE_TOP_P,\n",
    "    max_steps=MAX_GENERATION_STEPS,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "score = judge.evaluate(inference_sampler, PROMPT_TEMPLATE, prompts)\n",
    "\n",
    "print(f\"\\nFinal Score: {score}/{len(prompts)} ({score/len(prompts)*100:.1f}%) formatted correctly.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
