{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e7331c7c",
            "metadata": {},
            "source": [
                "# Tunix Inference & Evaluation\n",
                "\n",
                "This notebook loads a trained Tunix SFT checkpoint (Gemma 2 2B + LoRA) and runs inference on evaluation prompts.\n",
                "Use this to verify model performance without re-running training.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a5577b4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Setup & Install ---\n",
                "!pip install -q kagglehub\n",
                "!pip install -q ipywidgets\n",
                "!pip install -q tensorflow\n",
                "!pip install -q tensorflow_datasets\n",
                "!pip install -q tensorboardX\n",
                "!pip install -q transformers\n",
                "!pip install -q grain\n",
                "\n",
                "import socket\n",
                "import os\n",
                "\n",
                "def is_connected():\n",
                "    try:\n",
                "        socket.create_connection((\"1.1.1.1\", 53))\n",
                "        return True\n",
                "    except OSError:\n",
                "        pass\n",
                "    return False\n",
                "\n",
                "if is_connected():\n",
                "    !pip install -q -U chex==0.1.90\n",
                "    !pip install -q -U google-tunix[prod]==0.1.5 distrax==0.1.7 optax==0.2.6\n",
                "    !pip install git+https://github.com/google/qwix\n",
                "else:\n",
                "    print(\"Offline mode detected. Assuming dependencies are installed or wheels provided.\")\n",
                "    # Fallback: Try installing from local wheels if available\n",
                "    if os.path.exists(\"/kaggle/input/tunix-wheels\"):\n",
                "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels google-tunix\n",
                "        !pip install --no-index --find-links=/kaggle/input/tunix-wheels qwix\n",
                "\n",
                "# Fix Flax Version\n",
                "!pip uninstall -q -y flax\n",
                "!pip install flax==0.12.0\n",
                "\n",
                "!pip install -q datasets==3.2.0 optax==0.2.4 chex==0.1.88\n",
                "\n",
                "# --- Imports ---\n",
                "import functools\n",
                "import gc\n",
                "import os\n",
                "import re\n",
                "import time\n",
                "import shutil\n",
                "from pprint import pprint\n",
                "\n",
                "from flax import nnx\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "import kagglehub\n",
                "from orbax import checkpoint as ocp\n",
                "import qwix\n",
                "import numpy as np\n",
                "\n",
                "# Tunix Imports\n",
                "from tunix.generate import sampler as sampler_lib\n",
                "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
                "from tunix.models.gemma import model as gemma_lib\n",
                "from tunix.models.gemma import params as params_lib\n",
                "\n",
                "# --- Config ---\n",
                "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.95'\n",
                "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
                "\n",
                "print(f\"JAX Devices: {jax.devices()}\")\n",
                "\n",
                "# Paths\n",
                "CHECKPOINT_DIR = \"/kaggle/working/sft_checkpoint\"  # Where checkpoints were saved during training\n",
                "# CHECKPOINT_DIR = \"/kaggle/input/your-model-dataset/sft_checkpoint\" # Uncomment if loading from uploaded dataset\n",
                "\n",
                "RANK = 64\n",
                "ALPHA = 64.0\n",
                "MAX_SEQ_LEN = 2048\n",
                "\n",
                "# Inference Params\n",
                "INFERENCE_TEMPERATURE = 0.7\n",
                "INFERENCE_TOP_K = 50\n",
                "INFERENCE_TOP_P = 0.95\n",
                "EVAL_MAX_TOKENS = 1024\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d2c6d921",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Model Loading Utilities ---\n",
                "MESH = [(8, 1), (\"fsdp\", \"tp\")]\n",
                "\n",
                "def get_gemma_model(ckpt_path):\n",
                "    mesh = jax.make_mesh(*MESH)\n",
                "    model_config = gemma_lib.ModelConfig.gemma2_2b()\n",
                "    abs_gemma: nnx.Module = nnx.eval_shape(\n",
                "        lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
                "    )\n",
                "    abs_state = nnx.state(abs_gemma)\n",
                "    abs_state = jax.tree.map(\n",
                "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
                "        abs_state,\n",
                "        nnx.get_named_sharding(abs_state, mesh),\n",
                "    )\n",
                "    checkpointer = ocp.StandardCheckpointer()\n",
                "    # Restore base model params\n",
                "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
                "\n",
                "    graph_def, _ = nnx.split(abs_gemma)\n",
                "    gemma = nnx.merge(graph_def, restored_params)\n",
                "    return gemma, mesh, model_config\n",
                "\n",
                "def get_lora_model(base_model, mesh):\n",
                "    lora_provider = qwix.LoraProvider(\n",
                "        module_path=(\n",
                "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
                "            \".*attn_vec_einsum\"\n",
                "        ),\n",
                "        rank=RANK,\n",
                "        alpha=ALPHA,\n",
                "    )\n",
                "\n",
                "    model_input = base_model.get_model_input()\n",
                "    lora_model = qwix.apply_lora_to_model(\n",
                "        base_model, lora_provider, rngs=nnx.Rngs(params=0), **model_input\n",
                "    )\n",
                "\n",
                "    with mesh:\n",
                "        state = nnx.state(lora_model)\n",
                "        pspecs = nnx.get_partition_spec(state)\n",
                "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
                "        nnx.update(lora_model, sharded_state)\n",
                "\n",
                "    return lora_model\n",
                "\n",
                "def restore_lora_checkpoint(lora_model, checkpoint_path):\n",
                "    '''Restores LoRA adapter weights from Orbax checkpoint'''\n",
                "    print(f\"Restoring LoRA weights from {checkpoint_path}...\")\n",
                "    checkpointer = ocp.StandardCheckpointer()\n",
                "    \n",
                "    # We only need to restore the params structure\n",
                "    abstract_state = nnx.state(lora_model, nnx.LoRAParam)\n",
                "    restored_state = checkpointer.restore(checkpoint_path, target=abstract_state)\n",
                "    \n",
                "    # Update model with restored LoRA params\n",
                "    nnx.update(lora_model, restored_state)\n",
                "    print(\"LoRA weights restored.\")\n",
                "    return lora_model\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a8763b62",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 1. Load Base Model ---\n",
                "if \"KAGGLE_USERNAME\" not in os.environ:\n",
                "    kagglehub.login()\n",
                "\n",
                "# Download Base Gemma 2\n",
                "model_path = { \"gemma2\": \"google/gemma-2/flax/\" }\n",
                "model_version = \"gemma2-2b-it\" \n",
                "kaggle_ckpt_path = kagglehub.model_download(f\"{model_path['gemma2']}{model_version}\")\n",
                "\n",
                "# Convert/Prepare Base Checkpoint\n",
                "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
                "if not os.path.exists(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")):\n",
                "    print(\"Converting base checkpoint...\")\n",
                "    params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\"))\n",
                "    gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
                "    checkpointer = ocp.StandardCheckpointer()\n",
                "    _, state = nnx.split(gemma)\n",
                "    checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
                "    checkpointer.wait_until_finished()\n",
                "    del params, gemma, state\n",
                "    gc.collect()\n",
                "\n",
                "# Load Base Model\n",
                "print(\"Loading Base Model...\")\n",
                "base_model, mesh, model_config = get_gemma_model(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"))\n",
                "lora_model = get_lora_model(base_model, mesh=mesh)\n",
                "\n",
                "# Setup Tokenizer\n",
                "tokenizer = tokenizer_lib.Tokenizer(\n",
                "    tokenizer_path=os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_checkpoint",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2. Load Trained Adapters ---\n",
                "# Find latest checkpoint\n",
                "import glob\n",
                "try:\n",
                "    # Orbax checkpoints are typically directories named by step number, e.g., 20500\n",
                "    # We need to find the step directories inside CHECKPOINT_DIR\n",
                "    # Structure: CHECKPOINT_DIR/20500/default/checkpoint...\n",
                "    \n",
                "    # List subdirectories that are integers (steps)\n",
                "    subdirs = [d for d in os.listdir(CHECKPOINT_DIR) if os.path.isdir(os.path.join(CHECKPOINT_DIR, d)) and d.isdigit()]\n",
                "    if not subdirs:\n",
                "        raise ValueError(f\"No step checkpoints found in {CHECKPOINT_DIR}\")\n",
                "    \n",
                "    latest_step = max([int(d) for d in subdirs])\n",
                "    checkpoint_path = os.path.join(CHECKPOINT_DIR, str(latest_step))\n",
                "    \n",
                "    print(f\"Loading checkpoint from step: {latest_step}\")\n",
                "    print(f\"Path: {checkpoint_path}\")\n",
                "    \n",
                "    # Orbax Manager usually stores items under 'default' or similar key if using CheckpointManager\n",
                "    # But PeftTrainer usage implies CheckpointManager structure.\n",
                "    # Let's try to restore directly from the step directory, which CheckpointManager manages.\n",
                "    # Note: StandardCheckpointer expects the directory CONTAINING the data.\n",
                "    \n",
                "    # With CheckpointManager, the structure for step N is usually root/N/default/\n",
                "    # If PeftTrainer uses 'default' item name.\n",
                "    potential_path = os.path.join(checkpoint_path, \"default\")\n",
                "    if os.path.exists(potential_path):\n",
                "        checkpoint_path = potential_path\n",
                "    \n",
                "    restore_lora_checkpoint(lora_model, checkpoint_path)\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Failed to load checkpoint: {e}\")\n",
                "    print(\"Listing directory:\")\n",
                "    if os.path.exists(CHECKPOINT_DIR):\n",
                "        pprint(os.listdir(CHECKPOINT_DIR))\n",
                "    else:\n",
                "        print(\"Checkpoint dir not found.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run_eval",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 3. Run Inference ---\n",
                "print(\"Running Evaluation...\")\n",
                "\n",
                "prompts = [\n",
                "    \"Write a short story about a robot learning to paint.\",\n",
                "    \"Write a haiku about artificial intelligence.\",\n",
                "    \"Propose three innovative uses for AI in education.\",\n",
                "    \"Summarize the key benefits and risks of renewable energy in 3 paragraphs.\",\n",
                "    \"Solve step-by-step: If 2x + 5 = 15, what is x?\",\n",
                "    \"Write a Python function to check if a string is a palindrome.\",\n",
                "    \"Explain why the sky is blue to a 5-year-old.\",\n",
                "    \"Explain the process of photosynthesis step by step.\",\n",
                "    \"What are the ethical implications of AI in healthcare?\",\n",
                "    \"Should AI systems have rights? Argue both sides.\",\n",
                "]\n",
                "\n",
                "SYSTEM_PROMPT = \"You are a deep thinking AI. Think step by step about the problem and provide your reasoning between <reasoning> and </reasoning> tags. Then, provide the final answer between <answer> and </answer> tags.\"\n",
                "TEMPLATE = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{{question}}<end_of_turn>\\n<start_of_turn>model\"\n",
                "formatted_prompts = [TEMPLATE.format(question=p) for p in prompts]\n",
                "\n",
                "inference_sampler = sampler_lib.Sampler(\n",
                "    transformer=lora_model,\n",
                "    tokenizer=tokenizer,\n",
                "    cache_config=sampler_lib.CacheConfig(\n",
                "        cache_size=MAX_SEQ_LEN + 512,\n",
                "        num_layers=model_config.num_layers,\n",
                "        num_kv_heads=model_config.num_kv_heads,\n",
                "        head_dim=model_config.head_dim,\n",
                "    ),\n",
                ")\n",
                "\n",
                "out_data = inference_sampler(\n",
                "    input_strings=formatted_prompts,\n",
                "    max_generation_steps=EVAL_MAX_TOKENS,\n",
                "    temperature=INFERENCE_TEMPERATURE,\n",
                "    top_k=INFERENCE_TOP_K,\n",
                "    top_p=INFERENCE_TOP_P,\n",
                "    echo=False\n",
                ")\n",
                "\n",
                "print(\"--- Results ---\")\n",
                "for p, o in zip(prompts, out_data.text):\n",
                "    print(f\"Prompt: {p}\")\n",
                "    print(f\"Output: {o}\\n\")\n",
                "    print(\"-\"*50)\n"
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 5
}