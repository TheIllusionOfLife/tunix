[Important] submission template and FAQs
Thank you for your interest and passion about the Tunix hackathon! We are very impressed with your progress so far. To further help you succeed in this hackathon, we have put together a notebook submission template for you and a list of Frequently Asked Questions based on your feedback on the discussion forum and Discord.

We are always listening to your feedback, so this thread will be updated from time to time. If you have new questions along the way, make sure you check back here.

Notebook submission template
Based on the community feedback, this is the submission template we strongly recommend all participants to follow for consistent evaluation. Not following this template increases the chance that we cannot reproduce your model correctly, which means you will lose a lot of points.

Frequently Asked Questions
1.The competition description says participants are expected to use Gemma2 2B or Gemma3 1B for this hackathon. Can I use another model (for example, Gemma3 270M or Gemma3 4B)?

Answer: the short answer is No. We chose these 2 models in consideration of compute available on Kaggle, model capability and JAX model availability:

Kaggle only offers limited TPU compute (v5e-8 with 16G HBM per core): 9 hours per session and 20 hours per week. While it's true that larger models are better, it requires a lot more compute and we do not want to force participants to purchase compute credits.
Models smaller than 1B are too weak for this hackathon. We have experimented with Gemma3 270m. Although it works technically, the resulting model is too small to do reasoning unfortunately.
Currently Tunix only supports Gemma, Qwen and Llama models. Gemma model series have the best support but Gemma 3n is not implemented. Overall, given that we only have limited bandwidth for evaluating model quality, we are going to stick with Gemma2 2B and Gemma3 1B this time.
2.Can you clarify the difference between single-session and multi-session modes for evaluation?

Answer: Good question. For model quality eval we have 2 modes:

(45 pts) Single-session mode. In this mode, you can only load one of the 2 stock Gemma models from Google via the official Tunix APIs and finish the training in one (loading other checkpoints is not allowed and will be heavily penalized). We will be running your notebook in a single TPU session (9hrs) and reproduce the model before sending it over to eval. If you use private data or tools that we cannot access, our reproduction training cannot finish and you get 0 pt.
(Optional 15 pts) Unrestricted mode. We also call this mode ‘multi-session mode’ in the original competition description. In this mode, you can do whatever you like, private data/technique, training across multiple TPU sessions, resuming from earlier checkpoints, etc., as long as you explicitly provide a Kaggle model ID (for example, 'windmaple/gpt2' is a model I uploaded on Kaggle) in the submitted notebook. We will be using the Gemma2 2B/Gemma3 1B modelling code in Tunix to load this model up for evaluation.
Please refer to the submission template for more details on how to participate in both modes.

3.Can we create new synthetic datasets to finetune the model?

Answer: Yes you can. But do note that in the single session mode, we will be reproducing your model first and then do evals; if you use private data and do not make it publicly accessible, we cannot reproduce your model and you may not get any point.

For multi-session mode, we do not care how you train the model; we just need a loadable Kaggle model ID for eval.

4.Can we use other LLMs or tools to guide or assist the finetuning, for example, as reward signals or data generation?

Answer: Yes you can. However, similarly to the question above, since in the single-session mode we will be reproducing your model before evaluation, you need to make sure whatever is used in your training is accessible to us. In general we have access to the majority of Google products (e.g., Gemini API) but may not have access to non-Google products. If you are not sure, please ask in the discussion forum or Discord.

5.The starter notebooks only use reinforcement learning. Can we use other techniques such as supervised finetuning?

Answer: Absolutely! Tunix supports a range of post-training techniques, including SFT, preference tuning, RL and distillation. You can find other notebooks in the Tunix example folder.

6.How will the evaluation be performed after my submission?

Answer: After your submission, this is what will happen:

For the notebook and video, we will have a group of human judges evaluate the quality.
For the single-session mode, we will reproduce your model by running your notebook first. We will then take the last checkpoint in a 9-hour run and use that for evaluation. It is critical for you to make sure that we are able to reproduce your model. We will use AI+human to review your model’s output, based on a private evaluation dataset (see below).
For the unrestricted mode, we will take the Kaggle model ID you specify in the notebook (see the submission template) and use that for evaluation (with AI+human). If no Kaggle model ID is explicitly specified in the notebook or your model is not loadable by Tunix, no point is given.
A private evaluation dataset will be constructed from scratch for AI-based eval (human judges may do some vibe evals as well). The dataset will cover a range of domains and we will not be using any public dataset. And evaluation rubrics will be more comprehensive than what are used in the math reasoning starter notebook (accuracy, partial accuracy and format accuracy).

We will not release the evaluation dataset or disclose its composition. One thing we could mention is that verifiable tasks (math&coding) will have much lower weights because 1) the starter notebooks already cover math and 1B or 2B models aren’t very good with math in general, especially without tools 2) Gemma is not particularly well trained with code.

7.Where can I download datasets?

Answer: Unlike other Kaggle competition, this particular hackathon does not provide any stock dataset. You are expected to find any suitable dataset (e.g., Kaggle, Hugging Face or other platforms) by yourself.

8.Why is there no leaderboard?

Answer: This is a Kaggle hackathon, which is a little different from a typical Kaggle competition. We chose this format on purpose because evaluation is a bit tricky. There is no single metric to optimize for. For example, your video/notebook quality is not something that can be put on the leaderboard. In addition, we do not want you to finetune a model just to optimize some metric; we want you to train a general model that can actually be useful in real life. Participants are expected to come up with their own training data and evaluation strategies, as in any real world training.

9.When do I need to make my training data publicly available?

Answer: You need to make sure your training data is accessible for model reproduction before our evaluation process starts (this will be shortly after the submission deadline). Other than this, there is no requirement on when or on which platform you release your dataset.

10.My notebook is throwing a strange error. What should I do?

Answer: There could be many reasons. We recommend looking into:

Are you using the TPU image on Kaggle?
Are you using the right library versions (Tunix, Flax, etc.)?
Can you run the starter notebooks at all? If none of these helps, you can ask in the discussion forum or the Discord channel and we will do our best to help.