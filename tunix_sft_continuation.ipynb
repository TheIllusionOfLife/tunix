{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b89dc26",
   "metadata": {},
   "source": [
    "# Tunix SFT: Continuation Training (Unrestricted Mode)\n",
    "\n",
    "**Strategy**: Continue Supervised Fine-Tuning on a larger dataset (GlaiveAI) starting from the Session 1 checkpoint.\n",
    "\n",
    "**Prerequisites**:\n",
    "1. Run Session 1 notebook (`tunix_sft_train.ipynb`) and save output.\n",
    "2. Upload the Session 1 output as a Kaggle Dataset (e.g., `tunix-session1-checkpoint`).\n",
    "3. Attach that dataset to this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# Update these paths based on your Kaggle Dataset names\n",
    "\n",
    "# Path to checkpoint from Session 1 (Uploaded as Dataset)\n",
    "# Format: /kaggle/input/{dataset-name}/{folder-structure}\n",
    "PREV_CHECKPOINT_PATH = \"/kaggle/input/tunix-session1-checkpoint/final_sft_model/checkpoint\"\n",
    "\n",
    "# Path to continuation training data (Fresh GlaiveAI samples - NOT overlapping with session 1)\n",
    "# This is a NEW Kaggle dataset containing 100K samples from GlaiveAI (skipped first 30K used in session 1)\n",
    "CONTINUATION_DATA_PATH = \"/kaggle/input/tunix-sft-continuation-data\"\n",
    "\n",
    "# Training Config\n",
    "SFT_STEPS = 5000  # More steps for extended training\n",
    "LEARNING_RATE = 5e-6 # Lower LR for continuation\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION = 16\n",
    "\n",
    "# System Prompt (must match session 1)\n",
    "SYSTEM_PROMPT = \"You are a deep thinking AI. Think step by step about the problem and provide your reasoning between <reasoning> and </reasoning> tags. Then, provide the final answer between <answer> and </answer> tags.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b968135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup & Install ---\n",
    "!pip install -q wandb==0.22.0\n",
    "!pip install -q kagglehub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q transformers\n",
    "!pip install -q grain\n",
    "\n",
    "# Tunix/Qwix Installation\n",
    "import socket\n",
    "import os\n",
    "\n",
    "def is_connected():\n",
    "    try:\n",
    "        socket.create_connection((\"1.1.1.1\", 53))\n",
    "        return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "if is_connected():\n",
    "    !pip install \"google-tunix[prod]==0.1.5\"\n",
    "    !pip install git+https://github.com/google/qwix\n",
    "else:\n",
    "    print(\"Offline mode detected. Assuming legacy installation or wheels.\")\n",
    "\n",
    "\n",
    "# Fix Flax Version\n",
    "!pip uninstall -q -y flax\n",
    "!pip install flax==0.12.0\n",
    "!pip install -q datasets==3.2.0 optax==0.2.4 chex==0.1.88\n",
    "\n",
    "# --- Imports ---\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from flax import nnx\n",
    "import grain\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "import qwix\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Tunix Imports\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma import model as gemma_lib\n",
    "from tunix.models.gemma import params as params_lib\n",
    "from tunix.sft import peft_trainer\n",
    "\n",
    "# Stability\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.95'\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "print(f\"JAX Devices: {jax.devices()}\")\n",
    "\n",
    "# Constants\n",
    "MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "SFT_OUTPUT_DIR = \"/kaggle/working/sft_continuation_checkpoint\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Utilities ---\n",
    "MESH = [(8, 1), (\"fsdp\", \"tp\")]\n",
    "\n",
    "def get_gemma_model(ckpt_path):\n",
    "    # Load Base Model Structure\n",
    "    mesh = jax.make_mesh(*MESH)\n",
    "    model_config = gemma_lib.ModelConfig.gemma2_2b()\n",
    "    abs_gemma: nnx.Module = nnx.eval_shape(\n",
    "        lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
    "    )\n",
    "    abs_state = nnx.state(abs_gemma)\n",
    "    abs_state = jax.tree.map(\n",
    "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "        abs_state,\n",
    "        nnx.get_named_sharding(abs_state, mesh),\n",
    "    )\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "\n",
    "    graph_def, _ = nnx.split(abs_gemma)\n",
    "    gemma = nnx.merge(graph_def, restored_params)\n",
    "    return gemma, mesh, model_config\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    # Tunix LoRA Config\n",
    "    RANK = 64\n",
    "    ALPHA = 64.0\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "            \".*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "\n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, rngs=nnx.Rngs(params=0), **model_input\n",
    "    )\n",
    "\n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "\n",
    "    return lora_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load Checkpoint & Prepare Model ---\n",
    "\n",
    "# 1. Download Base Model (for tokenizer & structure)\n",
    "if \"KAGGLE_USERNAME\" not in os.environ:\n",
    "    kagglehub.login()\n",
    "\n",
    "kaggle_ckpt_path = kagglehub.model_download(f\"google/gemma-2/flax/gemma2-2b-it\")\n",
    "\n",
    "# Prepare intermediate conversion\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "if not os.path.exists(INTERMEDIATE_CKPT_DIR):\n",
    "    print(\"Converting base model checkpoint...\")\n",
    "    params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\"))\n",
    "    gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    _, state = nnx.split(gemma)\n",
    "    checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "    checkpointer.wait_until_finished()\n",
    "    del params, gemma, state\n",
    "    gc.collect()\n",
    "\n",
    "# 2. Initialize Models\n",
    "print(\"Initializing Base Model...\")\n",
    "base_model, mesh, model_config = get_gemma_model(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"))\n",
    "lora_model = get_lora_model(base_model, mesh=mesh)\n",
    "\n",
    "# 3. Load Previous Session State (LoRA weights)\n",
    "print(f\"Restoring Session 1 Checkpoint from: {PREV_CHECKPOINT_PATH}\")\n",
    "\n",
    "try:\n",
    "    # Map structure for LoRA params\n",
    "    abs_lora_params = jax.tree.map(\n",
    "        lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "        nnx.state(lora_model, nnx.LoRAParam),\n",
    "    )\n",
    "    \n",
    "    # Restore\n",
    "    prev_checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_lora_params = prev_checkpointer.restore(PREV_CHECKPOINT_PATH, target=abs_lora_params)\n",
    "    \n",
    "    # Update model\n",
    "    nnx.update(lora_model, restored_lora_params)\n",
    "    print(\"✅ Successfully restored previous SFT state.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to restore checkpoint: {e}\")\n",
    "    print(\"Double check PREV_CHECKPOINT_PATH. If this is the first run, this is expected to fail.\")\n",
    "    print(\"CRITICAL: Continuing without loaded state means restarting training from scratch!\")\n",
    "    # raise e # Uncomment to enforce strict loading\n",
    "\n",
    "# 4. Tokenizer\n",
    "tokenizer = tokenizer_lib.Tokenizer(\n",
    "    tokenizer_path=os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load Continuation Dataset (Pre-sampled GlaiveAI) ---\n",
    "# This parquet file contains 100K fresh samples from GlaiveAI\n",
    "# (samples 30,001 - 130,000, NOT overlapping with single session)\n",
    "\n",
    "import glob\n",
    "import re\n",
    "\n",
    "print(f\"Loading continuation data from {CONTINUATION_DATA_PATH}...\")\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "def standardize_to_gemma_format(text, question=None):\n",
    "    '''Standardize GlaiveAI format to Gemma conversation format'''\n",
    "    # Replace GlaiveAI's <think> tags with our <reasoning> tags\n",
    "    text = re.sub(r\"<think>\", \"<reasoning>\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"</think>\", \"</reasoning>\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Case 1: Has <reasoning> but no <answer> - extract answer from after </reasoning>\n",
    "    if \"<reasoning>\" in text and \"<answer>\" not in text:\n",
    "        parts = text.split(\"</reasoning>\")\n",
    "        if len(parts) > 1:\n",
    "            reasoning_part = parts[0] + \"</reasoning>\"\n",
    "            answer_part = parts[1].strip()\n",
    "            if answer_part:\n",
    "                text = f\"{reasoning_part}\\n<answer>{answer_part}</answer>\"\n",
    "            else:\n",
    "                # No content after reasoning - use last sentence as answer fallback\n",
    "                reasoning_match = re.search(r\"<reasoning>(.*?)</reasoning>\", text, re.DOTALL)\n",
    "                if reasoning_match:\n",
    "                    reasoning_text = reasoning_match.group(1).strip()\n",
    "                    sentences = reasoning_text.split(\".\")\n",
    "                    answer_fallback = sentences[-1].strip() if sentences and sentences[-1].strip() else reasoning_text[:200]\n",
    "                    text = f\"{text}\\n<answer>{answer_fallback}</answer>\"\n",
    "    \n",
    "    # Case 2: No <reasoning> AND no <answer> - wrap entire text with both tags\n",
    "    elif \"<reasoning>\" not in text and \"<answer>\" not in text:\n",
    "        # Treat text as combined reasoning+answer\n",
    "        # Use all but last paragraph as reasoning, last paragraph as answer\n",
    "        paragraphs = text.strip().split(\"\\n\\n\")\n",
    "        if len(paragraphs) > 1:\n",
    "            reasoning = \"\\n\\n\".join(paragraphs[:-1])\n",
    "            answer = paragraphs[-1]\n",
    "        else:\n",
    "            # Single paragraph - use as both\n",
    "            reasoning = text.strip()\n",
    "            answer = text.strip()\n",
    "        text = f\"<reasoning>{reasoning}</reasoning>\\n<answer>{answer}</answer>\"\n",
    "    \n",
    "    # Build full conversation format\n",
    "    if question:\n",
    "        formatted = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n<start_of_turn>model\\n{text}\"\n",
    "        return formatted\n",
    "    return text\n",
    "\n",
    "# Load parquet files from continuation dataset\n",
    "try:\n",
    "    for parquet_file in glob.glob(f\"{CONTINUATION_DATA_PATH}/*.parquet\"):\n",
    "        ds = datasets.load_dataset(\"parquet\", data_files=parquet_file, split=\"train\")\n",
    "        print(f\"Loaded {len(ds)} samples from {parquet_file}\")\n",
    "        \n",
    "        for sample in ds:\n",
    "            q = sample.get(\"prompt\", \"\")\n",
    "            a = sample.get(\"response\", \"\")\n",
    "            \n",
    "            if q and a:\n",
    "                formatted = standardize_to_gemma_format(a, question=q)\n",
    "                all_texts.append({\"text\": formatted})\n",
    "    \n",
    "    print(f\"Total continuation samples: {len(all_texts)}\")\n",
    "    \n",
    "    # Create HuggingFace dataset\n",
    "    sft_dataset = datasets.Dataset.from_list(all_texts)\n",
    "    sft_dataset = sft_dataset.shuffle(seed=42)\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nSample: {sft_dataset[0]['text'][:500]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Failed to load continuation data: {e}\")\n",
    "    raise RuntimeError(f\"Dataset loading failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8754f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Continuation Training ---\n",
    "\n",
    "print(\"Starting SFT Continuation...\")\n",
    "\n",
    "# Imports for Training\n",
    "import numpy as np\n",
    "from tunix.sft import utils as sft_utils\n",
    "\n",
    "# Optimizer (Lower LR)\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=LEARNING_RATE,\n",
    "    warmup_steps=100,\n",
    "    decay_steps=SFT_STEPS,\n",
    "    end_value=1e-7\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(learning_rate=schedule, weight_decay=0.01)\n",
    ")\n",
    "\n",
    "# Training Config\n",
    "checkpoint_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=500, max_to_keep=2\n",
    ")\n",
    "\n",
    "MAX_SEQ_LEN = 1024\n",
    "\n",
    "def create_data_iterator(dataset, batch_size, tokenizer):\n",
    "    '''Create batches with tokenization and masking'''\n",
    "    indices = np.random.permutation(len(dataset))\n",
    "    \n",
    "    # Infinite iterator matching steps\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            if len(batch_indices) < batch_size:\n",
    "                continue\n",
    "                \n",
    "            texts = [dataset[int(idx)]['text'] for idx in batch_indices]\n",
    "            \n",
    "            # Tokenize\n",
    "            batch_input_tokens = []\n",
    "            batch_input_mask = []\n",
    "            \n",
    "            for text in texts:\n",
    "                # Use Tunix Tokenizer.tokenize which handles BOS/EOS\n",
    "                tokens = tokenizer.tokenize(text, add_eos=True).tolist()\n",
    "                \n",
    "                # Truncate / Pad\n",
    "                if len(tokens) > MAX_SEQ_LEN:\n",
    "                    tokens = tokens[:MAX_SEQ_LEN]\n",
    "                    mask = [True] * MAX_SEQ_LEN\n",
    "                else:\n",
    "                    pad_len = MAX_SEQ_LEN - len(tokens)\n",
    "                    mask = [True] * len(tokens) + [False] * pad_len\n",
    "                    pad_id = getattr(tokenizer, 'pad_id', lambda: 0)()\n",
    "                    tokens = tokens + [pad_id] * pad_len\n",
    "                \n",
    "                batch_input_tokens.append(tokens)\n",
    "                batch_input_mask.append(mask)\n",
    "            \n",
    "            # Convert to JAX arrays\n",
    "            input_tokens = jnp.array(batch_input_tokens, dtype=jnp.int32)\n",
    "            input_mask = jnp.array(batch_input_mask, dtype=jnp.bool_)\n",
    "            \n",
    "            # Create PEFT required inputs\n",
    "            positions = sft_utils.build_positions_from_mask(input_mask)\n",
    "            attention_mask = sft_utils.make_causal_attn_mask(input_mask)\n",
    "            \n",
    "            yield {\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"input_mask\": input_mask,\n",
    "                \"positions\": positions,\n",
    "                \"attention_mask\": attention_mask\n",
    "            }\n",
    "\n",
    "training_config = peft_trainer.TrainingConfig(\n",
    "    max_steps=SFT_STEPS,\n",
    "    checkpoint_root_directory=SFT_OUTPUT_DIR,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    checkpointing_options=checkpoint_options,\n",
    "    pbar_description=\"SFT Continuation\",\n",
    "    metrics_prefix=\"sft_cont\",\n",
    "    eval_every_n_steps=10000,\n",
    ")\n",
    "\n",
    "trainer = peft_trainer.PeftTrainer(\n",
    "    model=lora_model,\n",
    "    optimizer=optimizer,\n",
    "    training_config=training_config\n",
    ")\n",
    "\n",
    "# Create Iterator\n",
    "train_iter = create_data_iterator(sft_dataset, TRAIN_BATCH_SIZE, tokenizer)\n",
    "\n",
    "print(f\"Starting Continuation Training for {SFT_STEPS} steps...\")\n",
    "print(f\"Learning Rate Peak: {LEARNING_RATE}\")\n",
    "\n",
    "with mesh:\n",
    "    trainer.train(train_ds=train_iter, skip_jit=False)\n",
    "\n",
    "print(\"Continuation Training Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70382968",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save Continuation Model ---\n",
    "FINAL_SAVE_DIR = \"/kaggle/working/final_continuation_model\"\n",
    "os.makedirs(FINAL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "checkpointer.save(os.path.join(FINAL_SAVE_DIR, \"checkpoint\"), nnx.state(lora_model, nnx.LoRAParam))\n",
    "checkpointer.wait_until_finished()\n",
    "\n",
    "print(f\"✅ Model saved to {FINAL_SAVE_DIR}\")\n",
    "print(\"1. Download output.\")\n",
    "print(\"2. Upload as Kaggle Model.\")\n",
    "print(\"3. Update Unrestricted Model ID.\")\n",
    "\n",
    "unrestricted_kaggle_model = \"yuyamukai/tunix-gemma2-sft-unrestricted\"\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
