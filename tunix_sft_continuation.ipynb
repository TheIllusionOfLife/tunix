{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194442b6",
   "metadata": {},
   "source": [
    "# Tunix SFT: Continuation Training (Unrestricted Mode)\n",
    "\n",
    "**Strategy**: Continue Supervised Fine-Tuning on a larger dataset (GlaiveAI) starting from the Session 1 checkpoint.\n",
    "\n",
    "**Prerequisites**:\n",
    "1. Run Session 1 notebook (`tunix_sft_train.ipynb`) and save output.\n",
    "2. Upload the Session 1 output as a Kaggle Dataset (e.g., `tunix-session1-checkpoint`).\n",
    "3. Attach that dataset to this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5134654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# Update these paths based on your Kaggle Dataset names\n",
    "\n",
    "# Path to checkpoint from Session 1 (Uploaded as Dataset)\n",
    "PREV_CHECKPOINT_PATH = \"/kaggle/input/tunix-session1-checkpoint/final_sft_model/checkpoint\"\n",
    "\n",
    "# Path to continuation training data (Fresh GlaiveAI samples)\n",
    "CONTINUATION_DATA_PATH = \"/kaggle/input/tunix-sft-continuation-data\"\n",
    "# Note: Adaptive filtering (10.4k->15k) is applied during loading to ensure >80% retention.\n",
    "\n",
    "# Training Hyperparams - Adjust for HP tuning\n",
    "# Training Hyperparams - Adjust for HP tuning\n",
    "# SFT_STEPS is now dynamic\n",
    "# SFT_STEPS = 5000 (Removed)\n",
    "LEARNING_RATE = 5e-6  # Lower LR for continuation (try: 1e-5, 5e-6, 2e-6)\n",
    "WARMUP_STEPS = 100  # Warmup steps\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION = 16\n",
    "EFFECTIVE_BATCH = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION  # 32\n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "# LoRA Hyperparams (must match session 1)\n",
    "RANK = 64\n",
    "ALPHA = 64.0\n",
    "\n",
    "# System Prompt (must match session 1)\n",
    "SYSTEM_PROMPT = \"You are a deep thinking AI. Think step by step about the problem and provide your reasoning between <reasoning> and </reasoning> tags. Then, provide the final answer between <answer> and </answer> tags.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be824db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup & Install ---\n",
    "!pip install -q wandb==0.22.0\n",
    "!pip install -q kagglehub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q transformers\n",
    "!pip install -q grain\n",
    "\n",
    "# Tunix/Qwix Installation\n",
    "import socket\n",
    "import os\n",
    "\n",
    "def is_connected():\n",
    "    try:\n",
    "        socket.create_connection((\"1.1.1.1\", 53))\n",
    "        return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "if is_connected():\n",
    "    !pip install \"google-tunix[prod]==0.1.5\"\n",
    "    !pip install git+https://github.com/google/qwix\n",
    "else:\n",
    "    print(\"Offline mode detected. Assuming legacy installation or wheels.\")\n",
    "\n",
    "\n",
    "# Fix Flax Version\n",
    "!pip uninstall -q -y flax\n",
    "!pip install flax==0.12.0\n",
    "!pip install -q datasets==3.2.0 optax==0.2.4 chex>=0.1.90\n",
    "\n",
    "# --- Imports ---\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from flax import nnx\n",
    "import grain\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "import qwix\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Tunix Imports\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma import model as gemma_lib\n",
    "from tunix.models.gemma import params as params_lib\n",
    "from tunix.sft import peft_trainer\n",
    "\n",
    "# Stability\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.95'\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "print(f\"JAX Devices: {jax.devices()}\")\n",
    "\n",
    "# Constants\n",
    "MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "SFT_OUTPUT_DIR = \"/kaggle/working/sft_continuation_checkpoint\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- WandB Logging with Metrics Backend ---\n",
    "WANDB_ENABLED = False\n",
    "\n",
    "class WandbBackend:\n",
    "    '''Custom backend to stream metrics to WandB during training'''\n",
    "    def log_scalar(self, event: str, value, **kwargs):\n",
    "        if WANDB_ENABLED:\n",
    "            step = kwargs.get(\"step\", 0)\n",
    "            wandb.log({event: float(value)}, step=step)\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "    if secret_value:\n",
    "        wandb.login(key=secret_value)\n",
    "        wandb.init(\n",
    "            project=\"tunix-sft-continuation\",\n",
    "            name=\"sft-cont-v1\",\n",
    "            anonymous=\"allow\",\n",
    "            config={\n",
    "                \"sft_steps\": SFT_STEPS,\n",
    "                \"learning_rate\": LEARNING_RATE,\n",
    "                \"warmup_steps\": WARMUP_STEPS,\n",
    "                \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
    "                \"gradient_accumulation\": GRADIENT_ACCUMULATION,\n",
    "                \"effective_batch\": EFFECTIVE_BATCH,\n",
    "                \"max_seq_len\": MAX_SEQ_LEN,\n",
    "                \"lora_rank\": RANK,\n",
    "                \"lora_alpha\": ALPHA,\n",
    "            }\n",
    "        )\n",
    "        WANDB_ENABLED = True\n",
    "        print(\"WandB Logging Enabled for continuation training.\")\n",
    "    else:\n",
    "        raise ValueError(\"Empty WANDB_API_KEY\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"WandB not enabled: {e}\")\n",
    "    os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "    print(\"Proceeding without cloud logging.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Utilities ---\n",
    "MESH = [(8, 1), (\"fsdp\", \"tp\")]\n",
    "\n",
    "def get_gemma_model(ckpt_path):\n",
    "    # Load Base Model Structure\n",
    "    mesh = jax.make_mesh(*MESH)\n",
    "    model_config = gemma_lib.ModelConfig.gemma2_2b()\n",
    "    abs_gemma: nnx.Module = nnx.eval_shape(\n",
    "        lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
    "    )\n",
    "    abs_state = nnx.state(abs_gemma)\n",
    "    abs_state = jax.tree.map(\n",
    "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "        abs_state,\n",
    "        nnx.get_named_sharding(abs_state, mesh),\n",
    "    )\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "\n",
    "    graph_def, _ = nnx.split(abs_gemma)\n",
    "    gemma = nnx.merge(graph_def, restored_params)\n",
    "    return gemma, mesh, model_config\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    # Tunix LoRA Config\n",
    "    RANK = 64\n",
    "    ALPHA = 64.0\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "            \".*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "\n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, rngs=nnx.Rngs(params=0), **model_input\n",
    "    )\n",
    "\n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "\n",
    "    return lora_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load Checkpoint & Prepare Model ---\n",
    "\n",
    "# 1. Download Base Model (for tokenizer & structure)\n",
    "if \"KAGGLE_USERNAME\" not in os.environ:\n",
    "    kagglehub.login()\n",
    "\n",
    "kaggle_ckpt_path = kagglehub.model_download(f\"google/gemma-2/flax/gemma2-2b-it\")\n",
    "\n",
    "# Prepare intermediate conversion\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "if not os.path.exists(INTERMEDIATE_CKPT_DIR):\n",
    "    print(\"Converting base model checkpoint...\")\n",
    "    params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\"))\n",
    "    gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    _, state = nnx.split(gemma)\n",
    "    checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "    checkpointer.wait_until_finished()\n",
    "    del params, gemma, state\n",
    "    gc.collect()\n",
    "\n",
    "# 2. Initialize Models\n",
    "print(\"Initializing Base Model...\")\n",
    "base_model, mesh, model_config = get_gemma_model(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"))\n",
    "lora_model = get_lora_model(base_model, mesh=mesh)\n",
    "\n",
    "# 3. Load Previous Session State (LoRA weights)\n",
    "print(f\"Restoring Session 1 Checkpoint from: {PREV_CHECKPOINT_PATH}\")\n",
    "\n",
    "try:\n",
    "    # Map structure for LoRA params\n",
    "    abs_lora_params = jax.tree.map(\n",
    "        lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "        nnx.state(lora_model, nnx.LoRAParam),\n",
    "    )\n",
    "    \n",
    "    # Restore\n",
    "    prev_checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_lora_params = prev_checkpointer.restore(PREV_CHECKPOINT_PATH, target=abs_lora_params)\n",
    "    \n",
    "    # Update model\n",
    "    nnx.update(lora_model, restored_lora_params)\n",
    "    print(\"✅ Successfully restored previous SFT state.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to restore checkpoint: {e}\")\n",
    "    print(\"Double check PREV_CHECKPOINT_PATH. If this is the first run, this is expected to fail.\")\n",
    "    print(\"CRITICAL: Continuing without loaded state means restarting training from scratch!\")\n",
    "    # raise e # Uncomment to enforce strict loading\n",
    "\n",
    "# 4. Tokenizer\n",
    "tokenizer = tokenizer_lib.Tokenizer(\n",
    "    tokenizer_path=os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load Continuation Dataset (Pre-sampled GlaiveAI) ---\n",
    "# This parquet file contains 100K fresh samples from GlaiveAI\n",
    "# (samples 30,001 - 130,000, NOT overlapping with single session)\n",
    "\n",
    "import glob\n",
    "import re\n",
    "\n",
    "print(f\"Loading continuation data from {CONTINUATION_DATA_PATH}...\")\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "def standardize_to_gemma_format(text, question=None):\n",
    "    '''Standardize GlaiveAI format to Gemma conversation format'''\n",
    "    # Replace GlaiveAI's <think> tags with our <reasoning> tags\n",
    "    text = re.sub(r\"<think>\", \"<reasoning>\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"</think>\", \"</reasoning>\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Case 1: Has <reasoning> but no <answer> - extract answer from after </reasoning>\n",
    "    if \"<reasoning>\" in text and \"<answer>\" not in text:\n",
    "        parts = text.split(\"</reasoning>\")\n",
    "        if len(parts) > 1:\n",
    "            reasoning_part = parts[0] + \"</reasoning>\"\n",
    "            answer_part = parts[1].strip()\n",
    "            if answer_part:\n",
    "                text = f\"{reasoning_part}\\n<answer>{answer_part}</answer>\"\n",
    "            else:\n",
    "                # No content after reasoning - use last sentence as answer fallback\n",
    "                reasoning_match = re.search(r\"<reasoning>(.*?)</reasoning>\", text, re.DOTALL)\n",
    "                if reasoning_match:\n",
    "                    reasoning_text = reasoning_match.group(1).strip()\n",
    "                    sentences = reasoning_text.split(\".\")\n",
    "                    answer_fallback = sentences[-1].strip() if sentences and sentences[-1].strip() else reasoning_text[:200]\n",
    "                    text = f\"{text}\\n<answer>{answer_fallback}</answer>\"\n",
    "    \n",
    "    # Case 2: No <reasoning> AND no <answer> - wrap entire text with both tags\n",
    "    elif \"<reasoning>\" not in text and \"<answer>\" not in text:\n",
    "        # Treat text as combined reasoning+answer\n",
    "        # Use all but last paragraph as reasoning, last paragraph as answer\n",
    "        paragraphs = text.strip().split(\"\\n\\n\")\n",
    "        if len(paragraphs) > 1:\n",
    "            reasoning = \"\\n\\n\".join(paragraphs[:-1])\n",
    "            answer = paragraphs[-1]\n",
    "        else:\n",
    "            # Single paragraph - use as both\n",
    "            reasoning = text.strip()\n",
    "            answer = text.strip()\n",
    "        text = f\"<reasoning>{reasoning}</reasoning>\\n<answer>{answer}</answer>\"\n",
    "    \n",
    "    # Case 3: Has <answer> but no <reasoning> - wrap content before <answer> as reasoning\n",
    "    elif \"<answer>\" in text and \"<reasoning>\" not in text:\n",
    "        parts = text.split(\"<answer>\")\n",
    "        if len(parts) > 1:\n",
    "            pre_answer = parts[0].strip()\n",
    "            answer_content = \"<answer>\" + parts[1]\n",
    "            if pre_answer:\n",
    "                text = f\"<reasoning>{pre_answer}</reasoning>\\n{answer_content}\"\n",
    "            else:\n",
    "                # No content before answer - use answer content as reasoning too\n",
    "                answer_match = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL)\n",
    "                if answer_match:\n",
    "                    text = f\"<reasoning>{answer_match.group(1).strip()}</reasoning>\\n{answer_content}\"\n",
    "    \n",
    "    # Build full conversation format\n",
    "    if question:\n",
    "        formatted = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n<start_of_turn>model\\n{text}\"\n",
    "        return formatted\n",
    "    return text\n",
    "\n",
    "# Load parquet files from continuation dataset\n",
    "try:\n",
    "    parquet_files = glob.glob(f\"{CONTINUATION_DATA_PATH}/*.parquet\")\n",
    "    \n",
    "    if parquet_files:\n",
    "        # 1. Pass 1: Counting (No Memory Load)\n",
    "        print(\"Pass 1: Scanning continuation dataset...\")\n",
    "        total_samples = 0\n",
    "        threshold_counts = {t: 0 for t in [10400, 12500, 15000]}\n",
    "        threshold_counts[None] = 0 # No filter\n",
    "\n",
    "        for parquet_file in parquet_files:\n",
    "            ds = datasets.load_dataset(\"parquet\", data_files=parquet_file, split=\"train\")\n",
    "            print(f\"Scanning {os.path.basename(parquet_file)} ({len(ds)} samples)...\")\n",
    "            \n",
    "            for sample in ds:\n",
    "                response_len = len(sample.get(\"response\", \"\"))\n",
    "                total_samples += 1 # Count ALL samples for ratio\n",
    "                \n",
    "                if response_len < 50: continue\n",
    "                \n",
    "                for t in threshold_counts:\n",
    "                    if t is None or response_len <= t:\n",
    "                        threshold_counts[t] += 1\n",
    "        \n",
    "        print(f\"Total samples scanned: {total_samples}\")\n",
    "        \n",
    "        # 2. Select Threshold\n",
    "        selected_threshold = None\n",
    "        thresholds_ordered = [10400, 12500, 15000, None]\n",
    "        \n",
    "        for t in thresholds_ordered:\n",
    "            count = threshold_counts[t]\n",
    "            ratio = count / total_samples if total_samples > 0 else 0\n",
    "            print(f\"Threshold: {t} -> Kept: {count}/{total_samples} ({ratio:.2%})\")\n",
    "            \n",
    "            if ratio >= 0.8:\n",
    "                selected_threshold = t\n",
    "                print(f\"Selected Threshold: {selected_threshold}\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"All thresholds yielded < 80% data. Disabling length filter.\")\n",
    "            selected_threshold = None\n",
    "        \n",
    "        # 3. Pass 2: Loading & Formatting\n",
    "        print(\"Pass 2: Loading selected continuation samples (Streaming to JSONL)...\")\n",
    "        \n",
    "        # Stream directly to JSONL file to avoid holding all strings in RAM\n",
    "        with open(\"continuation_data.jsonl\", \"w\") as f:\n",
    "            import json\n",
    "            for parquet_file in parquet_files:\n",
    "                ds = datasets.load_dataset(\"parquet\", data_files=parquet_file, split=\"train\")\n",
    "                for sample in ds:\n",
    "                    prompt = sample.get(\"prompt\", \"\")\n",
    "                    response = sample.get(\"response\", \"\")\n",
    "                    \n",
    "                    # Apply filter\n",
    "                    if len(response) < 50: continue\n",
    "                    if selected_threshold is not None and len(response) > selected_threshold:\n",
    "                        continue\n",
    "                        \n",
    "                    if prompt and response:\n",
    "                        formatted = standardize_to_gemma_format(response, question=prompt)\n",
    "                        f.write(json.dumps({\"text\": formatted}) + \"\\n\")\n",
    "        \n",
    "        # Load using memory-mapped Arrow dataset\n",
    "        print(\"Loading dataset from JSONL (Memory Safe)...\")\n",
    "        sft_dataset = datasets.load_dataset(\"json\", data_files=\"continuation_data.jsonl\", split=\"train\")\n",
    "        dataset_size = len(sft_dataset)\n",
    "        print(f\"Final Continuation Dataset Size: {dataset_size}\")\n",
    "        \n",
    "        # Reduce disk pressure\n",
    "        if os.path.exists(\"continuation_data.jsonl\"):\n",
    "            os.remove(\"continuation_data.jsonl\")\n",
    "            print(\"Removed temporary continuation_data.jsonl\")\n",
    "        \n",
    "        # --- Dynamic SFT Steps Calculation ---\n",
    "        # Target: ~2 epochs for continuation\n",
    "        # SAFETY: Use math.ceil and max(1, ...)\n",
    "        import math\n",
    "        TARGET_EPOCHS = 2\n",
    "        SFT_STEPS = max(1, math.ceil((dataset_size * TARGET_EPOCHS) / EFFECTIVE_BATCH))\n",
    "        print(f\"Dynamic SFT Steps: {SFT_STEPS} (based on {dataset_size} samples, {TARGET_EPOCHS} epochs)\")\n",
    "\n",
    "    else:\n",
    "        print(f\"WARNING: No parquet files found in {CONTINUATION_DATA_PATH}\")\n",
    "        # Default fallback to prevent crash if files missing\n",
    "        dataset_size = 0\n",
    "        SFT_STEPS = 100 \n",
    "        # Create empty dummy dataset to prevent NameError downstream\n",
    "        sft_dataset = datasets.Dataset.from_dict({\"text\": []})\n",
    "    \n",
    "    print(f\"Total continuation samples: {dataset_size}\")\n",
    "    \n",
    "    # Shuffle & Show Sample (Only if data exists)\n",
    "    if dataset_size > 0:\n",
    "        sft_dataset = sft_dataset.shuffle(seed=42)\n",
    "        print(f\"\\nSample: {sft_dataset[0]['text'][:500]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Failed to load continuation data: {e}\")\n",
    "    raise RuntimeError(f\"Dataset loading failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715bf18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Continuation Training ---\n",
    "\n",
    "print(\"Starting SFT Continuation...\")\n",
    "\n",
    "# Imports for Training\n",
    "import numpy as np\n",
    "from tunix.sft import utils as sft_utils\n",
    "\n",
    "# Optimizer (Lower LR)\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=LEARNING_RATE,\n",
    "    warmup_steps=100,\n",
    "    decay_steps=SFT_STEPS,\n",
    "    end_value=1e-7\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(learning_rate=schedule, weight_decay=0.01)\n",
    ")\n",
    "\n",
    "# Training Config\n",
    "checkpoint_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=500, max_to_keep=2\n",
    ")\n",
    "\n",
    "def create_data_iterator(dataset, batch_size, tokenizer):\n",
    "    '''Create batches with tokenization and masking'''\n",
    "    indices = np.random.permutation(len(dataset))\n",
    "    \n",
    "    # Infinite iterator matching steps\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            if len(batch_indices) < batch_size:\n",
    "                continue\n",
    "                \n",
    "            texts = [dataset[int(idx)]['text'] for idx in batch_indices]\n",
    "            \n",
    "            # Tokenize\n",
    "            batch_input_tokens = []\n",
    "            batch_input_mask = []\n",
    "            \n",
    "            for text in texts:\n",
    "                # Use Tunix Tokenizer.tokenize which handles BOS/EOS\n",
    "                tokens = tokenizer.tokenize(text, add_eos=True).tolist()\n",
    "                \n",
    "                # Truncate / Pad\n",
    "                if len(tokens) > MAX_SEQ_LEN:\n",
    "                    tokens = tokens[:MAX_SEQ_LEN]\n",
    "                    mask = [True] * MAX_SEQ_LEN\n",
    "                else:\n",
    "                    pad_len = MAX_SEQ_LEN - len(tokens)\n",
    "                    mask = [True] * len(tokens) + [False] * pad_len\n",
    "                    pad_id = getattr(tokenizer, 'pad_id', lambda: 0)()\n",
    "                    tokens = tokens + [pad_id] * pad_len\n",
    "                \n",
    "                batch_input_tokens.append(tokens)\n",
    "                batch_input_mask.append(mask)\n",
    "            \n",
    "            # Convert to JAX arrays\n",
    "            input_tokens = jnp.array(batch_input_tokens, dtype=jnp.int32)\n",
    "            input_mask = jnp.array(batch_input_mask, dtype=jnp.bool_)\n",
    "            \n",
    "            # Create PEFT required inputs\n",
    "            positions = sft_utils.build_positions_from_mask(input_mask)\n",
    "            attention_mask = sft_utils.make_causal_attn_mask(input_mask)\n",
    "            \n",
    "            yield {\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"input_mask\": input_mask,\n",
    "                \"positions\": positions,\n",
    "                \"attention_mask\": attention_mask\n",
    "            }\n",
    "\n",
    "# Training Configuration with WandB Metrics Backend\n",
    "from tunix.sft import metrics_logger as sft_metrics_logger\n",
    "\n",
    "metrics_logging_options = sft_metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/kaggle/working/logs\",\n",
    "    backend_factories=[WandbBackend] if WANDB_ENABLED else []\n",
    ")\n",
    "\n",
    "training_config = peft_trainer.TrainingConfig(\n",
    "    max_steps=SFT_STEPS,\n",
    "    checkpoint_root_directory=SFT_OUTPUT_DIR,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    checkpointing_options=checkpoint_options,\n",
    "    pbar_description=\"SFT Continuation\",\n",
    "    metrics_prefix=\"sft_cont\",\n",
    "    metrics_logging_options=metrics_logging_options,\n",
    "    eval_every_n_steps=10000,\n",
    ")\n",
    "\n",
    "trainer = peft_trainer.PeftTrainer(\n",
    "    model=lora_model,\n",
    "    optimizer=optimizer,\n",
    "    training_config=training_config\n",
    ")\n",
    "\n",
    "# Create Iterator\n",
    "train_iter = create_data_iterator(sft_dataset, TRAIN_BATCH_SIZE, tokenizer)\n",
    "\n",
    "print(f\"Starting Continuation Training for {SFT_STEPS} steps...\")\n",
    "print(f\"Learning Rate Peak: {LEARNING_RATE}\")\n",
    "\n",
    "with mesh:\n",
    "    trainer.train(train_ds=train_iter, skip_jit=False)\n",
    "\n",
    "print(\"Continuation Training Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Visual Sanity Check & Validation ---\n",
    "print(\"Running Post-Training Evaluation...\")\n",
    "\n",
    "try:\n",
    "    inference_sampler = sampler_lib.Sampler(\n",
    "        transformer=lora_model,\n",
    "        tokenizer=tokenizer,\n",
    "        cache_config=sampler_lib.CacheConfig(\n",
    "            cache_size=MAX_SEQ_LEN + 512,\n",
    "            num_layers=model_config.num_layers,\n",
    "            num_kv_heads=model_config.num_kv_heads,\n",
    "            head_dim=model_config.head_dim,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    test_prompts = [\n",
    "        \"What are the ethical implications of AI in healthcare?\",\n",
    "        \"Write a short story about a robot learning to paint.\",\n",
    "        \"Explain why the sky is blue to a 5-year-old.\",\n",
    "        \"Solve this math problem step-by-step: If 2x + 5 = 15, what is x?\"\n",
    "    ]\n",
    "    \n",
    "    formatted_prompts = [TEMPLATE.format(question=p) for p in test_prompts]\n",
    "    \n",
    "    out_data = inference_sampler(\n",
    "        input_strings=formatted_prompts,\n",
    "        max_generation_steps=1024,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Validation Logic\n",
    "    print(\"--- Post-Training Outputs ---\")\n",
    "    valid_format_count = 0\n",
    "    results_for_wandb = []\n",
    "    \n",
    "    for p, o in zip(test_prompts, out_data.text):\n",
    "        print(f\"Prompt: {p}\")\n",
    "        print(f\"Output: {o[:500]}...\")\n",
    "        \n",
    "        has_reasoning = bool(re.search(r\"<reasoning>.*?</reasoning>\", o, re.DOTALL))\n",
    "        has_answer = bool(re.search(r\"<answer>.*?</answer>\", o, re.DOTALL))\n",
    "        \n",
    "        is_valid = has_reasoning and has_answer\n",
    "        if is_valid:\n",
    "            valid_format_count += 1\n",
    "            print(\"✅ Format Check: Passed\")\n",
    "        else:\n",
    "            print(f\"❌ Format Check: Failed\")\n",
    "            \n",
    "        results_for_wandb.append([p, o, is_valid])\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"Format Validation: {valid_format_count}/{len(test_prompts)} passed.\")\n",
    "    \n",
    "    # Safe WandB Logging\n",
    "    try:\n",
    "        if 'wandb' in locals() and wandb.run is not None:\n",
    "            tbl = wandb.Table(columns=[\"Prompt\", \"Output\", \"IsValid\"], data=results_for_wandb)\n",
    "            wandb.log({\"eval_results\": tbl})\n",
    "            print(\"Logged results to WandB.\")\n",
    "    except Exception as w_err:\n",
    "        print(f\"Post-training WandB logging failed: {w_err}\")\n",
    "    \n",
    "    # Force sync before exit\n",
    "    print(\"Syncing WandB...\")\n",
    "    try:\n",
    "        if 'wandb' in locals() and wandb.run is not None:\n",
    "             wandb.finish()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    import time\n",
    "    time.sleep(5)\n",
    "    print(\"Done.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17fcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save Continuation Model ---\n",
    "FINAL_SAVE_DIR = \"/kaggle/working/final_continuation_model\"\n",
    "os.makedirs(FINAL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "checkpointer.save(os.path.join(FINAL_SAVE_DIR, \"checkpoint\"), nnx.state(lora_model, nnx.LoRAParam))\n",
    "checkpointer.wait_until_finished()\n",
    "\n",
    "print(f\"✅ Model saved to {FINAL_SAVE_DIR}\")\n",
    "print(\"1. Download output.\")\n",
    "print(\"2. Upload as Kaggle Model.\")\n",
    "print(\"3. Update Unrestricted Model ID.\")\n",
    "\n",
    "unrestricted_kaggle_model = \"yuyamukai/tunix-gemma2-sft-unrestricted\"\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
